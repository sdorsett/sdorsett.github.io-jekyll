<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      sdorsett.github.io &middot; Things I find interesting
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700|PT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p></p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item active" href="/">Home</a>

    

    
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/about/">About</a>
        
      
    
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/posts/">All Posts</a>
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
    <a class="sidebar-nav-item" href="https://github.com/sdorsett/archive/v1.0.0.zip">Download</a>
    <a class="sidebar-nav-item" href="https://github.com/sdorsett">GitHub project</a>
    <span class="sidebar-nav-item">Currently v1.0.0</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2018. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">sdorsett.github.io</a>
            <small>Things I find interesting</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2014/12/28/getting-packer-installed-on-OS-X/">
        Using packer on OS X to create an ESXi .box template for vagrant deployment
      </a>
    </h1>

    <span class="post-date">28 Dec 2014</span>

    <p>I&#39;ve been recently working on using packer to create vagrant .box files rather than manually creating them as I documented in a previous post. For this post I will be using fusion and the vagrant vmware provider, each of which have an associated cost, but I will cover a free alternative using packer and CentOS in a future post.</p>

<p>Several github projects by gosddc have helped me in getting packer up and running on my Macbook:</p>

<ul>
<li><p><a href="https://github.com/gosddc/packer-post-processor-vagrant-vmware-ovf">gosddc/packer-post-processor-vagrant-vmware-ovf</a>.
This repo contains a packer post processor that leverages VMware OVF Tool to create a vmware_ovf Vagrant box that is compatible with vagrant-vcloud, vagrant-vcenter and vagrant-vcloudair vagrant providers. It is only compatible with the packer VMware builder.</p></li>
<li><p><a href="https://github.com/gosddc/packer-templates">gosddc/packer-templates</a>.
This repo contains Packer templates for boxes available at https://vagrantcloud.com/gosddc, they only work with VMware and require the packer-post-processor-vagrant-vmware-ovf post-processor to work. These templates are a good starting point for generating pakcer templates on VMware products.</p></li>
</ul>

<h2>1. Let&#39;s get started by installing packer.</h2>

<h4>A.  Download packer from the following link:</h4>

<p><a href="https://www.packer.io/downloads.html">https://www.packer.io/downloads.html</a></p>

<h4>B.  Unzip the downloaded files to /usr/local/packer_7.5:</h4>

<h4>C.  Add /usr/local/packer_7.5 to your path by running the following command.</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">sdorsett-mbp:~ sdorsett$ export PATH=&quot;/usr/local/packer_7.5:$PATH&quot;</code></pre></figure>

<h4>D. Add the export command we just ran into ~/.bash_profile to ensure this path change persists after reboots.</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">sdorsett-mbp:~ sdorsett$ cat ~/.bash_profile
export PATH=&quot;/usr/local/packer_7.5:$PATH&quot;
sdorsett-mbp:~ sdorsett$</code></pre></figure>

<h2>2. Next we need to download and install VMware ovftool, since the vagrant-vmware-ovf requires it.</h2>

<h4>A. Download and install the latest version of the VMware OVF tool. VMware-ovftool-3.5.0-1274719-mac.x64.dmg is what I used.</h4>

<h4>B. Verify ovftool is succesfully added to your path by running &quot;ovftool -v&quot;. This command should output the version of ovftool we installed.</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">sdorsett-mbp:~ sdorsett$ ovftool -v
VMware ovftool 3.5.0 (build-1274719)</code></pre></figure>

<h2>3. Now we can download and install the gosddc packer components we will need.</h2>

<h4>A. Download the most recent version of the compiled packer-processor-vagrant-vmware-ovf binary from the following link:</h4>

<p><a href="https://github.com/gosddc/packer-post-processor-vagrant-vmware-ovf/releases">https://github.com/gosddc/packer-post-processor-vagrant-vmware-ovf/releases</a></p>

<h4>B. Unzip packer-post-processor-vagrant-vmware-ovf and copy it to &quot;usr/local/packer_7.5&quot;. Ensure the permissions of this file match the other files in this directory.</h4>

<h4>C. Create a directory to contain the packer templates:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">sdorsett-mbp:~ sdorsett$ mkdir ~/Documents/packer
sdorsett-mbp:~ sdorsett$ cd ~/Documents/packer/</code></pre></figure>

<h5>D. Clone the gosddc packer-templates repository:</h5>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">sdorsett-mbp:packer sdorsett$ git clone https://github.com/gosddc/packer-templates.git
sdorsett-mbp:packer sdorsett$ cd packer-templates</code></pre></figure>

<h2>4. Now we need to download the ESXi 5.5 .iso and copy it into the proper directory location.</h2>

<h4>A. create an &quot;iso&quot; directory for storing the ESXi iso files:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">sdorsett-mbp:packer-templates sdorsett$ mkdir ~/Documents/packer/packer-templates/iso</code></pre></figure>

<h4>B. Download and copy the &quot;VMware-VMvisor-Installer-5.5.0-1331820.x86_64.iso&quot; ESXi 5.5 installer to the iso directory.</h4>

<h2>5. Finally we will need to modify, validate and build the packer esxi.json packer template we will be using.</h2>

<h4>A. modify ~/Documents/packer/packer-templates/templates/esxi.json to look like the following:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">sdorsett-mbp:packer-templates sdorsett$ cat templates/esxi.json
{
  &quot;variables&quot;: {
    &quot;version&quot;: &quot;1.0&quot;
  },
  &quot;builders&quot;: [
    {
      &quot;name&quot;: &quot;esxi55&quot;,
      &quot;vm_name&quot;: &quot;esxi55&quot;,
      &quot;vmdk_name&quot;: &quot;esxi55-disk0&quot;,
      &quot;type&quot;: &quot;vmware-iso&quot;,
      &quot;headless&quot;: true,
      &quot;disk_size&quot;: 4096,
      &quot;guest_os_type&quot;: &quot;vmkernel5&quot;,
      &quot;iso_url&quot;: &quot;./iso/VMware-VMvisor-Installer-5.5.0-1331820.x86_64.iso&quot;,
      &quot;iso_checksum&quot;: &quot;ef599dc7e647177027684c0eee346ccdbc8704f2&quot;,
      &quot;iso_checksum_type&quot;: &quot;sha1&quot;,
      &quot;ssh_username&quot;: &quot;root&quot;,
      &quot;ssh_password&quot;: &quot;vagrant&quot;,
      &quot;ssh_wait_timeout&quot;: &quot;60m&quot;,
      &quot;shutdown_command&quot;: &quot;esxcli system maintenanceMode set -e true -t 0 ; esxcli system shutdown poweroff -d 10 -r &#39;Packer Shutdown&#39; ; esxcli system maintenanceMode set -e false -t 0&quot;,
      &quot;http_directory&quot;: &quot;.&quot;,
      &quot;boot_wait&quot;: &quot;5s&quot;,
      &quot;vmx_data&quot;: {
        &quot;memsize&quot;: &quot;4096&quot;,
        &quot;numvcpus&quot;: &quot;2&quot;,
        &quot;vhv.enable&quot;: &quot;TRUE&quot;
      },
      &quot;boot_command&quot;: [
        &quot;&lt;enter&gt;&lt;wait&gt;O&lt;wait&gt; ks=http://{{ .HTTPIP }}:{{ .HTTPPort }}/scripts/esxi-5-kickstart.cfg&lt;enter&gt;&quot;
      ]
    }
  ],
  &quot;provisioners&quot;: [
    {
      &quot;type&quot;: &quot;file&quot;,
      &quot;source&quot;: &quot;puppet/modules/vagrantbaseconfig/files/vagrant.pub&quot;,
      &quot;destination&quot;: &quot;/etc/ssh/keys-root/authorized_keys&quot;
    },
    {
      &quot;type&quot;: &quot;shell&quot;,
      &quot;script&quot;: &quot;scripts/esxi-vmware-tools_install.sh&quot;
    },
    {
      &quot;type&quot;: &quot;shell&quot;,
      &quot;script&quot;: &quot;scripts/esxi-cloning_configuration.sh&quot;
    }
  ],
  &quot;post-processors&quot;: [
   {
     &quot;type&quot;: &quot;vagrant-vmware-ovf&quot;,
     &quot;compression_level&quot;: 9,
     &quot;output&quot;: &quot;{{.BuildName}}-{{.Provider}}-{{user `version`}}.box&quot;

   }
  ]
}</code></pre></figure>

<p>There are several things I would like to point out in the esxi.json file we just created.</p>

<ol>
<li>builder - this section specifies that we will be using the &quot;vmware-iso&quot; builder, with VMware fusion, to create our packer template. We can modify attributes of our template virtual machine in this section:

<ul>
<li>disk size ( disk_size)</li>
<li>memory ( memsize )</li>
<li>vCPU count ( numvcpus )</li>
</ul></li>
<li>provisioners - this section specifies we will be using multiple provisioners to modify our template after it has been created:

<ul>
<li>a file provisioner that will copy the vagrant public ssh key into our ESXi template</li>
<li>a shell script to install the vmware tools VIB for nested ESXi</li>
<li>a shell script to make necessary MAC address changes in our nested ESXi template.</li>
</ul></li>
<li>post-processors - this final section will convert the virtual machine artifact generated in VMware fusion into a vmware-ovf .box file we can use with vagrant</li>
</ol>

<h4>B. Validate the packer esxi.json file is ready for building by running the following command:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">sdorsett-mbp:packer-templates sdorsett$ packer validate templates/esxi.json
Template validated successfully.</code></pre></figure>

<h4>C. Start the build by running:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">sdorsett-mbp:packer-templates sdorsett$ packer build templates/esxi.json
esxi55 output will be in this color.

==&gt; esxi55: Downloading or copying ISO
    esxi55: Downloading or copying: file:///Users/sdorsett/Documents/packer/packer-templates/iso/VMware-VMvisor-Installer-5.5.0-1331820.x86_64.iso
==&gt; esxi55: Creating virtual machine disk
==&gt; esxi55: Building and writing VMX file
==&gt; esxi55: Starting HTTP server on port 8351
==&gt; esxi55: Starting virtual machine...
    esxi55: The VM will be run headless, without a GUI. If you want to
    esxi55: view the screen of the VM, connect via VNC without a password to
    esxi55: 127.0.0.1:5986
==&gt; esxi55: Waiting 5s for boot...
==&gt; esxi55: Connecting to VM via VNC
==&gt; esxi55: Typing the boot command over VNC...
==&gt; esxi55: Waiting for SSH to become available...
==&gt; esxi55: Connected to SSH!
==&gt; esxi55: Uploading puppet/modules/vagrantbaseconfig/files/vagrant.pub =&gt; /etc/ssh/keys-root/authorized_keys
==&gt; esxi55: Provisioning with shell script: scripts/esxi-vmware-tools_install.sh
    esxi55: Installation Result
    esxi55: Message: The update completed successfully, but the system needs to be rebooted for the changes to be effective.
    esxi55: Reboot Required: true
    esxi55: VIBs Installed: VMware_bootbank_esx-tools-for-esxi_9.7.0-0.0.00000
    esxi55: VIBs Removed:
    esxi55: VIBs Skipped:
==&gt; esxi55: Provisioning with shell script: scripts/esxi-cloning_configuration.sh
    esxi55: diff: can&#39;t stat &#39;/tmp/auto-backup.35216//etc/ssh/keys-root/authorized_keys&#39;: No such file or directory
    esxi55: Saving current state in /bootbank
    esxi55: Clock updated.
    esxi55: Time: 02:09:44   Date: 12/30/2014   UTC
==&gt; esxi55: Gracefully halting virtual machine...
    esxi55: Waiting for VMware to clean up after itself...
==&gt; esxi55: Deleting unnecessary VMware files...
    esxi55: Deleting: output-esxi55/564d2ab2-395b-a9ba-9c17-2fe36682237c.vmem
    esxi55: Deleting: output-esxi55/esxi55.plist
    esxi55: Deleting: output-esxi55/vmware.log
==&gt; esxi55: Cleaning VMX prior to finishing up...
    esxi55: Unmounting floppy from VMX...
    esxi55: Detaching ISO from CD-ROM device...
==&gt; esxi55: Compacting the disk image
==&gt; esxi55: Running post-processor: vagrant-vmware-ovf
==&gt; esxi55 (vagrant-vmware-ovf): Creating Vagrant box for &#39;vmware_ovf&#39; provider
    esxi55 (vagrant-vmware-ovf): Deleting key: ide1:0.filename
    esxi55 (vagrant-vmware-ovf): Deleting key: floppy0.present
    esxi55 (vagrant-vmware-ovf): Setting key: floppy0.present = FALSE
    esxi55 (vagrant-vmware-ovf): Setting key: ide1:0.present = FALSE
    esxi55 (vagrant-vmware-ovf): Creating directory: output-esxi55/ovf
    esxi55 (vagrant-vmware-ovf): Starting ovftool
    esxi55 (vagrant-vmware-ovf): Reading files in output-esxi55/ovf
    esxi55 (vagrant-vmware-ovf): Copying: esxi55-disk1.vmdk
    esxi55 (vagrant-vmware-ovf): Copying: esxi55.mf
    esxi55 (vagrant-vmware-ovf): Copying: esxi55.ovf
    esxi55 (vagrant-vmware-ovf): Compressing: Vagrantfile
    esxi55 (vagrant-vmware-ovf): Compressing: esxi55-disk1.vmdk
    esxi55 (vagrant-vmware-ovf): Compressing: esxi55.mf
    esxi55 (vagrant-vmware-ovf): Compressing: esxi55.ovf
    esxi55 (vagrant-vmware-ovf): Compressing: metadata.json
Build &#39;esxi55&#39; finished.

==&gt; Builds finished. The artifacts of successful builds are:
--&gt; esxi55: &#39;vmware_ovf&#39; provider box: esxi55-vmware_ovf-1.0.box
sdorsett-mbp:packer-templates sdorsett$</code></pre></figure>

<p>If you want to keep an eye on the build process you can connect a VNC client to the address and port listed during the packer build process. For my build this was what was displayed:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">esxi55: The VM will be run headless, without a GUI. If you want to
    esxi55: view the screen of the VM, connect via VNC without a password to
    esxi55: 127.0.0.1:5986</code></pre></figure>

<p>When I connected the &quot;Chicken of the VNC&quot; client installed on my macbook to &quot;127.0.0.1:5986&quot; I could see where the build was at during the install process:</p>

<p><img src="/assets/01-esxi-vnc-client.png" alt="screenshot">  </p>

<h4>D. Once the packer build completes you should end up with a &quot;esxi55-vmware_ovf-1.0.box&quot; file in the same directory you ran the &quot;packer build&quot; command in. This .box file can be used with vagrant and the gosddc vagrant providers to deploy this template to ESXi, vCenter, vCloud Director and vCloud Air.</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">sdorsett-mbp:packer-templates sdorsett$ ls *.box
esxi55-vmware_ovf-1.0.box</code></pre></figure>

<h3>Hopefully you found this post helpful in getting packer and the packer vagrant-vmware-ovf post processor installed/configured. The next blog post will be covering how to install packer on CentOS and build a packer virtual machine on a remote ESXi host.</h3>

<h3>Please provide any feedback or suggestions to my twitter account located on the about page.</h3>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2014/05/26/vagrant-install/">
        Installing Vagrant and the vagrant-vcloud plugin on CentOS 6.x
      </a>
    </h1>

    <span class="post-date">26 May 2014</span>

    <p>In this post I&#39;m setting out to explain how to create a CentOS 6.4 vm, from template, in vCHS (or a vCloud Director instance) and then install vagrant-vcloud on that. </p>

<h3>1. Create a CentOS 6.x minimal virtual machine from a template in a vCHS organization.</h3>

<p>I will demonstrate creating a new CentOS vm in vCHS using a template, but you could just as easily create a new CentOS virtual machine from scratch in vCloud Director.</p>

<h4>A. Log into vCHS using your credentials and click on your virtual datacenter.</h4>

<h4>B. Next click on the &quot;Virtual Machines&quot; tab and then click the &quot;Deploy a Virtual Machine&quot; button:</h4>

<p><img src="/assets/01-create-centos-64-vm.png" alt="screenshot"></p>

<h4>C. Select the &quot;CentOS 6.4 64 Bit&quot; template and click &quot;Continue&quot;</h4>

<p><img src="/assets/02-select-centos-64-template.png" alt="screenshot"></p>

<h4>D. Name the virtual machine and set the guest OS name of your new virtual machine. Click &quot;Deploy This Virtual Machine&quot;:</h4>

<p>I named my virtual machine and set the guest OS name both to &quot;vagrant&quot;, but you can change these to what ever you prefer.</p>

<p><img src="/assets/03-virtual-machine-resource-settings.png" alt="screenshot"></p>

<h4>E. Power on and then click the name of the virtual machine you just created:</h4>

<p><img src="/assets/04-centos-vm-created.png" alt="screenshot"></p>

<h4>F. Click the &quot;Launch Console&quot; link to open the console of the virtual machine you just created.</h4>

<p>Notice that the guest OS password created by guest customization is displayed on the left. Log into the virtual machine, using &quot;root&quot; and the displayed password, then change this password immediately to something else:</p>

<p><img src="/assets/05-open-console-and-log-in.png" alt="screenshot"></p>

<h3>2. Modify /etc/resolv.conf</h3>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant ~]# cat /etc/resolv.conf
nameserver 8.8.8.8 
nameserver 8.8.4.4</code></pre></figure>

<p>At this point you should have network connectivity to your new vagrant vapp. While I won&#39;t cover it in the post, you can now either setup a DNAT rule to forward SSH to the vagrant vapp or continue the configuration using the vapp console.</p>

<h3>3. Install Ruby 1.9.3 using ruby-build:</h3>

<h4>A. Use yum to install the packages we&#39;ll need to get ruby, rubygems and vagrant installed:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant ~]# yum install -y gcc-c++ glibc-headers openssl-devel readline libyaml-devel readline-devel zlib zlib-devel iconv-devel libxml2 libxml2-devel libxslt libxslt-devel wget git</code></pre></figure>

<h4>B. Pull down the latest version of ruby-build using git:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant ~]# git clone https://github.com/sstephenson/ruby-build.git</code></pre></figure>

<h4>C. Change to the directory the previous command created:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant ~]# cd ruby-build/</code></pre></figure>

<h4>D. Run the &#39;install.sh&#39; script to install ruby-build:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant ruby-build]# ./install.sh</code></pre></figure>

<h4>E. I would recommend installing Ruby version 1.9.3-p545 with the following command:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant ruby-build]# ruby-build --definitionsruby-build 1.9.3-p545 /usr/local/
Downloading yaml-0.1.6.tar.gz...
-&gt; http://dqw8nmjcqpjn7.cloudfront.net/5fe00cda18ca5daeb43762b80c38e06e
Installing yaml-0.1.6...
Installed yaml-0.1.6 to /usr/local/

Downloading ruby-1.9.3-p545.tar.gz...
-&gt; http://dqw8nmjcqpjn7.cloudfront.net/8e8f6e4d7d0bb54e0edf8d9c4120f40c
Installing ruby-1.9.3-p545...

Installed ruby-1.9.3-p545 to /usr/local/</code></pre></figure>

<h3>4. Install rubygems by downloading the latest version from <a href="https://rubygems.org/pages/download">rubygems.org</a> and install it:</h3>

<h4>A. Use wget to download the latest version listed on the rubygems download page:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant ruby-build]# cd ~/
[root@vagrant ~]# wget http://production.cf.rubygems.org/rubygems/rubygems-2.2.2.tgz</code></pre></figure>

<h4>B. Use tar to unzip the tar-gzipped file we downloaded:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant ~]# tar xvzf rubygems-2.2.2.tgz
[root@vagrant ~]# cd rubygems-2.2.2</code></pre></figure>

<h4>C. Install rubygems by running the &#39;setup.rb&#39; script:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant rubygems-2.2.2]# ruby setup.rb</code></pre></figure>

<h3>5. Install Vagrant</h3>

<h4>A. Use wget to download the latest version listed on the <a href="https://www.vagrantup.com/downloads.html">Vagrant download</a> page:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant rubygems-2.2.2]# cd ~/
[root@vagrant ~]# wget https://dl.bintray.com/mitchellh/vagrant/vagrant_1.6.2_x86_64.rpm</code></pre></figure>

<h4>B. Use the &#39;rpm&#39; command to install the RPM package we downloaded in the previous step:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant ~]# rpm -i vagrant_1.6.2_x86_64.rpm</code></pre></figure>

<h4>C. Export the directory that the vagrant executable was install to, in order to make it easier to run:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant ~]# export PATH=$PATH:/opt/vagrant/bin/</code></pre></figure>

<h3>6. Installing the vagrant-vcloud plugin</h3>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant blog]# vagrant plugin install vagrant-vcloud
Installing the &#39;vagrant-vcloud&#39; plugin. This can take a few minutes...
Installed the plugin &#39;vagrant-vcloud (0.3.3)&#39;!</code></pre></figure>

<h3>7. Creating a Vagrantfile for testing vagrant-vcloud install</h3>

<h4>A.  We need to create a directory structure and Vagrantfile to test that the vagrant-vcloud provider is properly working. For the .box file we will use a sample precise32 (Ubuntu 12.04 32bit) vagrant .box file created for the vcloud provider by <a href="http://blog.tsugliani.fr/">Timo Sugliani</a>:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant ~]# mkdir -p ~/vagrant-vms/precise32-test
[root@vagrant ~]# cat ~/vagrant-vms/precise32-test/Vagrantfile 
precise32_vm_box_url = &quot;http://vagrant.tsugliani.fr/precise32.box&quot;

nodes = [
  { :hostname =&gt; &quot;test-vm&quot;,  
    :box =&gt; &quot;precise32&quot;, 
    :box_url =&gt; precise32_vm_box_url }
]

Vagrant.configure(&quot;2&quot;) do |config|

  # vCloud Director provider settings
  config.vm.provider :vcloud do |vcloud|

    vcloud.hostname = &#39;https://[Your_vCD_URL]:443&#39;
    vcloud.username = &#39;[Your_vCHS_username@somedomain.com]&#39;
    vcloud.password = &#39;[Your_vCHS_password]&#39;

    vcloud.org_name = &#39;[Your_vCHS_org_name]&#39;
    vcloud.vdc_name = &#39;[Your_vCHS_vdc_name]&#39;
    vcloud.catalog_name = &#39;[Your_vCHS_org_catalog]&#39;
    vcloud.network_bridge = true
    vcloud.vdc_network_name = &#39;[Your_vCHS_vdc_name]-default-routed&#39;

  end

  nodes.each do |node|
    config.vm.define node[:hostname] do |node_config|
      node_config.vm.box = node[:box]
      node_config.vm.hostname = node[:hostname]
      node_config.vm.box_url = node[:box_url]
    end
  end
end</code></pre></figure>

<p>In this configuration file:</p>

<ul>
<li>&quot;vcloud.hostname&quot; is your vCloud Director FQDN or IP address. This should be the base URL and not by the specific URL for your org.</li>
<li>&quot;vcloud.username&quot; is the username we will be using to connect to vCHS or vCloud Director</li>
<li>&quot;vcloud.password&quot; is the password we will be using to connect to vCHS or vCloud Director</li>
<li>&quot;vcloud.org_name&quot; will be the name of your vCHS or vCloud Director organization</li>
<li>&quot;vcloud.vdc_name&quot; will be the name of your vCHS or vCloud Director virtual datacenter</li>
<li>&quot;vcloud.catalog_name&quot; will be the name of your organization catalog a template derived from the .box file will be created in</li>
<li>&quot;vcloud.dc_network_name&quot; is the organization network the vagrant vapp will be connected to</li>
</ul>

<h4>B.  Now we can change directory to that directory and test everything with a &quot;vagrant up --provider=vcloud&quot;:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant ~]# cd ~/vagrant-vms/precise32-test
oot@vagrant precise32-test]# vagrant up --provider=vcloud
Bringing machine &#39;test-vm&#39; up with &#39;vcloud&#39; provider...
==&gt; test-vm: Box &#39;precise32&#39; could not be found. Attempting to find and install...
    test-vm: Box Provider: vcloud
    test-vm: Box Version: &gt;= 0
==&gt; test-vm: Adding box &#39;precise32&#39; (v0) for provider: vcloud
    test-vm: Downloading: http://vagrant.tsugliani.fr/precise32.box
==&gt; test-vm: Successfully added box &#39;precise32&#39; (v0) for &#39;vcloud&#39;!
==&gt; test-vm: Catalog item [precise32] in Catalog [Org Private Catalog] does not exist!
    test-vm: Would you like to upload the [precise32] box to [Org Private Catalog] Catalog?
    test-vm: Choice (yes/no): yes
==&gt; test-vm: Uploading [precise32]...
Uploading Box...
Uploading Box...
Uploading Box... Progress: 22%  ETA: 00:02:16</code></pre></figure>

<p>Once the .box files has started uploading, you can check the progress in the org catalog of your vCloud Director instance:</p>

<p><img src="/assets/06-precise32-upload.png" alt="screenshot"></p>

<h4>C. The uploading progress will get to 100% and you will see the vApp getting created, powered on and SSH access achieved:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">Uploading Box...
Uploading Box... Progress: 100% Time: 00:02:32                                                                                                   
==&gt; test-vm: Adding [precise32] to Catalog [Org Private Catalog]
==&gt; test-vm: Building vApp...
==&gt; test-vm: vApp Vagrant-root-vagrant-8b9115d1 successfully created.
==&gt; test-vm: Powering on VM...
==&gt; test-vm: Waiting for SSH Access on 192.168.109.4:22 ... 
==&gt; test-vm: Waiting for SSH Access on 192.168.109.4:22 ... 
==&gt; test-vm: Waiting for SSH Access on 192.168.109.4:22 ... 
==&gt; test-vm: Rsyncing folder: /root/vagrant-vms/precise32-test/ =&gt; /vagrant
[root@vagrant precise32-test]# root@vagrant precise32-test]#</code></pre></figure>

<h4>D. Test network connectivity by running &quot;vagrant ssh&quot;:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">root@vagrant precise32-test]# vagrant ssh
==&gt; test-vm: External IP for test-vm: 192.168.109.4
Welcome to Ubuntu 12.04 LTS (GNU/Linux 3.2.0-23-generic-pae i686)

 * Documentation:  https://help.ubuntu.com/
Welcome to your Vagrant-built virtual machine.
Last login: Thu Jul 18 11:20:25 2013
vagrant@test-vm:~$ hostname -f
test-vm</code></pre></figure>

<h4>E. Run &quot;vagrant vcloud --status&quot; to validate the vCloud Director status of your vagrant vapp:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">root@vagrant precise32-test]# vagrant vcloud --status
Initializing vCloud Director provider...
Fetching vCloud Director status...
+-------------------------------+-----------------------------------------+
|  Vagrant vCloud Director Status : https://[Your_vCD_URL]:443            |
+-------------------------------+-----------------------------------------+
| Organization Name             | [Your_vCHS_org_name]                    |
| Organization vDC Name         | vCHS-vagrant-demo                       |
| Organization vDC ID           | b9494d6a-ebda-4368-8f76-0e93b7edcb17    |
| Organization vDC Network Name | [Your_vCHS_vdc_name]-default-routed     |
+-------------------------------+-----------------------------------------+
| vApp Name                     | Vagrant-root-vagrant-8b9115d1           |
| vAppID                        | ec918e84-6026-42b2-8639-5390d8c88b0c    |
| -&gt; test-vm                    | 4fc82b9b-c79d-483b-9a71-a87d22289102    |
+-------------------------------+-----------------------------------------+</code></pre></figure>

<h4>F. Run &quot;vagrant vcloud --network&quot; to validate the vCloud Director network mapping of your vagrant vapp:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant precise32-test]# vagrant vcloud --network
Initializing vCloud Director provider...
Fetching vCloud Director network settings ...
+---------+---------------+------------+
|             Network Map              |
+---------+---------------+------------+
| VM Name | IP Address    | Connection |
+---------+---------------+------------+
| test-vm | 192.168.109.4 | Direct     |
+---------+---------------+------------+</code></pre></figure>

<h4>g. Destroy the cloned vm by running &quot;vagrant destroy&quot; command:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant precise32-test]# vagrant destroy
    test-vm: Are you sure you want to destroy the &#39;test-vm&#39; VM? [y/N] y
==&gt; test-vm: Powering off VM...
==&gt; test-vm: Single VM left in the vApp, Powering off vApp...
==&gt; test-vm: Destroying vApp...
[root@vagrant precise32-test]#</code></pre></figure>

<p>You should see &quot;Power Off vapp&quot; and &quot;Delete vapp&quot; tasks being run and then completing on your organizations vapps.</p>

<h3>Hopefully you found this post helpful in getting vagrant and the vagrant-cloud plugin installed and configured. The next blog post will be covering how to modify the Vagrantfile to create a vapp in a vCHS or vCloud Director instance that doesn&#39;t contain the vagrant vm.</h3>

<h3>Please provide any feedback or suggestions to my twitter account located on the about page.</h3>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2014/05/25/vagrant-vcloud/">
        Introducing the vagrant-vcloud provider
      </a>
    </h1>

    <span class="post-date">25 May 2014</span>

    <p>While continuing to explore what the vagrant-vsphere provider is capable of I came across the <a href="https://github.com/frapposelli/vagrant-vcloud">vagrant-vcloud</a> provider, which had recently released a new version. I work for the vCHS operations group, so I figured it would be interesting to compare the feature differences of the vsphere &amp; vcloud providers. </p>

<p>Over the next few blog posts I intend to cover the following vagrant-vcloud provider related topics:</p>

<ul>
<li><a href="https://sdorsett.github.io/2014/05/26/vagrant-install/">Installing Vagant and the vagrant-vcloud plugin on a CentOS 6.x virtual machine in vCloud Directory or vCHS</a></li>
<li>Creating a simple vagrant-vcloud Vagrantfile configuration to deploy a vm </li>
<li>Creating a more advanced vagrant-vcloud Vagrantfile configuration</li>
<li>Creating a CentOS 6.x .box that is customized for Vagrant and vcloud director</li>
</ul>

<p>These posts will hopefully explain the steps necessary to get vagrant-vcloud installed and working with a vCloud Director or vCHS environment.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2014/05/06/vagrant-and-puppet/">
        Creating a Puppet manifest and integrating it with Vagrant
      </a>
    </h1>

    <span class="post-date">06 May 2014</span>

    <p>This post will cover configuring Vagrant to automatically run a Puppet manifest on the vm created by &quot;vagrant up.&quot; This capability allows you to test your Puppet manifests, make changes and test again, all quickly and easily. Let&#39;s get started:</p>

<h3>1. Create the Puppet manifest &amp; modules we will be using for our Vagrant tests.</h3>

<p>For testing purposes we will be creating a Puppet manifest that ensures NTP is installed and is configured to use the following NTP servers:</p>

<ul>
<li>0.pool.ntp.org</li>
<li>1.pool.ntp.org</li>
<li>2.pool.ntp.org</li>
</ul>

<h4>A. First we should install tree, since it will help us visualize the directory structure of the puppet manifest directories we will be creating:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant vagrant-vms]# yum install -y tree
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
 * base: mirrors.centarra.com
 * epel: mirror.unl.edu
 * extras: mirrors.finalasp.com
 * updates: mirrors.centarra.com
Setting up Install Process
Resolving Dependencies
--&gt; Running transaction check
---&gt; Package tree.x86_64 0:1.5.3-2.el6 will be installed
--&gt; Finished Dependency Resolution

Dependencies Resolved

====================================================================
 Package      Arch           Version             Repository    Size
====================================================================
Installing:
 tree         x86_64         1.5.3-2.el6         base          36 k

Transaction Summary
====================================================================
Install       1 Package(s)

Total download size: 36 k
Installed size: 65 k
Downloading Packages:
tree-1.5.3-2.el6.x86_64.rpm                    |  36 kB     00:00     
Running rpm_check_debug
Running Transaction Test
Transaction Test Succeeded
Running Transaction
  Installing : tree-1.5.3-2.el6.x86_64                          1/1 
  Verifying  : tree-1.5.3-2.el6.x86_64                          1/1 

Installed:
  tree.x86_64 0:1.5.3-2.el6                                           

Complete!
[root@vagrant vagrant-vms]#</code></pre></figure>

<h4>B. Change to the directory that contains the Vagrantfile &amp; example_box directory we&#39;ve been working in over the last few blog posts:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant ~]# cd ~/vagrant-vms/
[root@vagrant vagrant-vms]# tree
.
├── example_box
│   └── dummy.box
└── Vagrantfile

1 directory, 2 files
[root@vagrant vagrant-vms]#</code></pre></figure>

<h4>C. Create the following modules &amp; manifests directory structure for our Puppet manifest files:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant vagrant-vms]# mkdir -p {manifests,modules/ntp/manifests,modules/ntp/templates,modules/common/manifests}
[root@vagrant vagrant-vms]# tree
.
├── example_box
│   └── dummy.box
├── manifests
├── modules
│   ├── common
│   │   └── manifests
│   └── ntp
│       ├── manifests
│       └── templates
└── Vagrantfile

8 directories, 2 files
[root@vagrant vagrant-vms]#</code></pre></figure>

<h4>D. Create manifests/site.pp with the following contents:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant vagrant-vms]# vi manifests/site.pp
node default {
  include common
    class {&#39;ntp&#39;:}
}
[root@vagrant vagrant-vms]#</code></pre></figure>

<p>This is the Puppet manifest that we will configure Vagrent to run automatically. The &quot;node default&quot; section will be applied by any hostname that runs this manifest. The &quot;node default&quot; section calls the common &amp; ntp modules which we will create next.</p>

<h4>E. Create modules/common/manifests/init.pp with the following contents:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant vagrant-vms]# cat modules/common/manifests/init.pp
class common {
  include common::data
}

class common::data {
  $ntpServerList = [ &#39;0.pool.ntp.org&#39;,&#39;1.pool.ntp.org&#39;,&#39;2.pool.ntp.org&#39; ]
}
[root@vagrant vagrant-vms]#</code></pre></figure>

<p>This manifest is what gets run when Puppets runs &quot;include common&quot; in the manifests/site.pp manifest. When this manifest is run it creates an array named &quot;common::data::ntpServerList&quot; containing the ntp servers we&#39;re needing to have defined.</p>

<h4>F. Create modules/ntp/manifests/init.pp with the following contents:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant vagrant-vms]# cat modules/ntp/manifests/init.pp
class ntp( $ntpServerList = $common::data::ntpServerList) {
  package { &#39;ntp&#39;:
  ensure =&gt; &#39;present&#39;,
  } #package

  file { &#39;/etc/ntp.conf&#39;:
    mode    =&gt; &quot;644&quot;,
    content =&gt; template(&quot;ntp/client-ntp.conf.erb&quot;),
    notify  =&gt; Service[&quot;ntpd&quot;],
    require =&gt; Package[&quot;ntp&quot;],
  } # file

  service { &#39;ntpd&#39;:
    ensure  =&gt; running,
    enable  =&gt; true,
    require =&gt; Package[&quot;ntp&quot;],
  } # service
}
[root@vagrant vagrant-vms]#</code></pre></figure>

<p>This manifest is what gets run when Puppet calls &quot;class {&#39;ntp&#39;:}&quot; in the manifests/site.pp manifest. When this manifest is run it will ensure the ntp package is installed, the file &quot;/etc/ntp.conf&quot; is created containing our list of ntp servers and the ntpd service is started.    </p>

<h4>G. Create modules/ntp/templates/client-ntp.conf.erb with the following contents:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"># This file is being maintained by Puppet.
# DO NOT EDIT

# Permit time synchronization with our time source, but do not
# permit the source to query or modify the service on this system.
restrict default kod nomodify notrap nopeer noquery
restrict -6 default kod nomodify notrap nopeer noquery

# Permit all access over the loopback interface.  This could
# be tightened as well, but to do so would effect some of
# the administrative functions.
restrict 127.0.0.1
restrict -6 ::1

&lt;% ntpServerList.each do |ntpServer| -%&gt;
server &lt;%= ntpServer %&gt;
&lt;% end -%&gt;

# Drift file.  Put this in a directory which the daemon can write to.
# No symbolic links allowed, either, since the daemon updates the file
# by creating a temporary in the same directory and then rename()&#39;ing
# it to the file.
driftfile /var/lib/ntp/drift
[root@vagrant vagrant-vms]#</code></pre></figure>

<p>This .erb file is a ruby template describing what the file /etc/ntp.conf should contain. The magic of this is the section:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">&lt;% ntpServerList.each do |ntpServer| -%&gt;
server &lt;%= ntpServer %&gt;
&lt;% end -%&gt;</code></pre></figure>

<p>This section is actually ruby code that runs a foreach loop on the array &quot;ntpServerList&quot; and adds a line for each ntp server contained in that array. </p>

<h4>H. Run the tree command and you should see the following file/directory structure:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant vagrant-vms]# tree
.
├── example_box
│   └── dummy.box
├── manifests
│   └── site.pp
├── modules
│   ├── common
│   │   └── manifests
│   │       └── init.pp
│   └── ntp
│       ├── manifests
│       │   └── init.pp
│       └── templates
│           └── client-ntp.conf.erb
└── Vagrantfile

8 directories, 6 files
[root@vagrant vagrant-vms]#</code></pre></figure>

<h3>2. Modify the Vagrantfile we created in the last post to include the following:</h3>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant vagrant-vms]# cat Vagrantfile
Vagrant.configure(&quot;2&quot;) do |config|
  config.vm.box = &#39;dummy&#39;
  config.vm.box_url = &#39;./example_box/dummy.box&#39;

  config.vm.provider :vsphere do |vsphere|
    vsphere.host = &#39;192.168.1.195&#39;
    vsphere.name = &#39;vagrant-test&#39;
    vsphere.clone_from_vm = true
    vsphere.template_name = &#39;vagrant-centos-6.5&#39;
    vsphere.user = &#39;root@localos&#39;
    vsphere.password = &#39;S0meR@nd0mP@ssw0rd&#39;
    vsphere.insecure = true
    vsphere.data_store_name = &#39;vsanDatastore&#39;
    vsphere.linked_clone = true
    vsphere.customization_spec_name = &#39;vagrant-centos&#39;
  end
  config.vm.provision &quot;puppet&quot; do |puppet|
    puppet.manifests_path = &quot;manifests&quot;
    puppet.manifest_file = &quot;site.pp&quot;
    puppet.module_path = &quot;modules&quot;
  end
end
[root@vagrant vagrant-vms]#</code></pre></figure>

<p>The following new lines have been added:</p>

<ul>
<li>&quot;puppet.manifests_path&quot; specifies the sub-directory, from the directory containing the Vagrantfile, that contains the puppet manifests.</li>
<li>&quot;puppet.manifest_file&quot; specifies the puppet manifest that will be initially run.</li>
<li>&quot;puppet.module_path&quot; specifies the sub-directory, from the directory containing the Vagrantfile, that contains the puppet modules.</li>
</ul>

<h3>3. Test the configuration changes we made by running a &quot;vagrant up&quot;:</h3>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant vagrant-vms]# vagrant up --provider=vsphere
Bringing machine &#39;default&#39; up with &#39;vsphere&#39; provider...
==&gt; default: Calling vSphere CloneVM with the following settings:
==&gt; default:  -- Source VM: vagrant-centos-6.5
==&gt; default:  -- Name: vagrant-test
==&gt; default: Waiting for SSH to become available...
==&gt; default: New virtual machine successfully cloned and started
==&gt; default: Rsyncing folder: /root/vagrant-vms/ =&gt; /vagrant
==&gt; default: Rsyncing folder: /root/vagrant-vms/manifests/ =&gt; /tmp/vagrant-puppet-1/manifests
==&gt; default: Rsyncing folder: /root/vagrant-vms/modules/ =&gt; /tmp/vagrant-puppet-1/modules-0
==&gt; default: Running provisioner: puppet...
Running Puppet with site.pp...
notice: /File[/etc/ntp.conf]/content: content changed &#39;{md5}d7e1e16f9c0cd6382f6b68b486163db1&#39; to &#39;{md5}f7a83a4ca84e1ba2bba0166c5620e9e7&#39;
notice: /Stage[main]/Ntp/Service[ntpd]: Triggered &#39;refresh&#39; from 1 events
notice: Finished catalog run in 0.72 seconds
[root@vagrant vagrant-vms]#</code></pre></figure>

<p>You might notice a few new lines since the &quot;vagrant up&quot; we ran in the last post. </p>

<ul>
<li>The manifests &amp; modules folders we specified in Vagrantfile have been copied over using rsync. </li>
<li>The Puppet agent is running a &quot;puppet apply&quot; using the site.pp file we specified. </li>
<li>/etc/ntp.conf was modified by Puppet</li>
<li>The NTP service was notified that it needed to refresh it&#39;s configuration since /etc/ntp.conf had been modified.</li>
</ul>

<h3>4. Connect to the vagrant vm to validate that /etc/ntp.conf has been modified to include the three NTP servers we specified:</h3>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant vagrant-vms]# vagrant ssh
[vagrant@vagrant-test ~]$ cat /etc/ntp.conf
# This file is being maintained by Puppet.
# DO NOT EDIT

# Permit time synchronization with our time source, but do not
# permit the source to query or modify the service on this system.
restrict default kod nomodify notrap nopeer noquery
restrict -6 default kod nomodify notrap nopeer noquery

# Permit all access over the loopback interface.  This could
# be tightened as well, but to do so would effect some of
# the administrative functions.
restrict 127.0.0.1
restrict -6 ::1

server 0.pool.ntp.org iburst
server 1.pool.ntp.org iburst
server 2.pool.ntp.org iburst

# Drift file.  Put this in a directory which the daemon can write to.
# No symbolic links allowed, either, since the daemon updates the file
# by creating a temporary in the same directory and then rename()&#39;ing
# it to the file.
driftfile /var/lib/ntp/drift
[vagrant@vagrant-test ~]$ exit
logout
Connection to 192.168.1.123 closed.
[root@vagrant vagrant-vms]#</code></pre></figure>

<h3>5. Modify the site.pp file to add a node definition for our vagrant vm:</h3>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant vagrant-vms]# cat site.pp
node default {
  include common
    class {&#39;ntp&#39;:}
}
node &#39;vagrant-test.mylab.net&#39; {
  include common
    class {&#39;ntp&#39;:
      ntpServerList =&gt; [&#39;3.pool.ntp.org&#39;,&#39;4.pool.ntp.org&#39;,&#39;5.pool.ntp.org&#39;]
    }
}</code></pre></figure>

<p>The new &quot;node &#39;vagrant-test.mylab.net&#39;&quot; definition will only be run by hosts with a matching hostname. Any host that doesn&#39;t have a matching host definition will continue to run the &quot;node default&quot; section. Node definitions allow you to change variables or even modules that will be run by specific hosts and provide flexibility in the changes made to a given host. Our new node section will configure &quot;vagrant-test.mylab.net&quot; to use a different set of ntp servers than any other host that runs this Puppet manifest  </p>

<h3>6. Run &#39;vagrant provision&#39; to apply the updated Puppet manifest to our Vagrant vm:</h3>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant vagrant-vms]# vagrant provision
==&gt; default: Rsyncing folder: /root/vagrant-vms/ =&gt; /vagrant
==&gt; default: Rsyncing folder: /root/vagrant-vms/manifests/ =&gt; /tmp/vagrant-puppet-1/manifests
==&gt; default: Rsyncing folder: /root/vagrant-vms/modules/ =&gt; /tmp/vagrant-puppet-1/modules-0
==&gt; default: Running provisioner: puppet...
Running Puppet with site.pp...
notice: /File[/etc/ntp.conf]/content: content changed &#39;{md5}f7a83a4ca84e1ba2bba0166c5620e9e7&#39; to &#39;{md5}414e811fdbfa3f8cfa38e1e4e6d0586f&#39;
notice: /Stage[main]/Ntp/Service[ntpd]: Triggered &#39;refresh&#39; from 1 events
notice: Finished catalog run in 0.34 seconds
[root@vagrant vagrant-vms]#</code></pre></figure>

<p>While we could have destroyed and recreated our Vagrant vm to test the change to the Puppet manifest, &#39;vagrant provision&#39; quickly resyncs the folders defined in our Vagrantfile and re-runs our Puppet manifest.</p>

<h3>7. Connect to the vagrant vm to validate that /etc/ntp.conf has been modified to include the three NTP servers we defined fof this specific node:</h3>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">root@vagrant vagrant-vms]# vagrant ssh
Last login: Tue May  6 15:09:50 2014 from 192.168.1.40
[vagrant@vagrant-test ~]$ cat /etc/ntp.conf
# This file is being maintained by Puppet.
# DO NOT EDIT

# Permit time synchronization with our time source, but do not
# permit the source to query or modify the service on this system.
restrict default kod nomodify notrap nopeer noquery
restrict -6 default kod nomodify notrap nopeer noquery

# Permit all access over the loopback interface.  This could
# be tightened as well, but to do so would effect some of
# the administrative functions.
restrict 127.0.0.1
restrict -6 ::1

server 3.pool.ntp.org iburst
server 4.pool.ntp.org iburst
server 5.pool.ntp.org iburst

# Drift file.  Put this in a directory which the daemon can write to.
# No symbolic links allowed, either, since the daemon updates the file
# by creating a temporary in the same directory and then rename()&#39;ing
# it to the file.
driftfile /var/lib/ntp/drift
[vagrant@vagrant-test ~]$ exit
logout
Connection to 192.168.1.123 closed.
[root@vagrant vagrant-vms]#</code></pre></figure>

<h3>8. Shutdown the vagrant machine:</h3>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant vagrant-vms]# vagrant destroy
==&gt; default: Calling vSphere PowerOff
==&gt; default: Calling vShpere Destroy
[root@vagrant vagrant-vms]#</code></pre></figure>

<h3>Hopefully you found this post helpful in demonstrating how to use Puppet with Vagrant, as well as the benefits it provides for testing Puppet manfests and modules</h3>

<h3>Please provide any feedback or suggestions to my twitter account located on the about page.</h3>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2014/04/24/vagrant-and-vcenter-guest-customization/">
        Using advanced vagrant-vsphere provider settings and vCenter guest customization
      </a>
    </h1>

    <span class="post-date">24 Apr 2014</span>

    <p>This post will pick up where we left off by demonstrating more vagrant-vsphere provider settings using the CentOS template we customized in the last blog post. Let&#39;s get started:</p>

<h3>1. Create a new customization specification in the vSphere web client. This customization specification will allow us to set the hostname of the vm created by &quot;vagrant up&quot; to the same name as the virtual machine.</h3>

<h4>A. Go to Home | Customization Specification Manager:</h4>

<p><img src="/assets/01-web-client-home.jpg" alt="screenshot"></p>

<h4>B. Click the &quot;Create a new specification&quot; button:</h4>

<p><img src="/assets/02-create-guest-customization.jpg" alt="screenshot"></p>

<h4>C. Select &quot;Linux&quot; for the &quot;Target VM Operating System&quot; and name the customization specification. Click &quot;Next&quot; to continue</h4>

<p><img src="/assets/03-new-guest-customization.jpg" alt="screenshot"></p>

<h4>D. For &quot;Computer Name&quot; select &quot;Use the virtual machine name&quot; and click &quot;Next&quot; to continue.</h4>

<p><img src="/assets/04-set-computer-name.jpg" alt="screenshot"></p>

<h4>E. For &quot;Time Zone&quot; select the time zone you want to use and click &quot;Next&quot; to continue.</h4>

<p><img src="/assets/05-set-timezone.jpg" alt="screenshot"></p>

<h4>F. For &quot;Configure Network&quot; keep the default and click &quot;Next to continue.</h4>

<p><img src="/assets/06-configure-network.jpg" alt="screenshot"></p>

<h4>G. For &quot;Enter DNS and Domain settings&quot; enter the DNS servers and the domain name you want to assign to the cloned vm. Click &quot;Next to continue.</h4>

<p><img src="/assets/07-dns-settings.jpg" alt="screenshot"></p>

<h4>H. Click &quot;Finish&quot; to create the customization specification.</h4>

<p><img src="/assets/08-finish.jpg" alt="screenshot"></p>

<h3>2. Modify the Vagrantfile we created in the initial &quot;<a href="http://sdorsett.github.io/2014/04/19/vagrant-install/">Installing Vagrant and the vagrant-vsphere plugin on CentOS 6.x</a>&quot; post.</h3>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant vagrant-vms]# cat Vagrantfile
Vagrant.configure(&quot;2&quot;) do |config|
  config.vm.box = &#39;dummy&#39;
  config.vm.box_url = &#39;./example_box/dummy.box&#39;

  config.vm.provider :vsphere do |vsphere|
    vsphere.host = &#39;192.168.1.195&#39;
    vsphere.name = &#39;vagrant-test&#39;
    vsphere.clone_from_vm = true
    vsphere.template_name = &#39;vagrant-centos-6.5&#39;
    vsphere.user = &#39;root@localos&#39;
    vsphere.password = &#39;S0meR@nd0mP@ssw0rd&#39;
    vsphere.insecure = true
    vsphere.data_store_name = &#39;vsanDatastore&#39;
    vsphere.linked_clone = true
    vsphere.customization_spec_name = &#39;vagrant-centos&#39;
  end
end</code></pre></figure>

<p>The following new lines have been added:</p>

<ul>
<li>&quot;vsphere.data_store_name&quot; is the datastore that the cloned vm will be deployed to.</li>
<li>&quot;vsphere.linked_clone&quot; being set to &quot;true&quot; will cause the cloned vm to be deployed as a linked clone. This will dramatically decrease the amount of time needed to clone the vm when &quot;vagrant up&quot; command is issued.</li>
<li>&quot;vsphere.customization_spec_name&quot; will specify the customization specification that will be applied to the cloned vm. Set this to the customization specification we created in the previous step.</li>
</ul>

<h3>3. Test the configuration changes we made by running a &quot;vagrant up&quot;:</h3>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant vagrant-vms]# vagrant up --provider=vsphere
Bringing machine &#39;default&#39; up with &#39;vsphere&#39; provider...
==&gt; default: Calling vSphere CloneVM with the following settings:
==&gt; default:  -- Source VM: vagrant-centos-6.5
==&gt; default:  -- Name: vagrant-test
==&gt; default: Waiting for SSH to become available...
==&gt; default: New virtual machine successfully cloned and started
==&gt; default: Rsyncing folder: /root/vagrant-vms/ =&gt; /vagrant
[root@vagrant vagrant-vms]#</code></pre></figure>

<p>You should find the time it took to deploy the linked clone vm was much faster than previous full clones of the vagrant template vm.</p>

<h3>4. Connect to the vagrant vm to validate the hostname of the vm matched the vm name &amp; search domain:</h3>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant vagrant-vms]# vagrant ssh
[vagrant@vagrant-test ~]$ hostname -f
vagrant-test.mylab.net
[vagrant@vagrant-test ~]$ exit
logout
Connection to 192.168.1.117 closed.
[root@vagrant vagrant-vms]#</code></pre></figure>

<h3>5. Clean up after ourselves and call it a day:</h3>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@vagrant vagrant-vms]# vagrant destroy
==&gt; default: Calling vSphere PowerOff
==&gt; default: Calling vShpere Destroy
[root@vagrant vagrant-vms]#</code></pre></figure>

<p>You might be wondering why setting the hostname &amp; domain name might be so important. We will see in future posts how to use hostnames to determine which manifests will be applied to specific vms we clone and power up with Vagrant.</p>

<h3>Hopefully you found this post helpful understanding more about the usefulness of customization specifications and advanced vagrant-vsphere configurations. The next blog post will start diving into using Vagrant to test Puppet manifests.</h3>

<h3>Please provide any feedback or suggestions to my twitter account located on the about page.</h3>

  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page5">Older</a>
  
  
    
      <a class="pagination-item newer" href="/page3">Newer</a>
    
  
</div>


      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

  </body>
</html>
