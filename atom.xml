<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>sdorsett.github.io</title>
 <link href="http://0.0.0.0:4000/atom.xml" rel="self"/>
 <link href="http://0.0.0.0:4000/"/>
 <updated>2018-08-22T01:01:37+00:00</updated>
 <id>http://0.0.0.0:4000</id>
 <author>
   <name>Stan Dorsett</name>
   <email></email>
 </author>

 
 <entry>
   <title>Using Terraform to deploy an OVH public cloud server.</title>
   <link href="http://0.0.0.0:4000/2018/08/21/using-terraform-to-deploy-to-ovh-public-cloud/"/>
   <updated>2018-08-21T00:00:00+00:00</updated>
   <id>http://0.0.0.0:4000/2018/08/21/using-terraform-to-deploy-to-ovh-public-cloud</id>
   <content type="html">&lt;p&gt;This is the fifth in a series of posts that will walk you through using the Openstack-based OVH public cloud. In this post you will get introduced to using Terraform to create Openstack servers using the Openstack API.
This post is assumes you have already signed up for an account with ovhcloud.com, added a payment method, created a cloud project, created an Openstack user, created a ssh keypair in the Horizon UI and downloaded the openrc.sh file for this user. If you have not done these steps you can follow the steps in &lt;a href=&quot;http://sdorsett.github.io/2018/08/08/creating-an-openstack-instance-on-ovh-public-cloud/&quot;&gt;the first&lt;/a&gt; and &lt;a href=&quot;http://sdorsett.github.io/2018/08/10/using-the-openstack-cli-to-create-a-server-on-ovh-public-cloud/&quot;&gt;second blog post&lt;/a&gt; in this series that will walk you through completing those steps.&lt;/p&gt;

&lt;p&gt;I will attempt in this post to present the options that are available to OVH public cloud customers along side the choices I made that were specific to my Openstack server.
Let&amp;#39;s get started...&lt;/p&gt;

&lt;h2&gt;1. Install Terraform&lt;/h2&gt;

&lt;p&gt;The first thing you will need to do before you can move forward with this post is install Terraform. Go to https://www.terraform.io/downloads.html, download the binary for your specific operating system, unzip the binary and copy it to a directory that is in your path. Here are the steps that I performed on my Mac laptop:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:~ standorsett$ cd ~/Downloads/
MacBook-Pro:Downloads standorsett$ wget https://releases.hashicorp.com/terraform/0.11.8/terraform_0.11.8_darwin_amd64.zip
--2018-08-21 19:12:36--  https://releases.hashicorp.com/terraform/0.11.8/terraform_0.11.8_darwin_amd64.zip
Resolving releases.hashicorp.com (releases.hashicorp.com)... 151.101.125.183
Connecting to releases.hashicorp.com (releases.hashicorp.com)|151.101.125.183|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 19269525 (18M) [application/zip]
Saving to: ‘terraform_0.11.8_darwin_amd64.zip’

terraform_0.11.8_darwin_amd64.zip            100%[=============================================================================================&amp;gt;]  18.38M  1.45MB/s    in 21s

2018-08-21 19:12:57 (900 KB/s) - ‘terraform_0.11.8_darwin_amd64.zip’ saved [19269525/19269525]

MacBook-Pro:Downloads standorsett$ unzip terraform_0.11.8_darwin_amd64.zip
Archive:  terraform_0.11.8_darwin_amd64.zip
  inflating: terraform
MacBook-Pro:Downloads standorsett$ mv terraform /usr/local/bin/
MacBook-Pro:Downloads standorsett$ terraform --version
Terraform v0.11.8

MacBook-Pro:Downloads standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;2. Create a new directory and add your main.tf file&lt;/h2&gt;

&lt;p&gt;When using the &lt;code&gt;terraform&lt;/code&gt; command, it will look for any files ending in .tf in the current directory and create them. As a result you should create a new directory for containing the terraform files we will use for creating the Openstack server.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:~ standorsett$ mkdir -p ~/Documents/terraform/ovh_public_cloud_centos_7
MacBook-Pro:~ standorsett$ cd ~/Documents/terraform/ovh_public_cloud_centos_7
MacBook-Pro:ovh_public_cloud_centos_7 standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Next you will need to create a file named main.tf in our new directory that will describe the server you are wanting to have created.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:ovh_public_cloud_centos_7 standorsett$ cat main.tf
# Configure the OpenStack Provider
provider &amp;quot;openstack&amp;quot; {
#  user_name is unset so it defaults to OS_USERNAME environment variable
#  tenant_name is unset so it defaults to OS_TENANT_NAME or OS_PROJECT_NAME environment variable
#  password is unset so it defaults to OS_PASSWORD environment variable
#  auth_url is unset so it defaults to OS_AUTH_URL environment variable
#  region is unset so it defaults to OS_REGION_NAME environment variable
}

# get an image named &amp;quot;Centos 7&amp;quot;
data &amp;quot;openstack_images_image_v2&amp;quot; &amp;quot;centos_7&amp;quot; {
  name = &amp;quot;Centos 7&amp;quot;
  most_recent = true
}

# get a flavor named &amp;quot;s1-2&amp;quot;
data &amp;quot;openstack_compute_flavor_v2&amp;quot; &amp;quot;s1-2&amp;quot; {
  name = &amp;quot;s1-2&amp;quot;
}

resource &amp;quot;openstack_compute_instance_v2&amp;quot; &amp;quot;basic&amp;quot; {
  name            = &amp;quot;terraform_centos_7&amp;quot;
  image_id        = &amp;quot;${data.openstack_images_image_v2.centos_7.id}&amp;quot;
  flavor_id       = &amp;quot;${data.openstack_compute_flavor_v2.s1-2.id}&amp;quot;
  key_pair        = &amp;quot;vagrant-keypair&amp;quot;                      # ssh keypair name
  security_groups = [&amp;quot;default&amp;quot;]

  network {
    name = &amp;quot;Ext-Net&amp;quot;
  }
}
MacBook-Pro:ovh_public_cloud_centos_7 standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The &lt;code&gt;provider.keypair_name&lt;/code&gt; should be set to match the name of the keypair you created in the Openstack Horizon UI in the second blog post. The &lt;code&gt;provider.image&lt;/code&gt; and &lt;code&gt;provider.flavor&lt;/code&gt; values listed above are the values I used with my testing.&lt;/p&gt;

&lt;p&gt;You also might notice that all of the details in the provider block of main.tf are commented out which means they will be pulled from environment variables set in your openrc.sh file.&lt;/p&gt;

&lt;h2&gt;4. Source your openrc.sh file&lt;/h2&gt;

&lt;p&gt;Source the openrc.sh you previously downloaded for your Openstack user. Enter the password for your Openstack user when prompted. This step will export the environmental variables that the Vagrantfile needs in order to create the Openstack server.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:ovh_public_cloud_centos_7 standorsett$ source ~/Downloads/openrc.sh
Please enter your OpenStack Password:
MacBook-Pro:ovh_public_cloud_centos_7 standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;5. Download all needed Terraform providers by running &lt;code&gt;terraform init&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Terraform will automatically download all Terraform providers referenced by any .tf files by running &lt;code&gt;terraform init&lt;/code&gt;. Here is the output from when I ran this command.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:ovh_public_cloud_centos_7 standorsett$
Initializing provider plugins...

The following providers do not have any version constraints in configuration,
so the latest version was installed.

To prevent automatic upgrades to new major versions that may contain breaking
changes, it is recommended to add version = &amp;quot;...&amp;quot; constraints to the
corresponding provider blocks in configuration, with the constraint strings
suggested below.

* provider.openstack: version = &amp;quot;~&amp;gt; 1.8&amp;quot;

Terraform has been successfully initialized!

You may now begin working with Terraform. Try running &amp;quot;terraform plan&amp;quot; to see
any changes that are required for your infrastructure. All Terraform commands
should now work.

If you ever set or change modules or backend configuration for Terraform,
rerun this command to reinitialize your working directory. If you forget, other
commands will detect it and remind you to do so if necessary.
MacBook-Pro:ovh_public_cloud_centos_7 standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;6. View all the proposed Terraform changes by running &lt;code&gt;terraform plan&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;By running &lt;code&gt;terraform plan&lt;/code&gt; Terrraform will output all the details of the resources that will be created when commanded to apply the changes.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:ovh_public_cloud_centos_7 standorsett$
Refreshing Terraform state in-memory prior to plan...
The refreshed state will be used to calculate this plan, but will not be
persisted to local or remote state storage.

data.openstack_images_image_v2.centos_7: Refreshing state...
data.openstack_compute_flavor_v2.s1-2: Refreshing state...

------------------------------------------------------------------------

An execution plan has been generated and is shown below.
Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  + openstack_compute_instance_v2.basic
      id:                         &amp;lt;computed&amp;gt;
      access_ip_v4:               &amp;lt;computed&amp;gt;
      access_ip_v6:               &amp;lt;computed&amp;gt;
      all_metadata.%:             &amp;lt;computed&amp;gt;
      availability_zone:          &amp;lt;computed&amp;gt;
      flavor_id:                  &amp;quot;a64381e7-c4e7-4b01-9fbe-da405c544d2e&amp;quot;
      flavor_name:                &amp;lt;computed&amp;gt;
      force_delete:               &amp;quot;false&amp;quot;
      image_id:                   &amp;quot;e0a89ff2-e98f-4a34-afae-68f9f6c9b5ad&amp;quot;
      image_name:                 &amp;lt;computed&amp;gt;
      key_pair:                   &amp;quot;vagrant-keypair&amp;quot;
      name:                       &amp;quot;terraform_centos_7&amp;quot;
      network.#:                  &amp;quot;1&amp;quot;
      network.0.access_network:   &amp;quot;false&amp;quot;
      network.0.fixed_ip_v4:      &amp;lt;computed&amp;gt;
      network.0.fixed_ip_v6:      &amp;lt;computed&amp;gt;
      network.0.floating_ip:      &amp;lt;computed&amp;gt;
      network.0.mac:              &amp;lt;computed&amp;gt;
      network.0.name:             &amp;quot;Ext-Net&amp;quot;
      network.0.port:             &amp;lt;computed&amp;gt;
      network.0.uuid:             &amp;lt;computed&amp;gt;
      power_state:                &amp;quot;active&amp;quot;
      region:                     &amp;lt;computed&amp;gt;
      security_groups.#:          &amp;quot;1&amp;quot;
      security_groups.3814588639: &amp;quot;default&amp;quot;
      stop_before_destroy:        &amp;quot;false&amp;quot;


Plan: 1 to add, 0 to change, 0 to destroy.

------------------------------------------------------------------------

Note: You didn&amp;#39;t specify an &amp;quot;-out&amp;quot; parameter to save this plan, so Terraform
can&amp;#39;t guarantee that exactly these actions will be performed if
&amp;quot;terraform apply&amp;quot; is subsequently run.
MacBook-Pro:ovh_public_cloud_centos_7 standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;6. Apply the proposed Terraform changes by running &lt;code&gt;terraform apply -auto-approve&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;After verifying that the proposed change outputed by &lt;code&gt;terraform plan&lt;/code&gt; look good, apply those changes by running &lt;code&gt;terraform apply -auto-approve&lt;/code&gt;.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:ovh_public_cloud_centos_7 standorsett$ terraform apply -auto-approve
data.openstack_images_image_v2.centos_7: Refreshing state...
data.openstack_compute_flavor_v2.s1-2: Refreshing state...
openstack_compute_instance_v2.basic: Creating...
  access_ip_v4:               &amp;quot;&amp;quot; =&amp;gt; &amp;quot;&amp;lt;computed&amp;gt;&amp;quot;
  access_ip_v6:               &amp;quot;&amp;quot; =&amp;gt; &amp;quot;&amp;lt;computed&amp;gt;&amp;quot;
  all_metadata.%:             &amp;quot;&amp;quot; =&amp;gt; &amp;quot;&amp;lt;computed&amp;gt;&amp;quot;
  availability_zone:          &amp;quot;&amp;quot; =&amp;gt; &amp;quot;&amp;lt;computed&amp;gt;&amp;quot;
  flavor_id:                  &amp;quot;&amp;quot; =&amp;gt; &amp;quot;a64381e7-c4e7-4b01-9fbe-da405c544d2e&amp;quot;
  flavor_name:                &amp;quot;&amp;quot; =&amp;gt; &amp;quot;&amp;lt;computed&amp;gt;&amp;quot;
  force_delete:               &amp;quot;&amp;quot; =&amp;gt; &amp;quot;false&amp;quot;
  image_id:                   &amp;quot;&amp;quot; =&amp;gt; &amp;quot;e0a89ff2-e98f-4a34-afae-68f9f6c9b5ad&amp;quot;
  image_name:                 &amp;quot;&amp;quot; =&amp;gt; &amp;quot;&amp;lt;computed&amp;gt;&amp;quot;
  key_pair:                   &amp;quot;&amp;quot; =&amp;gt; &amp;quot;vagrant-keypair&amp;quot;
  name:                       &amp;quot;&amp;quot; =&amp;gt; &amp;quot;terraform_centos_7&amp;quot;
  network.#:                  &amp;quot;&amp;quot; =&amp;gt; &amp;quot;1&amp;quot;
  network.0.access_network:   &amp;quot;&amp;quot; =&amp;gt; &amp;quot;false&amp;quot;
  network.0.fixed_ip_v4:      &amp;quot;&amp;quot; =&amp;gt; &amp;quot;&amp;lt;computed&amp;gt;&amp;quot;
  network.0.fixed_ip_v6:      &amp;quot;&amp;quot; =&amp;gt; &amp;quot;&amp;lt;computed&amp;gt;&amp;quot;
  network.0.floating_ip:      &amp;quot;&amp;quot; =&amp;gt; &amp;quot;&amp;lt;computed&amp;gt;&amp;quot;
  network.0.mac:              &amp;quot;&amp;quot; =&amp;gt; &amp;quot;&amp;lt;computed&amp;gt;&amp;quot;
  network.0.name:             &amp;quot;&amp;quot; =&amp;gt; &amp;quot;Ext-Net&amp;quot;
  network.0.port:             &amp;quot;&amp;quot; =&amp;gt; &amp;quot;&amp;lt;computed&amp;gt;&amp;quot;
  network.0.uuid:             &amp;quot;&amp;quot; =&amp;gt; &amp;quot;&amp;lt;computed&amp;gt;&amp;quot;
  power_state:                &amp;quot;&amp;quot; =&amp;gt; &amp;quot;active&amp;quot;
  region:                     &amp;quot;&amp;quot; =&amp;gt; &amp;quot;&amp;lt;computed&amp;gt;&amp;quot;
  security_groups.#:          &amp;quot;&amp;quot; =&amp;gt; &amp;quot;1&amp;quot;
  security_groups.3814588639: &amp;quot;&amp;quot; =&amp;gt; &amp;quot;default&amp;quot;
  stop_before_destroy:        &amp;quot;&amp;quot; =&amp;gt; &amp;quot;false&amp;quot;
openstack_compute_instance_v2.basic: Still creating... (10s elapsed)
openstack_compute_instance_v2.basic: Creation complete after 16s (ID: 130158c7-6e13-4b79-b729-b4250ac82b04)

Apply complete! Resources: 1 added, 0 changed, 0 destroyed.

MacBook-Pro:ovh_public_cloud_centos_7 standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;7. Use &lt;code&gt;terraform show&lt;/code&gt; to display the configuration details of the deployed Openstack server&lt;/h2&gt;

&lt;p&gt;After deploying your server with &lt;code&gt;terraform apply&lt;/code&gt; you can query the configuration details by running &lt;code&gt;terraform show&lt;/code&gt;. This command will return the details of the data objects queried as well as the server deployed.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:ovh_public_cloud_centos_7 standorsett$ terraform show
data.openstack_compute_flavor_v2.s1-2:
  id = a64381e7-c4e7-4b01-9fbe-da405c544d2e
  disk = 10
  is_public = true
  name = s1-2
  ram = 2000
  rx_tx_factor = 1
  swap = 0
  vcpus = 1
data.openstack_images_image_v2.centos_7:
  id = e0a89ff2-e98f-4a34-afae-68f9f6c9b5ad
  checksum = 8abade82e1295d92c7f360f3be1029d4
  container_format = bare
  disk_format = raw
  file = /v2/images/e0a89ff2-e98f-4a34-afae-68f9f6c9b5ad/file
  metadata.% = 0
  min_disk_gb = 0
  min_ram_mb = 0
  most_recent = true
  name = Centos 7
  owner = f0dffa44438a41bb9790b67b57b72dcf
  protected = false
  schema = /v2/schemas/image
  size_bytes = 5242880000
  sort_direction = asc
  sort_key = name
  visibility = public
openstack_compute_instance_v2.basic:
  id = 130158c7-6e13-4b79-b729-b4250ac82b04
  access_ip_v4 = 147.135.76.68
  access_ip_v6 = [2604:2dc0:101:100::42]
  all_metadata.% = 0
  availability_zone = nova
  flavor_id = a64381e7-c4e7-4b01-9fbe-da405c544d2e
  flavor_name = s1-2
  force_delete = false
  image_id = e0a89ff2-e98f-4a34-afae-68f9f6c9b5ad
  image_name = Centos 7
  key_pair = vagrant-keypair
  name = terraform_centos_7
  network.# = 1
  network.0.access_network = false
  network.0.fixed_ip_v4 = 147.135.76.68
  network.0.fixed_ip_v6 = [2604:2dc0:101:100::42]
  network.0.floating_ip =
  network.0.mac = fa:16:3e:ef:3d:4c
  network.0.name = Ext-Net
  network.0.port =
  network.0.uuid = b347ed75-8603-4ce0-a40c-c6c98a8820fc
  power_state = active
  region = US-EAST-VA-1
  security_groups.# = 1
  security_groups.3814588639 = default
  stop_before_destroy = false

MacBook-Pro:ovh_public_cloud_centos_7 standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If you look back at the output of your command you will see a line displaying the &lt;code&gt;network.0.fixed_ip_v4&lt;/code&gt; IP address that you can use to connect with ssh.&lt;/p&gt;

&lt;h2&gt;8. Connect to the deployed Openstack server using ssh.&lt;/h2&gt;

&lt;p&gt;You can now connect to the IP address that is displayed with &lt;code&gt;terraform show&lt;/code&gt; using the private ssh key you created in a previous blog post.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:ovh_public_cloud_centos_7 standorsett$ ssh -i ~/.ssh/id_rsa_vagrant-keypair centos@147.135.76.49
The authenticity of host &amp;#39;147.135.76.49 (147.135.76.49)&amp;#39; can&amp;#39;t be established.
ECDSA key fingerprint is SHA256:aVSsv0mHKgIizglcvouJkvtBBOiZfNcQTpIc5oxhNO4.
ECDSA key fingerprint is MD5:91:74:c1:46:20:9c:11:0d:07:1d:f5:33:43:a9:11:74.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added &amp;#39;147.135.76.49&amp;#39; (ECDSA) to the list of known hosts.
[centos@terraform-centos-7 ~]$ hostname
terraform-centos-7
[centos@terraform-centos-7 ~]$ ip a
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether fa:16:3e:04:af:e8 brd ff:ff:ff:ff:ff:ff
    inet 147.135.76.49/32 brd 147.135.76.49 scope global dynamic eth0
       valid_lft 86355sec preferred_lft 86355sec
    inet6 fe80::f816:3eff:fe04:afe8/64 scope link
       valid_lft forever preferred_lft forever
[centos@terraform-centos-7 ~]$ exit
logout
Connection to 147.135.76.49 closed.
MacBook-Pro:ovh_public_cloud_centos_7 standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;9. Destroy your Terraform Openstack server.&lt;/h2&gt;

&lt;p&gt;After you are finished testing your Openstack server, you should destroy it in order to prevent being charged for it’s usage. This can be done using the &lt;code&gt;terraform destroy --force&lt;/code&gt; command.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:ovh_public_cloud_centos_7 standorsett$ terraform destroy --force
data.openstack_images_image_v2.centos_7: Refreshing state...
data.openstack_compute_flavor_v2.s1-2: Refreshing state...
openstack_compute_instance_v2.basic: Refreshing state... (ID: 130158c7-6e13-4b79-b729-b4250ac82b04)
openstack_compute_instance_v2.basic: Destroying... (ID: 130158c7-6e13-4b79-b729-b4250ac82b04)
openstack_compute_instance_v2.basic: Still destroying... (ID: 130158c7-6e13-4b79-b729-b4250ac82b04, 10s elapsed)
openstack_compute_instance_v2.basic: Destruction complete after 11s

Destroy complete! Resources: 1 destroyed.
MacBook-Pro:ovh_public_cloud_centos_7 standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;That all for this post covering how to use Terraform to deploy a OVH public cloud server. In this post you have only scratched the surface of what you can do with Terraform, but hopefully if showed how simple it is to get started.
Please provide any feedback or suggestions to my twitter account located on the about page.&lt;/p&gt;

&lt;hr&gt;

&lt;h3&gt;Please provide any feedback or suggestions to my twitter account located on the about page.&lt;/h3&gt;
</content>
 </entry>
 
 <entry>
   <title>Using Vagrant to deploy an OVH public cloud server.</title>
   <link href="http://0.0.0.0:4000/2018/08/19/using-vagrant-to-deploy-to-ovh-public-cloud/"/>
   <updated>2018-08-19T00:00:00+00:00</updated>
   <id>http://0.0.0.0:4000/2018/08/19/using-vagrant-to-deploy-to-ovh-public-cloud</id>
   <content type="html">&lt;p&gt;This is the fourth in a series of posts that will walk you through using the Openstack-based OVH public cloud. In this post you will get introduced to using Vagrant to create Openstack servers using the Openstack API.
This post is assumes you have already signed up for an account with ovhcloud.com, added a payment method, created a cloud project, created an Openstack user, created a ssh keypair in the Horizon UI and downloaded the openrc.sh file for this user. If you have not done these steps you can follow the steps in &lt;a href=&quot;http://sdorsett.github.io/2018/08/08/creating-an-openstack-instance-on-ovh-public-cloud/&quot;&gt;the first&lt;/a&gt; and &lt;a href=&quot;http://sdorsett.github.io/2018/08/10/using-the-openstack-cli-to-create-a-server-on-ovh-public-cloud/&quot;&gt;second blog post&lt;/a&gt; in this series that will walk you through completing those steps. &lt;/p&gt;

&lt;p&gt;I will attempt in this post to present the options that are available to OVH public cloud customers along side the choices I made that were specific to my Openstack server.
Let&amp;#39;s get started...&lt;/p&gt;

&lt;h2&gt;1. Install Vagrant&lt;/h2&gt;

&lt;p&gt;The first thing you will need to do before you can move forward with this post is install Vagrant. Go to https://www.vagrantup.com/downloads.html, download the installer for your specific operating system and install Vagrant.&lt;/p&gt;

&lt;h2&gt;2. Install the Vagrant Openstack provider&lt;/h2&gt;

&lt;p&gt;You will next need to install the Vagrant Openstack provider.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:~ standorsett$ vagrant plugin install vagrant-openstack-provider
Installing the &amp;#39;vagrant-openstack-provider&amp;#39; plugin. This can take a few minutes...
Fetching: terminal-table-1.4.5.gem (100%)
Fetching: sshkey-1.6.1.gem (100%)
Fetching: colorize-0.7.3.gem (100%)
Fetching: public_suffix-2.0.5.gem (100%)
Fetching: vagrant-openstack-provider-0.13.0.gem (100%)
Installed the plugin &amp;#39;vagrant-openstack-provider (0.13.0)&amp;#39;!
MacBook-Pro:~ standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
 

&lt;h2&gt;3. Create a new directory and add your Vagrantfile&lt;/h2&gt;

&lt;p&gt;When using the &lt;code&gt;vagrant&lt;/code&gt; command, it will look for a Vagrantfile in the current directory. As a result you should create a new directory for containing the Vagrantfile we will use for our Openstack server. &lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:~ standorsett$ mkdir -p ~/Documents/vagrant/ovh_public_cloud_centos_7
MacBook-Pro:~ standorsett$ cd ~/Documents/vagrant/ovh_public_cloud_centos_7
MacBook-Pro:ovh_public_cloud_centos_7 standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Next you will need to create a Vagrantfile in our new directory that will describe the server you are wanting to have created.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:ovh_public_cloud_centos_7 standorsett$ cat Vagrantfile
require &amp;#39;vagrant-openstack-provider&amp;#39;

Vagrant.configure(&amp;#39;2&amp;#39;) do |config|

  config.vm.define :&amp;#39;vagrant-centos7&amp;#39; do |v|

    v.ssh.username = &amp;#39;centos&amp;#39;
    v.ssh.private_key_path = &amp;#39;~/.ssh/id_rsa-ovhus_public_cloud&amp;#39;
    v.ssh.insert_key = &amp;#39;false&amp;#39;
    v.vm.synced_folder &amp;#39;.&amp;#39;, &amp;#39;/vagrant&amp;#39;, type: &amp;#39;rsync&amp;#39;
    v.vm.provider :openstack do |provider|
      provider.openstack_auth_url    = ENV[&amp;#39;OS_AUTH_URL&amp;#39;]
      provider.openstack_network_url = &amp;#39;https://network.us-east-va-1.cloud.ovh.us/v2.0&amp;#39;
      provider.identity_api_version  = ENV[&amp;#39;OS_IDENTITY_API_VERSION&amp;#39;]
      provider.username              = ENV[&amp;#39;OS_USERNAME&amp;#39;]
      provider.password              = ENV[&amp;#39;OS_PASSWORD&amp;#39;]
      provider.domain_name           = ENV[&amp;#39;OS_USER_DOMAIN_NAME&amp;#39;]
      provider.project_name          = ENV[&amp;#39;OS_TENANT_NAME&amp;#39;]
      provider.flavor                = &amp;#39;&amp;#39;
      provider.image                 = &amp;#39;&amp;#39;
      provider.keypair_name          = &amp;#39;vagrant-keypair&amp;#39;
      provider.region                = ENV[&amp;#39;OS_REGION_NAME&amp;#39;]
      provider.networks              = [ &amp;#39;Ext-Net&amp;#39; ]
    end
  end
end
MacBook-Pro:ovh_public_cloud_centos_7 standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The &lt;code&gt;v.ssh.private_key_path&lt;/code&gt; should reference the ssh private key file that was created in our second blog post. The &lt;code&gt;provider.keypair_name&lt;/code&gt; should be set to match the name of the keypair you created in the Openstack Horizon UI in the second blog post. The &lt;code&gt;provider.image&lt;/code&gt; and &lt;code&gt;provider.flavor&lt;/code&gt; values are initially blank, but you will update those shortly.&lt;/p&gt;

&lt;p&gt;You also might notice that most of the details in this Vagrantfile are referencing &lt;code&gt;ENV&lt;/code&gt; which means they will be pulled from environment variables.&lt;/p&gt;

&lt;h2&gt;4. Source your openrc.sh file&lt;/h2&gt;

&lt;p&gt;Source the openrc.sh you previously downloaded for your Openstack user. Enter the password for your Openstack user when prompted. This step will export the environmental variables that the Vagrantfile needs in order to create the Openstack server.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:ovh_public_cloud_centos_7 standorsett$ source ~/Downloads/openrc.sh
Please enter your OpenStack Password:
MacBook-Pro:ovh_public_cloud_centos_7 standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;5. Test connecting to the OVH public cloud.&lt;/h2&gt;

&lt;p&gt;You can now test connecting to the OVH public cloud API by running &lt;code&gt;vagrant status&lt;/code&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:ovh_public_cloud_centos_7 standorsett$ vagrant status
Current machine states:

vagrant-centos7           not created (openstack)

The server is not created. Run `vagrant up` to create it.
MacBook-Pro:ovh_public_cloud_centos_7 standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If you received a message that the server was not created, then your credentials and Openstack details are valid. &lt;/p&gt;

&lt;h2&gt;6. Using the Vagrant Openstack provider to query for available flavor and images.&lt;/h2&gt;

&lt;p&gt;The Vagrant Openstack provider can be used us query the Openstack API for available resources. I knew I wanted to use a Centos image so I ran &lt;code&gt;vagrant openstack image-list | egrep &amp;#39;(--|Name|Centos)&amp;#39;&lt;/code&gt; to return images that contained the name Centos:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08192018-01-vagrant-openstack-image-list.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;I also knew I wanted to use a sandbox image, which all start with s1 in the name, so I ran &lt;code&gt;vagrant openstack flavor-list | egrep &amp;#39;(--|Name|s1)&amp;#39;&lt;/code&gt; to return all s1 images:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08192018-02-vagrant-openstack-flavor-list.png&quot; alt=&quot;screenshot&quot;&gt; &lt;/p&gt;

&lt;h2&gt;6. Update your Vagrantfile to reflect the image and flavor name that you want to use.&lt;/h2&gt;

&lt;p&gt;After using the Vagrant Openstack provider to return the available image and flavor names, update your Vagrantfile to reference the image and flavor you want to use. I chose “Centos 7” and “s1-2” for mine.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:ovh_public_cloud_centos_7 standorsett$ cat Vagrantfile
require &amp;#39;vagrant-openstack-provider&amp;#39;
 
Vagrant.configure(&amp;#39;2&amp;#39;) do |config|
 
  config.vm.define :&amp;#39;vagrant-centos7&amp;#39; do |v|
 
    v.ssh.username = &amp;#39;centos&amp;#39;
    v.ssh.private_key_path = &amp;#39;~/.ssh/id_rsa-ovhus_public_cloud&amp;#39;
    v.ssh.insert_key = &amp;#39;false&amp;#39;
    v.vm.synced_folder &amp;#39;.&amp;#39;, &amp;#39;/vagrant&amp;#39;, type: &amp;#39;rsync&amp;#39;
    v.vm.provider :openstack do |provider|
      provider.openstack_auth_url    = ENV[&amp;#39;OS_AUTH_URL&amp;#39;]
      provider.openstack_network_url = &amp;#39;https://network.us-east-va-1.cloud.ovh.us/v2.0&amp;#39;
      provider.identity_api_version  = ENV[&amp;#39;OS_IDENTITY_API_VERSION&amp;#39;]
      provider.username              = ENV[&amp;#39;OS_USERNAME&amp;#39;]
      provider.password              = ENV[&amp;#39;OS_PASSWORD&amp;#39;]
      provider.domain_name           = ENV[&amp;#39;OS_USER_DOMAIN_NAME&amp;#39;]
      provider.project_name          = ENV[&amp;#39;OS_TENANT_NAME&amp;#39;]
      provider.flavor                = &amp;#39;s1-2&amp;#39;
      provider.image                 = &amp;#39;Centos 7&amp;#39;
      provider.keypair_name          = &amp;#39;vagrant-keypair&amp;#39;
      provider.region                = ENV[&amp;#39;OS_REGION_NAME&amp;#39;]
      provider.networks              = [ &amp;#39;Ext-Net&amp;#39; ]
    end
  end
end
MacBook-Pro:ovh_public_cloud_centos_7 standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;7. Run &lt;code&gt;vagrant up&lt;/code&gt; to create the Openstack server&lt;/h2&gt;

&lt;p&gt;The next step is for you to run &lt;code&gt;vagrant up&lt;/code&gt; to create the Openstack server as defined in your Vagrantfile. &lt;/p&gt;

&lt;p&gt;I want to give one warning that this command will not succeed successfully with vagrant-openstack-provider version 0.13.0, but we will address these issues in a moment. I pressed &lt;code&gt;control-c&lt;/code&gt; after receiving a few of the Host unreachable messages to break out of Vagrant.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:ovh_public_cloud_centos_7 standorsett$ vagrant up
Bringing machine &amp;#39;vagrant-centos7&amp;#39; up with &amp;#39;openstack&amp;#39; provider...
==&amp;gt; vagrant-centos7: Finding flavor for server...
==&amp;gt; vagrant-centos7: Finding image for server...
==&amp;gt; vagrant-centos7: Finding network(s) for server...
==&amp;gt; vagrant-centos7: Launching a server with the following settings...
==&amp;gt; vagrant-centos7:  -- Tenant          :
==&amp;gt; vagrant-centos7:  -- Name            : vagrant-centos7
==&amp;gt; vagrant-centos7:  -- Flavor          : s1-2
==&amp;gt; vagrant-centos7:  -- FlavorRef       : a64381e7-c4e7-4b01-9fbe-da405c544d2e
==&amp;gt; vagrant-centos7:  -- Image           : Centos 7
==&amp;gt; vagrant-centos7:  -- ImageRef        : e0a89ff2-e98f-4a34-afae-68f9f6c9b5ad
==&amp;gt; vagrant-centos7:  -- KeyPair         : vagrant-keypair
==&amp;gt; vagrant-centos7:  -- Network         : b347ed75-8603-4ce0-a40c-c6c98a8820fc
==&amp;gt; vagrant-centos7: Waiting for the server to be built...
==&amp;gt; vagrant-centos7: Waiting for machine to boot. This may take a few minutes...
    vagrant-centos7: SSH address: 2604:2dc0:101:100::1b:22
    vagrant-centos7: SSH username: centos
    vagrant-centos7: SSH auth method: private key
    vagrant-centos7: Warning: Host unreachable. Retrying...
    vagrant-centos7: Warning: Host unreachable. Retrying...
    vagrant-centos7: Warning: Host unreachable. Retrying...
^C
==&amp;gt; vagrant-centos7: Waiting for cleanup before exiting...
An unknown error happened in Vagrant OpenStack provider

To easily debug what happened, we recommend to set the environment
variable VAGRANT_OPENSTACK_LOG to debug

    $ export VAGRANT_OPENSTACK_LOG=debug

If doing this does not help fixing your issue, there may be a bug
in the provider. Please submit an issue on Github at
https://github.com/ggiamarchi/vagrant-openstack-provider
with the stracktrace and the logs.

We are looking for feedback, so feel free to ask questions or
describe features you would like to see in this provider.
An unknown error happened in Vagrant OpenStack provider

To easily debug what happened, we recommend to set the environment
variable VAGRANT_OPENSTACK_LOG to debug

    $ export VAGRANT_OPENSTACK_LOG=debug

If doing this does not help fixing your issue, there may be a bug
in the provider. Please submit an issue on Github at
https://github.com/ggiamarchi/vagrant-openstack-provider
with the stracktrace and the logs.

We are looking for feedback, so feel free to ask questions or
describe features you would like to see in this provider.
Catched Error: Catched Error: Catched Error: Vagrant exited after cleanup due to external interrupt.
MacBook-Pro:ovh_public_cloud_centos_7 standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If you look back at the output of your command you will see a line displaying a message like &lt;code&gt;SSH address: 2604:2dc0:101:100::1b:22&lt;/code&gt;
You will also see that the same IPV6 address is returned for the HostName when you run &lt;code&gt;vagrant ssh-config&lt;/code&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:ovh_public_cloud_centos_7 standorsett$ vagrant ssh-config
Host vagrant-centos7
  HostName 2604:2dc0:101:100::1b
  User centos
  Port 22
  UserKnownHostsFile /dev/null
  StrictHostKeyChecking no
  PasswordAuthentication no
  IdentityFile /Users/standorsett/.ssh/id_rsa-ovhus_public_cloud
  IdentitiesOnly yes
  LogLevel ERROR

MacBook-Pro:ovh_public_cloud_centos_7 standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;8. Updating the Vagrant Openstack provider to return an IPV4 address&lt;/h2&gt;

&lt;p&gt;There is an &lt;a href=&quot;https://github.com/ggiamarchi/vagrant-openstack-provider/pull/359&quot;&gt;open pull request&lt;/a&gt; for the Vagrant openstack provider to allow a user to specify if the provider should connect to an IPV4 or IPV6 address. Until that pull request is merge I would suggest making &lt;a href=&quot;https://github.com/sghribi/vagrant-openstack-provider/commit/040c7168066bad36a9c9995418f3189d5a85a125&quot;&gt;these suggested changes&lt;/a&gt; to &lt;code&gt;~/.vagrant.d/gems/2.4.4/gems/vagrant-openstack-provider-0.13.0/lib/vagrant-openstack-provider/utils.rb&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08192018-04-edit-util-rb-file.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;After modifying the utils.rb file try running &lt;code&gt;vagrant ssh-config&lt;/code&gt; again to make sure the command is returning an IPV4 address.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:ovh_public_cloud_centos_7 standorsett$ vagrant ssh-config
Host vagrant-centos7
  HostName 147.135.76.34
  User centos
  Port 22
  UserKnownHostsFile /dev/null
  StrictHostKeyChecking no
  PasswordAuthentication no
  IdentityFile /Users/standorsett/.ssh/id_rsa-ovhus_public_cloud
  IdentitiesOnly yes
  LogLevel ERROR
 
MacBook-Pro:ovh_public_cloud_centos_7 standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;8. Use Vagrant to destroy and re-deploy the Openstack server&lt;/h2&gt;

&lt;p&gt;Now that Vagrant is returning an IPV4 address it should deploy correctly. Destroy your Openstack server by running &lt;code&gt;vagrant destroy -f&lt;/code&gt; &lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:ovh_public_cloud_centos_7 standorsett$ vagrant destroy
==&amp;gt; vagrant-centos7: Deleting server...
==&amp;gt; vagrant-centos7: Waiting for the server to be deleted...
MacBook-Pro:ovh_public_cloud_centos_7 standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
 

&lt;p&gt;...followed by &lt;code&gt;vagrant up&lt;/code&gt; to deploy it again&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:ovh_public_cloud_centos_7 standorsett$ vagrant up
Bringing machine &amp;#39;vagrant-centos7&amp;#39; up with &amp;#39;openstack&amp;#39; provider...
==&amp;gt; vagrant-centos7: Finding flavor for server...
==&amp;gt; vagrant-centos7: Finding image for server...
==&amp;gt; vagrant-centos7: Finding network(s) for server...
==&amp;gt; vagrant-centos7: Launching a server with the following settings...
==&amp;gt; vagrant-centos7:  -- Tenant          :
==&amp;gt; vagrant-centos7:  -- Name            : vagrant-centos7
==&amp;gt; vagrant-centos7:  -- Flavor          : s1-2
==&amp;gt; vagrant-centos7:  -- FlavorRef       : a64381e7-c4e7-4b01-9fbe-da405c544d2e
==&amp;gt; vagrant-centos7:  -- Image           : Centos 7
==&amp;gt; vagrant-centos7:  -- ImageRef        : e0a89ff2-e98f-4a34-afae-68f9f6c9b5ad
==&amp;gt; vagrant-centos7:  -- KeyPair         : vagrant-keypair
==&amp;gt; vagrant-centos7:  -- Network         : b347ed75-8603-4ce0-a40c-c6c98a8820fc
==&amp;gt; vagrant-centos7: Waiting for the server to be built...
==&amp;gt; vagrant-centos7: Waiting for machine to boot. This may take a few minutes...
    vagrant-centos7: SSH address: 147.135.76.17:22
    vagrant-centos7: SSH username: centos
    vagrant-centos7: SSH auth method: private key
    vagrant-centos7: Warning: Connection refused. Retrying...
    vagrant-centos7: Warning: Connection refused. Retrying...
==&amp;gt; vagrant-centos7: Machine booted and ready!
==&amp;gt; vagrant-centos7: Installing rsync to the VM...
==&amp;gt; vagrant-centos7: Rsyncing folder: /Users/standorsett/Documents/vagrant/ovh_public_cloud_centos_7/ =&amp;gt; /vagrant
MacBook-Pro:ovh_public_cloud_centos_7 standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You can see in my output that Vagrant successfully connected to the assigned IPV4 address, installed rsync and used it to copy over the folder that contained the Vagrantfile to /vagrant.&lt;/p&gt;

&lt;h2&gt;9. Connect to your Openstack server&lt;/h2&gt;

&lt;p&gt;Connecting to your Openstack server created by Vagrant is as easy as running &lt;code&gt;vagrant ssh&lt;/code&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:ovh_public_cloud_centos_7 standorsett$ vagrant ssh
[centos@vagrant-centos7 ~]$ cd /vagrant/
[centos@vagrant-centos7 vagrant]$ ls -la
total 12
drwxr-xr-x   2 centos centos 4096 Aug 19 23:31 .
dr-xr-xr-x. 19 root   root   4096 Aug 19 23:59 ..
-rw-r--r--   1 centos centos 1063 Aug 19 23:31 Vagrantfile
[centos@vagrant-centos7 vagrant]$ cat Vagrantfile
require &amp;#39;vagrant-openstack-provider&amp;#39;
 
Vagrant.configure(&amp;#39;2&amp;#39;) do |config|
 
  config.vm.define :&amp;#39;vagrant-centos7&amp;#39; do |v|
 
    v.ssh.username = &amp;#39;centos&amp;#39;
    v.ssh.private_key_path = &amp;#39;~/.ssh/id_rsa-ovhus_public_cloud&amp;#39;
    v.ssh.insert_key = &amp;#39;false&amp;#39;
    v.vm.synced_folder &amp;#39;.&amp;#39;, &amp;#39;/vagrant&amp;#39;, type: &amp;#39;rsync&amp;#39;
    v.vm.provider :openstack do |provider|
      provider.openstack_auth_url    = ENV[&amp;#39;OS_AUTH_URL&amp;#39;]
      provider.openstack_network_url = &amp;#39;https://network.us-east-va-1.cloud.ovh.us/v2.0&amp;#39;
      provider.identity_api_version  = ENV[&amp;#39;OS_IDENTITY_API_VERSION&amp;#39;]
      provider.username              = ENV[&amp;#39;OS_USERNAME&amp;#39;]
      provider.password              = ENV[&amp;#39;OS_PASSWORD&amp;#39;]
      provider.domain_name           = ENV[&amp;#39;OS_USER_DOMAIN_NAME&amp;#39;]
      provider.project_name          = ENV[&amp;#39;OS_TENANT_NAME&amp;#39;]
      provider.flavor                = &amp;#39;s1-2&amp;#39;
      provider.image                 = &amp;#39;Centos 7&amp;#39;
      provider.keypair_name          = &amp;#39;vagrant-keypair&amp;#39;
      provider.region                = ENV[&amp;#39;OS_REGION_NAME&amp;#39;]
      provider.networks              = [ &amp;#39;Ext-Net&amp;#39; ]
    end
  end
end
[centos@vagrant-centos7 vagrant]$ ip a show dev eth0
2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether fa:16:3e:79:76:78 brd ff:ff:ff:ff:ff:ff
    inet 147.135.76.17/32 brd 147.135.76.17 scope global dynamic eth0
       valid_lft 85791sec preferred_lft 85791sec
    inet6 fe80::f816:3eff:fe79:7678/64 scope link
       valid_lft forever preferred_lft forever
[centos@vagrant-centos7 vagrant]$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;10. Destroy your Vagrant Openstack server.&lt;/h2&gt;

&lt;p&gt;After you are finished testing your Openstack server, you should destroy it in order to prevent being charged for it’s usage. This can be done using the same &lt;code&gt;vagrant destroy&lt;/code&gt; command we did earlier.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:ovh_public_cloud_centos_7 standorsett$ vagrant destroy
==&amp;gt; vagrant-centos7: Deleting server...
==&amp;gt; vagrant-centos7: Waiting for the server to be deleted...
MacBook-Pro:ovh_public_cloud_centos_7 standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
 

&lt;p&gt;That all for this post covering how to use Vagrant to quickly deploy a OVH public cloud server. In this post you have only scratched the surface of what you can do with Vagrant, but hopefully if showed how simple it is to get started.
Please provide any feedback or suggestions to my twitter account located on the about page.&lt;/p&gt;

&lt;hr&gt;

&lt;h3&gt;Please provide any feedback or suggestions to my twitter account located on the about page.&lt;/h3&gt;
</content>
 </entry>
 
 <entry>
   <title>Using rclone to manage OVH Public Cloud swift storage</title>
   <link href="http://0.0.0.0:4000/2018/08/14/using-rclone-to-connect-to-ovh-public-cloud-swift-storage/"/>
   <updated>2018-08-14T00:00:00+00:00</updated>
   <id>http://0.0.0.0:4000/2018/08/14/using-rclone-to-connect-to-ovh-public-cloud-swift-storage</id>
   <content type="html">&lt;p&gt;This is the third in a series of posts that will walk you through using the Openstack-based OVH public cloud. In this post you will get introduced to using rclone to upload and retrieve files from the Openstack swift API. &lt;/p&gt;

&lt;p&gt;This post is assumes you have already signed up for an account with ovhcloud.com, added a payment method, created a cloud project and created an Openstack user. If you have not done these steps you can follow the steps in &lt;a href=&quot;http://sdorsett.github.io/2018/08/08/creating-an-openstack-instance-on-ovh-public-cloud/&quot;&gt;the first&lt;/a&gt; and &lt;a href=&quot;http://sdorsett.github.io/2018/08/10/using-the-openstack-cli-to-create-a-server-on-ovh-public-cloud/&quot;&gt;second blog post&lt;/a&gt; in this series that will walk you through completing those steps.&lt;/p&gt;

&lt;p&gt;I will attempt in this post to present the options that are available to OVH public cloud customers along side the choices I made that were specific to my Openstack server.&lt;/p&gt;

&lt;p&gt;Let&amp;#39;s get started...&lt;/p&gt;

&lt;hr&gt;

&lt;h2&gt;1. Sign into &lt;a href=&quot;https://ovhcloud.com/auth/&quot;&gt;ovhcloud.com&lt;/a&gt;.&lt;/h2&gt;

&lt;p&gt;The first thing you will need to do before you can move forward with this post is sign into &lt;a href=&quot;https://ovhcloud.com/auth/&quot;&gt;https://ovhcloud.com/auth/&lt;/a&gt; using the account you created in the first blog post..&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08102018-01-login-username-password.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;If you setup two-factor authentication, you will be prompted to enter the current two-factor token.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08102018-02-login-two-factor.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;h2&gt;2. Create a object container.&lt;/h2&gt;

&lt;p&gt;The first step to store files in your OVH Public Cloud will be to create a swift object container to store the files. Object containers are managed by clicking the &lt;code&gt;storage&lt;/code&gt; tab of the cloud project you want to store files in.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08142018-01-openstack-storage-empty.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;If you do not have any object containers create, you can create one by clicking the &lt;code&gt;Create a container&lt;/code&gt; button.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08142018-02-openstack-create-object-container.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;Select the datacenter you want to create the object container in.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08142018-03-openstack-create-object-container.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;Select what purpose for which you are intending to use the object container storage. I am intending to test file that need to be publicly available, without any sort of authentication, so I chose &lt;code&gt;Public&lt;/code&gt;. &lt;/p&gt;

&lt;p&gt;Name the object container a name that reflects it&amp;#39;s purpose and click &lt;code&gt;Create the container&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08142018-04-openstack-create-object-container.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;You will next be taken to a view that lists the details of the newly created object container.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08142018-05-openstack-create-object-container.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;h2&gt;3. Install rclone&lt;/h2&gt;

&lt;p&gt;Rclone is self described as &amp;#39;a command line program to sync files and directories to and from cloud storage.&amp;#39; It is opensource and is compatible with Openstack swift, so it will work with OVH Public Cloud object containers.&lt;/p&gt;

&lt;p&gt;The first thing you need to do is download the latest version for your operating system from the &lt;a href=&quot;https://rclone.org/downloads/&quot;&gt;rclone download page&lt;/a&gt;, unzip the binary and place it within our path.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:~ standorsett$ cd ~/Downloads/
MacBook-Pro:Downloads standorsett$ wget https://downloads.rclone.org/v1.42/rclone-v1.42-osx-amd64.zip
--2018-08-14 13:50:18--  https://downloads.rclone.org/v1.42/rclone-v1.42-osx-amd64.zip
Resolving downloads.rclone.org (downloads.rclone.org)... 5.153.250.7
Connecting to downloads.rclone.org (downloads.rclone.org)|5.153.250.7|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 9748516 (9.3M) [application/zip]
Saving to: ‘rclone-v1.42-osx-amd64.zip’

rclone-v1.42-osx-amd64.zip                    100%[==============================================================================================&amp;gt;]   9.30M   574KB/s    in 37s

2018-08-14 13:50:56 (255 KB/s) - ‘rclone-v1.42-osx-amd64.zip’ saved [9748516/9748516]

MacBook-Pro:Downloads standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;...unzip the binary,&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:Downloads standorsett$ unzip rclone-v1.42-osx-amd64.zip
Archive:  rclone-v1.42-osx-amd64.zip
   creating: rclone-v1.42-osx-amd64/
  inflating: rclone-v1.42-osx-amd64/README.html
  inflating: rclone-v1.42-osx-amd64/rclone
  inflating: rclone-v1.42-osx-amd64/rclone.1
  inflating: rclone-v1.42-osx-amd64/README.txt
MacBook-Pro:Downloads standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;...and place it within a folder that is exported in your path.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:Downloads standorsett$ sudo cp rclone-v1.42-osx-amd64/rclone /usr/local/bin/
MacBook-Pro:Downloads standorsett$ which rclone
/usr/local/bin/rclone
MacBook-Pro:Downloads standorsett$ rclone --version
rclone v1.42
- os/arch: darwin/amd64
- go version: go1.10.1
MacBook-Pro:Downloads standorsett$
MacBook-Pro:Downloads standorsett$ cd ~/
MacBook-Pro:~ standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;4. Source the openrc.sh file you downloaded in the &lt;a href=&quot;http://sdorsett.github.io/2018/08/10/using-the-openstack-cli-to-create-a-server-on-ovh-public-cloud/&quot;&gt;previous blog post&lt;/a&gt;.&lt;/h2&gt;

&lt;p&gt;The simplest way to use rclone is to source the openrc.sh file you downloaded in the previous blog post. This will set environment variables that the rclone cli will use to connect to the OVH public cloud. When you source the openrc.sh file you will be prompted for the password of the Openstack user listed in this file.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:~ standorsett$ source ~/Downloads/openrc.sh
Please enter your OpenStack Password:
MacBook-Pro:~ standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;5. Export two RCLONE_CONFIG environment variables&lt;/h2&gt;

&lt;p&gt;As mentioned on the &lt;a href=&quot;https://rclone.org/swift/#using-an-alternate-authentication-method&quot;&gt;rclone swift documentation&lt;/a&gt; you can use rclone with swift without a config file by exporting two environment variables.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:~ standorsett$ export RCLONE_CONFIG_MYREMOTE_TYPE=swift
MacBook-Pro:~ standorsett$ export RCLONE_CONFIG_MYREMOTE_ENV_AUTH=true
MacBook-Pro:~ standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;6. Using rclone to list remote object containers.&lt;/h2&gt;

&lt;p&gt;You can now list all remove object containers by running the command &lt;code&gt;rclone lsd myremote:&lt;/code&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:~ standorsett$ export RCLONE_CONFIG_MYREMOTE_TYPE=swift
MacBook-Pro:~ standorsett$ export RCLONE_CONFIG_MYREMOTE_ENV_AUTH=true
MacBook-Pro:~ standorsett$ rclone lsd myremote:
2018/08/14 13:23:16 NOTICE: Config file &amp;quot;/Users/standorsett/.config/rclone/rclone.conf&amp;quot; not found - using defaults
           0 2018-08-14 13:23:18         0 public_storage_container
MacBook-Pro:~ standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If you get tired of seeing the &amp;#39;rclone.conf not found&amp;#39; informational message, you can get rid of it by creating an empty config file with the same name mentioned.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:~ standorsett$ touch /Users/standorsett/.config/rclone/rclone.conf
MacBook-Pro:~ standorsett$ rclone lsd myremote:
           0 2018-08-14 13:23:40         0 public_storage_container
MacBook-Pro:~ standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You can see the command I ran returned the &lt;code&gt;public_storage_container&lt;/code&gt; object container I created in step 2.&lt;/p&gt;

&lt;h2&gt;6. Using rclone to upload files to the object container.&lt;/h2&gt;

&lt;p&gt;Now that you have confirmed rclone is seeing to the OVH Public Cloud object container you created in step 2, you can upload a file to it. I decided to test with the rclone zip file for OSX that I downlaoded in step 3.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:~ standorsett$ rclone copy ~/Downloads/rclone-v1.42-osx-amd64.zip myremote:public_storage_container
MacBook-Pro:~ standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You can now run &lt;code&gt;rclone ls myremote:[object_container_name]&lt;/code&gt; to list the contents of your object container. Here is the output of when I ran that command. I also decided to output the size of the file on the local disk to compare the two. Both the local and remote files contain the same number of bytes.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:~ standorsett$ rclone ls myremote:public_storage_container
  9748516 rclone-v1.42-osx-amd64.zip
MacBook-Pro:~ standorsett$ ls -la ~/Downloads/rclone-v1.42-osx-amd64.zip
-rw-r--r--@ 1 standorsett  staff  9748516 Jun 16 12:22 /Users/standorsett/Downloads/rclone-v1.42-osx-amd64.zip
MacBook-Pro:~ standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The file sizes of the local and remote files matched, but I also wanted to be safe and compare the local MD5 hashsum&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:~ standorsett$ md5 ~/Downloads/rclone-v1.42-osx-amd64.zip
MD5 (/Users/standorsett/Downloads/rclone-v1.42-osx-amd64.zip) = 6fc9f13129bd890164be65bc11f6c870
MacBook-Pro:~ standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;...to the remote hashsum&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:~ standorsett$ rclone hashsum MD5 myremote:public_storage_container/rclone-v1.42-osx-amd64.zip
6fc9f13129bd890164be65bc11f6c870  rclone-v1.42-osx-amd64.zip
MacBook-Pro:~ standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;...and verified that the two hashsums matched.&lt;/p&gt;

&lt;h2&gt;7. Downloading the file from the public object container.&lt;/h2&gt;

&lt;p&gt;If you created a public object container, all the files contained within the object container can be downloaded without providing any authentication. You should alway consider the purpose of the files being uploaded to the object container when deciding if it should be private or public.&lt;/p&gt;

&lt;p&gt;You can test that the files can be access without any authentication. First confirm the file is visible in the ovhcloud.com UI.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08142018-06-confirming-file-uploaded-in-ui.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;You can then paste the &lt;code&gt;Container URL&lt;/code&gt; displayed in the object container in a browser and see all the objects contained in it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08142018-07-confirming-file-uploaded-in-browser.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;The final test you can do is to add the file name you uploaded to the container URL and test downloading the file.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:~ standorsett$ wget https://storage.us-east-va-1.cloud.ovh.us/v1/AUTH_2897923a654b40d0b8b36a502bda4a8f/public_storage_container/rclone-v1.42-osx-amd64.zip
--2018-08-14 22:21:38--  https://storage.us-east-va-1.cloud.ovh.us/v1/AUTH_2897923a654b40d0b8b36a502bda4a8f/public_storage_container/rclone-v1.42-osx-amd64.zip
Resolving storage.us-east-va-1.cloud.ovh.us (storage.us-east-va-1.cloud.ovh.us)... 147.135.3.101
Connecting to storage.us-east-va-1.cloud.ovh.us (storage.us-east-va-1.cloud.ovh.us)|147.135.3.101|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 9748516 (9.3M) [application/zip]
Saving to: ‘rclone-v1.42-osx-amd64.zip’

rclone-v1.42-osx-amd64.zip                    100%[==============================================================================================&amp;gt;]   9.30M  2.45MB/s    in 4.3s

2018-08-14 22:21:42 (2.16 MB/s) - ‘rclone-v1.42-osx-amd64.zip’ saved [9748516/9748516]

MacBook-Pro:~ standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;8. Delete the uploaded file with rclone.&lt;/h2&gt;

&lt;p&gt;Once you are finished with your test files, you should delete them to prevent being charged for their usage. You can delete the files by running the rclone command of &lt;code&gt;rclone deletefile remote:path&lt;/code&gt;. Here is the output of when I ran this command and then listed out the files in my object container to verify it had been deleted..&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:~ standorsett$ rclone deletefile myremote:public_storage_container/rclone-v1.42-osx-amd64.zip
MacBook-Pro:~ standorsett$ rclone ls myremote:public_storage_container
MacBook-Pro:~ standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;hr&gt;

&lt;h3&gt;That all for this post covering how to use the rclone utility to manage your OVH public cloud swift storage.&lt;/h3&gt;

&lt;h3&gt;Please provide any feedback or suggestions to my twitter account located on the about page.&lt;/h3&gt;
</content>
 </entry>
 
 <entry>
   <title>Using the Openstack cli to create a server on OVH public cloud</title>
   <link href="http://0.0.0.0:4000/2018/08/10/using-the-openstack-cli-to-create-a-server-on-ovh-public-cloud/"/>
   <updated>2018-08-10T00:00:00+00:00</updated>
   <id>http://0.0.0.0:4000/2018/08/10/using-the-openstack-cli-to-create-a-server-on-ovh-public-cloud</id>
   <content type="html">&lt;p&gt;This is the second in a series of posts that will walk you through using the Openstack-based OVH public cloud. In this post you will get introduced to using the Openstack cli to create an Openstack server.&lt;/p&gt;

&lt;p&gt;This post is assumes you have already signed up for an account with ovhcloud.com, added a payment method and created a cloud project. If you have not done these steps you can follow &lt;a href=&quot;http://sdorsett.github.io/2018/08/08/creating-an-openstack-instance-on-ovh-public-cloud/&quot;&gt;the first blog post&lt;/a&gt; in this series that will walk you through completing those steps. &lt;/p&gt;

&lt;p&gt;I will attempt in this post to present the options that are available to OVH public cloud customers along side the choices I made that were specific to my Openstack server.&lt;/p&gt;

&lt;p&gt;Let&amp;#39;s get started...&lt;/p&gt;

&lt;hr&gt;

&lt;h2&gt;1. Sign into &lt;a href=&quot;https://ovhcloud.com/auth/&quot;&gt;ovhcloud.com&lt;/a&gt;.&lt;/h2&gt;

&lt;p&gt;The first thing you will need to do before you can move forward with this post is sign into &lt;a href=&quot;https://ovhcloud.com/auth/&quot;&gt;https://ovhcloud.com/auth/&lt;/a&gt; using the account you created in the first blog post..&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08102018-01-login-username-password.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;If you setup two-factor authentication, you will be prompted to enter the current two-factor token.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08102018-02-login-two-factor.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;h2&gt;2. Create a new user account to use with the Openstack API.&lt;/h2&gt;

&lt;p&gt;The Openstack API will not use the credentials you used to log into the ovhcloud.com site, but will instead use an Openstack user to create, modify or view objects in an OVH cloud project.&lt;/p&gt;

&lt;p&gt;If have not created one already, you will need to create an Openstack user account within the cloud project you want to work with. You can display all the Openstack user accounts that have been created in a cloud project by clicking the &amp;#39;Openstack&amp;#39; tab within any cloud project. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08102018-03-openstack-users.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;Create a new user by clicking the &amp;quot;Add user&amp;quot; button.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08102018-04-openstack-add-user.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;Once you have specified the username and clicked &amp;quot;Confirm&amp;quot; you will be taken back to the Openstack tab. You will see the user you created and will also be able to see the password generated for this user for a short period of time. Make sure you record this password to a safe place since it will not be visible for very long. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08102018-05-openstack-users-password-shown.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;After a period of time you will no longer be able to see the password for the user you created, but If you do happen to forget the password, you can always have a new password regenerated.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08102018-14-openstack-users-password-hidden.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;h2&gt;3. Download an Openstack configuration file.&lt;/h2&gt;

&lt;p&gt;Now that you have an Openstack user created you can download an openrc.sh file that will contain all the details need by the Openstack cli to connect as this user. Click the &amp;#39;...&amp;#39; icon and the end of the line for the user you want to download a configuration file for and select &amp;#39;Downloading an Openstack configuration file&amp;#39;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08102018-06-download-openrc.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;Select the region (datacenter) you want to connect to and click &amp;#39;confirm&amp;#39;. Your browser will next prompt you where you want to save the downloaded openrc.sh file.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08102018-07-download-openrc-region.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;If you look at the downloaded file you will see that it sets environment variables containing the Openstack details needed to connect to your cloud project.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:Desktop standorsett$ cat ~/Downloads/openrc.sh
#!/bin/bash

# To use an Openstack cloud you need to authenticate against keystone, which
# returns a **Token** and **Service Catalog**. The catalog contains the
# endpoint for all services the user/tenant has access to - including nova,
# glance, keystone, swift.
#
export OS_AUTH_URL=https://auth.cloud.ovh.us/v3/
export OS_IDENTITY_API_VERSION=3

export OS_USER_DOMAIN_NAME=${OS_USER_DOMAIN_NAME:-&amp;quot;Default&amp;quot;}
export OS_PROJECT_DOMAIN_NAME=${OS_PROJECT_DOMAIN_NAME:-&amp;quot;Default&amp;quot;}


# With the addition of Keystone we have standardized on the term **tenant**
# as the entity that owns the resources.
export OS_TENANT_ID=2897923a654b40d0b8b36a502bda4a8f
export OS_TENANT_NAME=&amp;quot;3543682553721111&amp;quot;

# In addition to the owning entity (tenant), openstack stores the entity
# performing the action as the **user**.
export OS_USERNAME=&amp;quot;5r7jJyuwwwsv&amp;quot;

# With Keystone you pass the keystone password.
echo &amp;quot;Please enter your OpenStack Password: &amp;quot;
read -sr OS_PASSWORD_INPUT
export OS_PASSWORD=$OS_PASSWORD_INPUT

# If your configuration has multiple regions, we set that information here.
# OS_REGION_NAME is optional and only valid in certain environments.
export OS_REGION_NAME=&amp;quot;US-EAST-VA-1&amp;quot;
# Do not leave a blank variable, unset it if it was empty
if [ -z &amp;quot;$OS_REGION_NAME&amp;quot; ]; then unset OS_REGION_NAME; fi
MacBook-Pro:Desktop standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The only piece of information that this file does not contain is your password, which it will prompt for when you source this file.&lt;/p&gt;

&lt;h2&gt;4. Log into the Openstack Horizon UI.&lt;/h2&gt;

&lt;p&gt;Just like you needed to create an Openstack user for interacting with the Openstack API, you will need to create a ssh keypair used for connecting to Openstack servers. This can be done within the Openstack Horizon UI. &lt;/p&gt;

&lt;p&gt;Click the &lt;code&gt;...&lt;/code&gt; icon and the end of the line for the user you want to use for interacting with the Openstack API and select &lt;code&gt;Launch Openstack Horizon&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08102018-08-launch-openstack-horizon.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;This will open another browser tab for the Openstack Horizon UI with your username already filled in. Provide the password for this user and click &lt;code&gt;connect&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08102018-09-login-openstack-horizon.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;h2&gt;5. Create a ssh key-pair in the Openstack Horizon UI.&lt;/h2&gt;

&lt;p&gt;We need to add a new ssh keypair that the Openstack API can add to servers that are created through the API. To keep things simple I added the same public ssh key that I generated in the first blog post.&lt;/p&gt;

&lt;p&gt;To add a new ssh key pair go to &lt;code&gt;Project | Compute | Key pairs | Create Key Pair&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08102018-10-openstack-horizon-key-pairs.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;Give the key pair a friendly name you can remember and paste the contents of the public key file. Click &lt;code&gt;Import Key Pair&lt;/code&gt; to add this key pair.  &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08102018-12-openstack-horizon-import-key-pair.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;The key pair you just added will now be visible in the Horizon UI.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08102018-13-openstack-horizon-key-pairs.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;h2&gt;6. Installing the Openstack cli.&lt;/h2&gt;

&lt;p&gt;You will need to install the openstack cli on the operating system you are using. You can easily find instruction for installing it on whatever OS you prefer to use. I quickly installed it on my Mac laptop with homebrew using the following commands.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;brew install python2
pip2 install --upgrade pip setuptools
pip2 install --upgrade python-openstackclient&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;7. Sourcing the openrc.sh file&lt;/h2&gt;

&lt;p&gt;The first step to using the Openstack cli you just installed is to source the openrc.sh file you downloaded in step 3. This will set environment variables that the Openstack cli will use to connect to the OVH public cloud. When you source the openrc.sh file you will be prompted for the password of the Openstack user listed in this file.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:~ standorsett$ source ~/Downloads/openrc.sh
Please enter your OpenStack Password:
MacBook-Pro:~ standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If you take a look at your exported environmental variables after sourcing the openrc.sh file, you will see the values from the opensh.rc file set along with your password.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:~ standorsett$ export | grep OS_
declare -x OS_AUTH_URL=&amp;quot;https://auth.cloud.ovh.us/v3/&amp;quot;
declare -x OS_IDENTITY_API_VERSION=&amp;quot;3&amp;quot;
declare -x OS_PASSWORD=&amp;quot;*******************************&amp;quot;
declare -x OS_PROJECT_DOMAIN_NAME=&amp;quot;Default&amp;quot;
declare -x OS_REGION_NAME=&amp;quot;US-EAST-VA-1&amp;quot;
declare -x OS_TENANT_ID=&amp;quot;2897923a654b40d0b8b36a502bda4a8f&amp;quot;
declare -x OS_TENANT_NAME=&amp;quot;3543682553721111&amp;quot;
declare -x OS_USERNAME=&amp;quot;5r7jJyuwwwsv&amp;quot;
declare -x OS_USER_DOMAIN_NAME=&amp;quot;Default&amp;quot;
MacBook-Pro:~ standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;8. Using the Openstack cli to create an Openstack server&lt;/h2&gt;

&lt;p&gt;Now that you have sourced the openrc.sh file you can create an Openstack server using the Openstack cli.&lt;/p&gt;

&lt;p&gt;The four pieces of information you will need to create a new server are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The ssh key pair name that will be allowed to connect.&lt;/li&gt;
&lt;li&gt;The operating system (image) you want to use.&lt;/li&gt;
&lt;li&gt;The size of the server (flavor) you want to use.&lt;/li&gt;
&lt;li&gt;The security group you want to have applied to the server.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;First list the ssh key pairs so you can see the key pair you created in step 5 by running &lt;code&gt;openstack keypair list&lt;/code&gt;. You will need the name of this keypair shortly.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08102018-16-openstack-cli-keypair-list.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;Next list the available operating system images by running &lt;code&gt;openstack image list&lt;/code&gt;. You will need the ID of the image you want to use&lt;/p&gt;

&lt;p&gt;I noted &lt;code&gt;e0a89ff2-e98f-4a34-afae-68f9f6c9b5ad&lt;/code&gt; for the ID of the Centos 7 image I wanted to use in my test.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08102018-17-openstack-cli-image-list.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;Next list the available server sizes or flavors by running &lt;code&gt;openstack flavor list&lt;/code&gt;. You will need the ID of the flavor of the server you want to create. I wanted to use a &lt;code&gt;s1-2&lt;/code&gt; flavor image since it was the smallest sandbox server with the lowest hourly rate, so I noted it&amp;#39;s ID was &lt;code&gt;a64381e7-c4e7-4b01-9fbe-da405c544d2e&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08102018-18-openstack-cli-flavor-list.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;Finally list the security groups by running &lt;code&gt;openstack security group list&lt;/code&gt;. The default security group which allows all inbound and outbound traffic is named &lt;code&gt;default&lt;/code&gt;. I decided to use this default security group for my test rather than create a new one that was more restrictive.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08102018-19-openstack-cli-security-group-list.png&quot; alt=&quot;screenshot&quot;&gt;
The default security group is named &lt;code&gt;default&lt;/code&gt; so you will need to specify it when creating a server..&lt;/p&gt;

&lt;p&gt;Now knowing the flavor id, image id, keypair name and security group name you are ready to create a new openstack server using a command like the following&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;openstack server create --flavor a64381e7-c4e7-4b01-9fbe-da405c544d2e --image e0a89ff2-e98f-4a34-afae-68f9f6c9b5ad --key-name ovhus_public_cloud --security-group default openstack_cli_test
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;p&gt;Below is a screenshot of me running that command.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08102018-20-openstack-cli-server-create.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;After the Openstack server has been created you can run &lt;code&gt;openstack server list&lt;/code&gt; to show the details of the server you just created. The networks field will show the IP address of the server.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08102018-21-openstack-cli-server-list.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;Now that you know the IP address of the server you just created, You can test connecting to this server using the ssh key you created in the previous blog post.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08102018-22-openstack-cli-server-ssh.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;h2&gt;9. Validating the Openstack server in ovhcloud.com UI&lt;/h2&gt;

&lt;p&gt;You can check the ovhcloud.com UI to confirm the server has been created and the IP address is the same as what &lt;code&gt;openstack server list&lt;/code&gt; displayed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08102018-23-ovhcloud-ui-server-created.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;h2&gt;10. Deleting the Openstack server using the Openstack cli.&lt;/h2&gt;

&lt;p&gt;Once you are finished with your openstack server, you should delete the server to prevent being charged for it&amp;#39;s usage. Run the following command in the Openstack cli to delete the server.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08102018-24-openstack-cli-server-delete.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;Running &lt;code&gt;openstack server list&lt;/code&gt; will show that the server has been sucessfully deleted. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08102018-25-openstack-cli-server-list.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;You can also check the ovhcloud.com UI to confirm the server has been deleted.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08102018-26-ovhcloud-ui-server-deleted.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;hr&gt;

&lt;h3&gt;That all for this post covering how to use the Openstack cli utility to create servers on the OVH public cloud.&lt;/h3&gt;

&lt;h3&gt;Please provide any feedback or suggestions to my twitter account located on the about page.&lt;/h3&gt;
</content>
 </entry>
 
 <entry>
   <title>Manually creating an openstack server on OVH public cloud</title>
   <link href="http://0.0.0.0:4000/2018/08/08/creating-an-openstack-server-on-ovh-public-cloud/"/>
   <updated>2018-08-08T00:00:00+00:00</updated>
   <id>http://0.0.0.0:4000/2018/08/08/creating-an-openstack-server-on-ovh-public-cloud</id>
   <content type="html">&lt;p&gt;This is the first in a series of posts that will walk you through using the openstack-based OVH public cloud. In this post you will get introduced to using the ovhcloud.com UI to create an Openstack server.&lt;/p&gt;

&lt;p&gt;I will attempt in this post to present the options that are available to OVH public cloud customers along side the choices I made that were specific to my server.&lt;/p&gt;

&lt;p&gt;Let&amp;#39;s get started...&lt;/p&gt;

&lt;hr&gt;

&lt;h2&gt;1. Create an account at &lt;a href=&quot;https://ovhcloud.com/auth/&quot;&gt;ovhcloud.com&lt;/a&gt;.&lt;/h2&gt;

&lt;p&gt;The first thing you will need to do before you can create OVH public cloud servers is setup an account by going to &lt;a href=&quot;https://ovhcloud.com/auth/&quot;&gt;https://ovhcloud.com/auth/&lt;/a&gt; and clicking the &amp;#39;Create an account&amp;#39; button. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08082018-01-create-an-account.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;After providing my email address, specifying a complex password and adding my billing address I received an email within minutes that my account had been activated.&lt;/p&gt;

&lt;h2&gt;2. Setup two-factor authentication.&lt;/h2&gt;

&lt;p&gt;The second thing you should do after creating an account is setup two-factor authentication. Enabling two-factor authentication will require that you provide a code (token) from your two-factor authentication application on your phone or physical device, along with your username and password when you log into ovhcloud.com. I would highly recommend setting up two-factor authentication to help prevent unauthorized access to your OVH account.&lt;/p&gt;

&lt;p&gt;I found the option to enable two-factor authentication by going to My Account | Security | Two-factor authentication | activate double authentication&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08082018-02-activate-two-factor-auth.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;OVH supports mobile applications and physical devices for two-factor authentication. I selected mobile application since I would be using Google Authenticator.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08082018-04-activate-two-factor-auth.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;Next I scanned the QR code on my phone, provided a code (token) generated by the Google Authenticator app and gave it a friendly name.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08082018-05-activate-two-factor-auth.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;After clicking next you will be given emergency codes that can be used if you lose your mobile app or physical two-factor device. Copy these emergency codes and store them in a safe place since they can never be retrieved again. After clicking next the two-factor authentication option will show as being enabled.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08082018-06-activate-two-factor-auth.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;h2&gt;3. Adding a payment method.&lt;/h2&gt;

&lt;p&gt;You will now need to add a payment method, since it will be used to purchase cloud credits in the next step. I added a payment method by clicking my username at the upper right, clicking &amp;#39;My payment methods&amp;#39; and finally clicking the &amp;#39;Add a payment method&amp;#39; button. Provide your card number, experation date and security code and click the &amp;#39;Add&amp;#39; button.&lt;/p&gt;

&lt;p&gt;Once your payment method shows a &amp;quot;Confirmed&amp;quot; status you are ready to proceed to the next step.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08082018-07-add-payment-method.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;h2&gt;4. Creating a public cloud project.&lt;/h2&gt;

&lt;p&gt;The next step you need to do is create a cloud project. To do this I went to order dropdown and clicked &amp;#39;Cloud project&amp;#39;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08082018-08-create-cloud-project.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;Name your project and click &amp;#39;launch your cloud project&amp;#39;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08082018-09-create-cloud-project.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;On the next screen you will be purchasing credits that your running servers will consume.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08082018-10-create-cloud-project.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;Accept the terms and conditions by clicking the check box. Your cloud project will be ordered and payment method charged once you click the &amp;#39;Validate and pay&amp;#39; button. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08082018-11-create-cloud-project.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;I received an email from OVH within 5 minutes of ordering to tell me that my cloud project had been created.&lt;/p&gt;

&lt;h2&gt;5. Creating a new ssh key for OVH public cloud to use.&lt;/h2&gt;

&lt;p&gt;You will need to generate a ssh key to connect to the OVH openstack servers. On my mac laptop I ran the following command to generate a new ssh key for this purpose.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;ssh-keygen -b 4096 -t rsa -C &amp;quot;stan.dorsett+ovhus_public_cloud@gmail.com&amp;quot; -f ~/.ssh/id_rsa-ovhus_public_cloud&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;-b&lt;/code&gt; option will specify the number of bits in the ssh key.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;-t&lt;/code&gt; option with specify the type of ssh key generated.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;-C&lt;/code&gt; option will add a comment to the end of the public key file.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;-f&lt;/code&gt; option will specify the file that the private key will be saved to. &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When running the command you will also get asked you if you want to add a passphrase that will need to be provided when you use the ssh key. I didn&amp;#39;t choose to set a passphrase, but you can choose to if you feel the need.&lt;/p&gt;

&lt;p&gt;Here is the output from my running of this command.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:~ standorsett$ ssh-keygen -b 4096 -t rsa -C &amp;quot;stan.dorsett+ovhus_public_cloud@gmail.com&amp;quot; -f ~/.ssh/id_rsa-ovhus_public_cloud
Generating public/private rsa key pair.
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /Users/standorsett/.ssh/id_rsa-ovhus_public_cloud.
Your public key has been saved in /Users/standorsett/.ssh/id_rsa-ovhus_public_cloud.pub.
The key fingerprint is:
SHA256:7kSErA6S8zCIq0H/2hmcxE4+JUsW6ZmoDPIE/ULM8I4 stan.dorsett+ovhus_public_cloud@gmail.com
The key&amp;#39;s randomart image is:
+---[RSA 4096]----+
|.                |
| *   ...         |
|. *  oo .        |
|o* .+.+.         |
|E.=.o@ .S        |
|=Xo+O =o         |
|ooo..O  o        |
|..  o +o         |
|.  ..+  .        |
+----[SHA256]-----+
MacBook-Pro:~ standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Output the public key so you can use it in the next step.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:~ standorsett$ cat /Users/standorsett/.ssh/id_rsa-ovhus_public_cloud.pub
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDJ8LcFN8MRO/uwgVcdDg0grv
...lots of more characters here...
l0nXM7jjnjD9L3UMFw== stan.dorsett+ovhus_public_cloud@gmail.com
MacBook-Pro:~ standorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;6. Add the ssh pub key to the OVH public cloud.&lt;/h2&gt;

&lt;p&gt;You next need to add the public key generate in the previous step to OVH public cloud so it can be used to connect to servers you create.&lt;/p&gt;

&lt;p&gt;Go to the project you created in the previous step. You can find this by clicking Cloud | Servers | [your project name].&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08082018-12-view-cloud-project.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;Click the &amp;#39;SSh keys&amp;#39; tab to view or create new ssh keys.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08082018-13-add-ssh-key.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;Click &amp;#39;Add a new key&amp;#39; to add the public key generated in the previous step.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08082018-14-add-ssh-key.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;Paste the output from the public key file, provide a friendly name for the ssh key and click &amp;quot;Add this key.&amp;#39;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08082018-15-add-ssh-key.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;h2&gt;7. Create a new openstack server.&lt;/h2&gt;

&lt;p&gt;Under your cloud project and click the Infrastructure tab. From the Actions dropdown click &amp;#39;Add a server.&amp;#39;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08082018-16-add-a-server.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;Choose the OS (image) that you want to have deployed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08082018-17-pick-an-os-image.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;Select the server type (flavor) of the server you want to create. The server type will specify the number of vcpu, memory, disk size and price per hour of the openstack server that will be created:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08082018-18-select-instance-type.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;Select the ssh key you created in the previous step. Click the &amp;#39;Launch now&amp;#39; button to create the server.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08082018-19-launch-instance.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;Once the server is created a &amp;#39;Login information&amp;#39; window will be displayed with the ssh username and ip address you will need to connect to the server.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08082018-20-login-info.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;h2&gt;8. SSH to the new openstack server,&lt;/h2&gt;

&lt;p&gt;You can now use the login infomation provided in the last step along with the private ssh key you generated to connect to your new server. I used the following commands to ssh from my Mac laptop to connect.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;ssh -i /Users/standorsett/.ssh/id_rsa-ovhus_public_cloud centos@147.135.76.67&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;-i&lt;/code&gt; option is used to specify the ssh private key you generated earlier.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is the output from my running of this command.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;MacBook-Pro:~ standorsett$ ssh -i /Users/standorsett/.ssh/id_rsa-ovhus_public_cloud centos@147.135.76.67
The authenticity of host &amp;#39;147.135.76.67 (147.135.76.67)&amp;#39; can&amp;#39;t be established.
ECDSA key fingerprint is SHA256:aVSsv0mHKgIizglcvouJkvtBBOiZfNcQTpIc5oxhNO4.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added &amp;#39;147.135.76.67&amp;#39; (ECDSA) to the list of known hosts.
[centos@server-1 ~]$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You can now run what commands you would like on your new openstack server, like checking the IP address of the server you created.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[centos@server-1 ~]$ ip a
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether fa:16:3e:42:97:c4 brd ff:ff:ff:ff:ff:ff
    inet 147.135.76.67/32 brd 147.135.76.67 scope global dynamic eth0
       valid_lft 86350sec preferred_lft 86350sec
    inet6 fe80::f816:3eff:fe42:97c4/64 scope link
       valid_lft forever preferred_lft forever
[centos@server-1 ~]$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;9. Destroy the openstack server.&lt;/h2&gt;

&lt;p&gt;Once you are finished with your openstack server, you should delete the server in order to stop being charged for it&amp;#39;s usage. Click the drop down on the server and select &amp;#39;delete&amp;#39; to delete the server.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08082018-21-instance-options.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;Click &amp;#39;Confirm&amp;#39; to confirm that you want to delete the server. This is your last chance to reconsider if you really want to delete it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08082018-22-confirm-delete.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;Your cloud project will now be back to not showing any servers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08082018-23-empty-cloud-project.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;hr&gt;

&lt;h3&gt;That all for this initial post covering how to sign up and get started using the OVH public cloud.&lt;/h3&gt;

&lt;h3&gt;Please provide any feedback or suggestions to my twitter account located on the about page.&lt;/h3&gt;
</content>
 </entry>
 
 <entry>
   <title>Exploring the OVH public cloud and opensource tools that can use it.</title>
   <link href="http://0.0.0.0:4000/2018/08/06/exploring-the-ovhpublic-cloud/"/>
   <updated>2018-08-06T00:00:00+00:00</updated>
   <id>http://0.0.0.0:4000/2018/08/06/exploring-the-ovhpublic-cloud</id>
   <content type="html">&lt;p&gt;This post is the beginning of a series of posts that will explain how to use the openstack-based OVH public cloud UI and opensource tools that you can use to automate and manage your openstack infrastructure. &lt;/p&gt;

&lt;p&gt;Over the last few years, I have worked primarily with VMware products and have not taken the time to learn with openstack. Now that the OVH public cloud has launched in the United States, and I work for OVH US, I figured it was time to start learning how customers could put this offering to use.&lt;/p&gt;

&lt;p&gt;The posts in this series of blog that have been released are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://sdorsett.github.io/2018/08/08/creating-an-openstack-server-on-ovh-public-cloud/&quot;&gt;Manually creating an openstack server on OVH public cloud&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://sdorsett.github.io/2018/08/10/using-the-openstack-cli-to-create-a-server-on-ovh-public-cloud/&quot;&gt;Using the Openstack cli to create a server on OVH public cloud&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://sdorsett.github.io/2018/08/14/using-rclone-to-connect-to-ovh-public-cloud-swift-storage/&quot;&gt;Using rclone to manage OVH Public Cloud swift storage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://sdorsett.github.io/2018/08/19/using-vagrant-to-deploy-to-ovh-public-cloud/&quot;&gt;Using Vagrant to deploy an OVH public cloud server&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://sdorsett.github.io/2018/08/21/using-terraform-to-deploy-to-ovh-public-cloud/&quot;&gt;Using Terraform to deploy an OVH public cloud server&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr&gt;

&lt;h3&gt;Please provide any feedback or suggestions to my twitter account located on the about page.&lt;/h3&gt;
</content>
 </entry>
 
 <entry>
   <title>Scripted Packer build, ovftool export and Vagrant .box file creation</title>
   <link href="http://0.0.0.0:4000/2015/12/28/scripted-packer-build-and-export/"/>
   <updated>2015-12-28T00:00:00+00:00</updated>
   <id>http://0.0.0.0:4000/2015/12/28/scripted-packer-build-and-export</id>
   <content type="html">&lt;p&gt;This is the seventh in a series of posts on &lt;a href=&quot;https://sdorsett.github.io/2015/12/22/pipeline-for-creating-packer-box-files/&quot;&gt;using a Packer pipeline to generate Vagrant .box files&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the last post we covered &lt;a href=&quot;https://sdorsett.github.io/2015/12/27/using-ovftool-to-export-packer-generated-virtual-machines/&quot;&gt;using ovftool to convert Packer generated virtual machines into Vagrant .box files&lt;/a&gt;. I promised to show you a better way of exporting and creating the Vagrant .box files, so in this post we will be combining the following items in one script:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kicking off the Packer build of a specific template&lt;/li&gt;
&lt;li&gt;Exporting the Packer generated virtual machine&lt;/li&gt;
&lt;li&gt;Creating the necessary metadata.json &amp;amp; Vagrantfile files&lt;/li&gt;
&lt;li&gt;Compressing the files into a TAR file with the .box extension&lt;/li&gt;
&lt;li&gt;Copying the Vagrant .box files to /vat/www/html/box-files/ so they can accessed by HTTP&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&amp;#39;s get started...&lt;/p&gt;

&lt;h2&gt;1. Start off by connecting by SSH to CentOS virtual machine we have been using in previous posts.&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;sdorsett-mbp:~ sdorsett$ ssh root@192.168.1.52
root@192.168.1.52&amp;#39;s password:
Last login: Sat Dec 26 22:04:20 2015 from 192.168.1.163
[root@packer-centos ~]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;2. Create a new directory, in the packer-templates directory, for storing build scripts.&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos ~]# mkdir ~/packer-templates/build-scripts
[root@packer-centos ~]# cd packer-templates/build-scripts/
[root@packer-centos build-scripts]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;3. Next we will create a generic script for performing a Packer build of a template.&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos build-scripts]# cat generic-packer-build-script.sh
#!/bin/bash

source /root/.bashrc

echo &amp;quot;starting packer build of $PACKER_VM_NAME&amp;quot;
packer build -var-file=packer-remote-info.json /root/packer-templates/templates/$PACKER_VM_NAME.json

echo &amp;quot;registering ${PACKER_VM_NAME} virtual machine on ${PACKER_REMOTE_HOST}&amp;quot;
/usr/bin/sshpass -p ${PACKER_REMOTE_PASSWORD} ssh root@${PACKER_REMOTE_HOST} &amp;quot;vim-cmd solo/registervm /vmfs/volumes/${PACKER_REMOTE_DATASTORE}/output-${PACKER_VM_NAME}/*.vmx&amp;quot;

mkdir -p /root/box_files/ovf/empty_dir/
mkdir -p /root/box_files/vmx/empty_dir/

rm -rf /root/box_files/ovf/${PACKER_VM_NAME}
rm -rf /root/box_files/vmx/${PACKER_VM_NAME}

echo &amp;quot;output of /vmfs/volumes/${PACKER_REMOTE_DATASTORE}/output-${PACKER_VM_NAME}/*.vmxf:&amp;quot;
/usr/bin/sshpass -p ${PACKER_REMOTE_PASSWORD} ssh root@${PACKER_REMOTE_HOST} &amp;quot;cat /vmfs/volumes/${PACKER_REMOTE_DATASTORE}/output-${PACKER_VM_NAME}/*.vmxf&amp;quot;

ovftool vi://root:${PACKER_REMOTE_PASSWORD}@${PACKER_REMOTE_HOST}/${PACKER_VM_NAME} /root/box_files/ovf/
ovftool -tt=vmx vi://root:${PACKER_REMOTE_PASSWORD}@${PACKER_REMOTE_HOST}/${PACKER_VM_NAME} /root/box_files/vmx/

echo &amp;quot;creating metadata.json and Vagrantfile files in ovf virtual machine directory&amp;quot;
echo &amp;#39;{&amp;quot;provider&amp;quot;:&amp;quot;vmware_ovf&amp;quot;}&amp;#39; &amp;gt;&amp;gt; /root/box_files/ovf/${PACKER_VM_NAME}/metadata.json
touch /root/box_files/ovf/${PACKER_VM_NAME}/Vagrantfile
cd /root/box_files/ovf/empty_dir/
cd /root/box_files/ovf/${PACKER_VM_NAME}/

echo &amp;quot;compressing ovf virtual machine files to /var/www/html/box-files/${PACKER_VM_NAME}-vmware_ovf-1.0.box&amp;quot;
tar cvzf /var/www/html/box-files/$PACKER_VM_NAME-vmware_ovf-1.0.box ./*

echo &amp;quot;creating metadata.json and Vagrantfile files in vmx virtual machine directory&amp;quot;
echo &amp;#39;{&amp;quot;provider&amp;quot;:&amp;quot;vmware_desktop&amp;quot;}&amp;#39; &amp;gt;&amp;gt; /root/box_files/vmx/${PACKER_VM_NAME}/metadata.json
touch /root/box_files/vmx/${PACKER_VM_NAME}/Vagrantfile
cd /root/box_files/vmx/empty_dir/
cd /root/box_files/vmx/${PACKER_VM_NAME}/

echo &amp;quot;compressing vmx virtual machine files to /var/www/html/box-files/${PACKER_VM_NAME}-vmware_desktop-1.0.box&amp;quot;
tar cvzf /var/www/html/box-files/$PACKER_VM_NAME-vmware_desktop-1.0.box ./*

echo &amp;quot;cleaning up /root/box_files directories&amp;quot;
rm -rf /root/box_files/ovf/$PACKER_VM_NAME
rm -rf /root/box_files/vmx/$PACKER_VM_NAME

echo &amp;quot;deleting $PACKER_VM_NAME from $PACKER_REMOTE_HOST&amp;quot;
/usr/bin/sshpass -p ${PACKER_REMOTE_PASSWORD} ssh root@${PACKER_REMOTE_HOST}  &amp;quot;vim-cmd vmsvc/getallvms | grep ${PACKER_VM_NAME} | cut -d &amp;#39; &amp;#39; -f 1 | xargs vim-cmd vmsvc/destroy&amp;quot;

echo &amp;quot;packer build of $PACKER_VM_NAME has been  completed&amp;quot;

[root@packer-centos build-scripts]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If you look over this script you will notice it covers all of the tasks we performed manually in the last two posts. You might all notice the following environmental variables listed:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;PACKER_REMOTE_HOST&lt;/li&gt;
&lt;li&gt;PACKER_REMOTE_USERNAME&lt;/li&gt;
&lt;li&gt;PACKER_REMOTE_PASSWORD&lt;/li&gt;
&lt;li&gt;PACKER_REMOTE_DATASTORE&lt;/li&gt;
&lt;li&gt;PACKER_VM_NAME&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We are again using the idea of &amp;quot;separating data from code&amp;quot; to keep environment specific information out of the scripts. This will help in keeping sensitive information from being store with our scripts in a code repository.&lt;/p&gt;

&lt;h2&gt;4. We now need to added the above listed PACKER_REMOTE environmental variables into the .bashrc file, so the script will know how to connect to vCenter&lt;/h2&gt;

&lt;p&gt;Add the following lines to the bottom of /root/.bashrc. Make sure to update any of them that do you match your environment:&lt;/p&gt;

&lt;pre&gt;
PACKER_REMOTE_HOST=192.168.1.51
PACKER_REMOTE_USERNAME=root
PACKER_REMOTE_PASSWORD=password
PACKER_REMOTE_DATASTORE=datastore1
&lt;/pre&gt;

&lt;p&gt;After updating the ~/.bashrc file, reread the file by running the following command:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos build-scripts]# source ~/.bashrc
[root@packer-centos build-scripts]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You can also validate the environmental variables exist by using the echo commmand:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos build-scripts]# echo $PACKER_REMOTE_HOST
192.168.1.51
[root@packer-centos build-scripts]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;5. We need to delete the existing Packer generated virtual machines from the ESXi virtual machine embedded host client, since the Packer build will halt if the virtual machine it is trying to build already exists:&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/01-delete-virtual-machine.png&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;h2&gt;6. With the generic Packer build script and environmental variables in place we can test our script out. First off we need to ensure the execution bit is set on our generic Packer build script:&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos build-scripts]# cd ~/packer-templates/
[root@packer-centos packer-templates]# chmod +x build-scripts/generic-packer-build-script.sh
[root@packer-centos packer-templates]# ls -la build-scripts/generic-packer-build-script.sh
-rwxr-xr-x 1 root root 2091 Dec 26 22:27 build-scripts/generic-packer-build-script.sh
[root@packer-centos packer-templates]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Now that we have set the script as executable, we can use it to build to centos67 template by passing in the PACKER&lt;em&gt;VM&lt;/em&gt;NAME environmental variable like the following:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos packer-templates]# PACKER_VM_NAME=centos67 build-scripts/generic-packer-build-script.sh
starting packer build of centos67
centos67 output will be in this color.

==&amp;gt; centos67: Downloading or copying ISO
    centos67: Downloading or copying: file:///root/packer-templates/iso/CentOS-6.7-x86_64-minimal.iso
==&amp;gt; centos67: Uploading ISO to remote machine...
==&amp;gt; centos67: Creating virtual machine disk
==&amp;gt; centos67: Building and writing VMX file
==&amp;gt; centos67: Starting HTTP server on port 8001
==&amp;gt; centos67: Registering remote VM...
==&amp;gt; centos67: Starting virtual machine...
==&amp;gt; centos67: Waiting 5s for boot...
==&amp;gt; centos67: Connecting to VM via VNC
==&amp;gt; centos67: Typing the boot command over VNC...
==&amp;gt; centos67: Waiting for SSH to become available...
...&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Checking the embedded host client we can see that the CentOS 6.7 operating system is being installed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/02-centos67-os-install.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;p&gt;Once the Packer build completes you will see the following output:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;centos67: ==&amp;gt; Zero out the free space to save space in the final image
centos67: dd: writing `/EMPTY&amp;#39;: No space left on device
centos67: 12950+0 records in
centos67: 12949+0 records out
centos67: 13578776576 bytes (14 GB) copied, 462.514 s, 29.4 MB/s
==&amp;gt; centos67: Gracefully halting virtual machine...
centos67: Waiting for VMware to clean up after itself...
==&amp;gt; centos67: Deleting unnecessary VMware files...
centos67: Deleting: /vmfs/volumes/datastore1/output-centos67/vmware.log
==&amp;gt; centos67: Cleaning VMX prior to finishing up...
centos67: Unmounting floppy from VMX...
centos67: Detaching ISO from CD-ROM device...
centos67: Disabling VNC server...
==&amp;gt; centos67: Compacting the disk image
==&amp;gt; centos67: Unregistering virtual machine...
Build &amp;#39;centos67&amp;#39; finished.

==&amp;gt; Builds finished. The artifacts of successful builds are:
--&amp;gt; centos67: VM files in directory: /vmfs/volumes/datastore1/output-centos67&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;At this point the script will re-register the vm Packer built and output the .vmxf file of the virtual machine. This is use for ensuring vmtools was properly installed, since this file contains the vmtool components and their versions that were successfully installed:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;registering centos67 virtual machine on 192.168.1.51
18
output of /vmfs/volumes/datastore1/output-centos67/*.vmxf:
&amp;lt;?xml version=&amp;quot;1.0&amp;quot;?&amp;gt;
&amp;lt;Foundry&amp;gt;&amp;lt;VM/&amp;gt;&amp;lt;tools-install-info&amp;gt;&amp;lt;installError&amp;gt;0&amp;lt;/installError&amp;gt;&amp;lt;updateCounter&amp;gt;1&amp;lt;/updateCounter&amp;gt;&amp;lt;/tools-install-info&amp;gt;&amp;lt;tools-manifest&amp;gt;&amp;lt;monolithic version=&amp;quot;9.9.4&amp;quot;/&amp;gt;&amp;lt;svga33 version=&amp;quot;10.3.0.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;svga4 version=&amp;quot;10.4.0.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;vmmouse42 version=&amp;quot;1.0.0.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;svga42 version=&amp;quot;10.10.2.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;vmmouse43 version=&amp;quot;12.6.4.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;svga43 version=&amp;quot;10.16.7.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;vmmouse43_64 version=&amp;quot;12.6.4.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;svga43_64 version=&amp;quot;10.16.7.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;vmmouse67 version=&amp;quot;12.6.4.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;svga67 version=&amp;quot;10.16.7.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;vmmouse67_64 version=&amp;quot;12.6.4.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;svga67_64 version=&amp;quot;10.16.7.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;vmmouse68 version=&amp;quot;12.6.4.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;svga68 version=&amp;quot;10.16.7.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;vmmouse68_64 version=&amp;quot;12.6.4.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;svga68_64 version=&amp;quot;10.16.7.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;vmmouse70 version=&amp;quot;12.7.0.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;svga70 version=&amp;quot;11.0.99.4&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;vmmouse70_64 version=&amp;quot;12.7.0.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;svga70_64 version=&amp;quot;11.0.99.4&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;vmmouse71 version=&amp;quot;12.7.0.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;svga71 version=&amp;quot;11.0.99.4&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;vmmouse71_64 version=&amp;quot;12.7.0.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;svga71_64 version=&amp;quot;11.0.99.4&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;vmmouse73 version=&amp;quot;12.7.0.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;svga73 version=&amp;quot;11.0.99.4&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;vmmouse73_64 version=&amp;quot;12.7.0.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;svga73_64 version=&amp;quot;11.0.99.4&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;vmmouse73_99 version=&amp;quot;12.7.0.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;svga73_99 version=&amp;quot;11.0.99.4&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;vmmouse73_99_64 version=&amp;quot;12.7.0.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;svga73_99_64 version=&amp;quot;11.0.99.4&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;vmmouse74 version=&amp;quot;12.7.0.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;svga74 version=&amp;quot;11.0.99.4&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;vmmouse74_64 version=&amp;quot;12.7.0.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;svga74_64 version=&amp;quot;11.0.99.4&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;vmmouse75 version=&amp;quot;12.7.0.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;svga75 version=&amp;quot;11.0.99.4&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;vmmouse75_64 version=&amp;quot;12.7.0.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;svga75_64 version=&amp;quot;11.0.99.4&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;vmmouse76 version=&amp;quot;12.7.0.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;svga76 version=&amp;quot;11.0.99.4&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;vmmouse76_64 version=&amp;quot;12.7.0.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;svga76_64 version=&amp;quot;11.0.99.4&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;checkvm version=&amp;quot;9.9.4.51858&amp;quot; installed=&amp;quot;TRUE&amp;quot;/&amp;gt;&amp;lt;vmtoolsd version=&amp;quot;9.9.4.51858&amp;quot; installed=&amp;quot;TRUE&amp;quot;/&amp;gt;&amp;lt;upgrader version=&amp;quot;9.9.4.51858&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;hgfsclient version=&amp;quot;9.9.4.51858&amp;quot; installed=&amp;quot;TRUE&amp;quot;/&amp;gt;&amp;lt;hgfsmounter version=&amp;quot;9.9.4.51858&amp;quot; installed=&amp;quot;TRUE&amp;quot;/&amp;gt;&amp;lt;vmguestlib version=&amp;quot;9.9.4.51858&amp;quot; installed=&amp;quot;TRUE&amp;quot;/&amp;gt;&amp;lt;vmguestlibjava version=&amp;quot;9.9.4.51858&amp;quot; installed=&amp;quot;TRUE&amp;quot;/&amp;gt;&amp;lt;toolbox-cmd version=&amp;quot;9.9.4.51858&amp;quot; installed=&amp;quot;TRUE&amp;quot;/&amp;gt;&amp;lt;vmci version=&amp;quot;9.6.2.0&amp;quot; installed=&amp;quot;TRUE&amp;quot;/&amp;gt;&amp;lt;vmhgfs version=&amp;quot;1.4.20.1&amp;quot; installed=&amp;quot;TRUE&amp;quot;/&amp;gt;&amp;lt;vmmemctl version=&amp;quot;1.2.1.2&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;vmsync version=&amp;quot;1.1.0.1&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;vmxnet version=&amp;quot;2.1.0.0&amp;quot; installed=&amp;quot;TRUE&amp;quot;/&amp;gt;&amp;lt;vmxnet3 version=&amp;quot;1.3.0.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;vmblock version=&amp;quot;1.1.2.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;vsock version=&amp;quot;9.6.1.0&amp;quot; installed=&amp;quot;TRUE&amp;quot;/&amp;gt;&amp;lt;pvscsi version=&amp;quot;1.2.3.0&amp;quot; installed=&amp;quot;FALSE&amp;quot;/&amp;gt;&amp;lt;/tools-manifest&amp;gt;&amp;lt;/Foundry&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The next stage of the build script will run the ovftool commands to export .vmx and .ovf format virtual machines from ESXi.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;Opening VI source: vi://root@192.168.1.51:443/centos67
Opening OVF target: /root/box_files/ovf/
Writing OVF package: /root/box_files/ovf/centos67/centos67.ovf
Transfer Completed
Completed successfully
Opening VI source: vi://root@192.168.1.51:443/centos67
Opening VMX target: /root/box_files/vmx/
Writing VMX file: /root/box_files/vmx/centos67/centos67.vmx
Transfer Completed
Completed successfully&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The next stage of the build script will create the metadata.json and Vagrantfiles needed for the Vagrant .box files. It will then TAR up all the virtual machine files contained in the ovf &amp;amp; vmx folders. This stage will also save the TAR files to /var/www/html/box-files/ so that they will be available over http.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;creating metadata.json and Vagrantfile files in ovf virtual machine directory
compressing ovf virtual machine files to /var/www/html/box-files/centos67-vmware_ovf-1.0.box
./centos67-disk1.vmdk
./centos67.mf
./centos67.ovf
./metadata.json
./Vagrantfile
creating metadata.json and Vagrantfile files in vmx virtual machine directory
compressing vmx virtual machine files to /var/www/html/box-files/centos67-vmware_desktop-1.0.box
./centos67-disk1.vmdk
./centos67.vmx
./metadata.json
./Vagrantfile&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The final section of the build script will remove the directories that ovftool generated in /root/box_files (to keep disk space usage to a minimum) and delete the Packer virtual machine from ESXi.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;cleaning up /root/box_files directories
deleting centos67 from 192.168.1.51
packer build of centos67 has been  completed
[root@packer-centos packer-templates]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;6. We can now list the files in /var/www/html/box-files and see the two .box files generated by the script.&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos packer-templates]# ls -lah /var/www/html/box-files/
total 654M
drwxr-xr-x 2 root root 4.0K Dec 28 15:44 .
drwxr-xr-x 3 root root 4.0K Dec 23 18:09 ..
-rw-r--r-- 1 root root 319M Dec 28 15:46 centos67-vmware_desktop-1.0.box
-rw-r--r-- 1 root root 336M Dec 28 15:44 centos67-vmware_ovf-1.0.box
[root@packer-centos packer-templates]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We can also open a browser and see the .box files the script generated.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/01-browse-vagrant-box-files.png&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;h3&gt;If you would like to not have to manually create the build script covered in this post, you can clone down &lt;a href=&quot;https://github.com/sdorsett/packer-templates/tree/adding-build-script&quot;&gt;this github repository&lt;/a&gt; by running the following command:&lt;/h3&gt;

&lt;pre&gt;
git clone -b &quot;adding-build-script&quot; https://github.com/sdorsett/packer-templates.git
&lt;/pre&gt;  

&lt;p&gt;If you cloned the packer-templates repo in the last post, you can pull down the updates by running the following command:&lt;/p&gt;

&lt;pre&gt;
git fetch --all
git pull origin &quot;adding-build-script&quot;
&lt;/pre&gt;  

&lt;h3&gt;This brings us to the end of this post. I hope I made good on my promise of showing an automated ways of converting the Packer templates to .box files. In the next post I think we should cover how to test our Vagrant .box files are functional by deploying them using Vagrant.&lt;/h3&gt;

&lt;h3&gt;Please provide any feedback or suggestions to my twitter account located on the about page.&lt;/h3&gt;
</content>
 </entry>
 
 <entry>
   <title>Using ovftool to convert Packer generated virtual machines into Vagrant .box files</title>
   <link href="http://0.0.0.0:4000/2015/12/27/using-ovftool-to-export-packer-generated-virtual-machines/"/>
   <updated>2015-12-27T00:00:00+00:00</updated>
   <id>http://0.0.0.0:4000/2015/12/27/using-ovftool-to-export-packer-generated-virtual-machines</id>
   <content type="html">&lt;p&gt;This is the sixth in a series of posts on &lt;a href=&quot;https://sdorsett.github.io/2015/12/22/pipeline-for-creating-packer-box-files/&quot;&gt;using a Packer pipeline to generate Vagrant .box files&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the last post we covered &lt;a href=&quot;https://sdorsett.github.io/2015/12/26/copy-our-existing-template-and-add-the-puppet-agent/&quot;&gt;copying our existing CentOS 6.7 template and adding the Puppet agent&lt;/a&gt; in order to generate a new Packer template. In this post we will be covering how to use ovftool to convert Packer generated virtual machines into Vagrant .box files. This post will be going over the manual steps on purpose, since I feel it will make more sense when we start to cover automating the steps that you can already performed by hand.&lt;/p&gt;

&lt;p&gt;Let&amp;#39;s get started...&lt;/p&gt;

&lt;h2&gt;1. Start off by connecting by SSH to CentOS virtual machine we have been using in previous posts.&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;sdorsett-mbp:~ sdorsett$ ssh root@192.168.1.52
root@192.168.1.52&amp;#39;s password:
Last login: Sat Dec 26 20:52:17 2015 from 192.168.1.163
[root@packer-centos ~]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;2. Create two new directories for downloading virtual machines, one for .ovf and one for .vmx virtual machine formats.&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;root@packer-centos ~]# mkdir -p ~/box_files/{ovf,vmx}
[root@packer-centos ~]# tree ~/box_files/
/root/box_files/
├── ovf
└── vmx

2 directories, 0 files
[root@packer-centos ~]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;3. Now that we have locations for our ovftool exported virtual machine, we need to re-register the Packer created virtual machines. SSH to your ESXi virtual machine and cd to the datastore that you created the Packer virtual machines on.&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;sdorsett-mbp:Desktop sdorsett$ ssh root@192.168.1.51
Password:
The time and date of this login have been sent to the system logs.

VMware offers supported, powerful system administration tools.  Please
see www.vmware.com/go/sysadmintools for details.

The ESXi Shell can be disabled by an administrative user. See the
vSphere Security documentation for more information.
[root@packer-esxi:~] cd /vmfs/volumes/datastore1/
[root@packer-esxi:/vmfs/volumes/5679efd9-3e1658e4-f977-0050569b912b] ls output-*
output-centos67:
centos67-flat.vmdk  centos67.vmdk       centos67.vmx
centos67.nvram      centos67.vmsd       centos67.vmxf

output-centos67-pe-puppet-382:
centos67-pe-puppet-382-flat.vmdk  centos67-pe-puppet-382.vmsd
centos67-pe-puppet-382.nvram      centos67-pe-puppet-382.vmx
centos67-pe-puppet-382.vmdk       centos67-pe-puppet-382.vmxf
[root@packer-esxi:/vmfs/volumes/5679efd9-3e1658e4-f977-0050569b912b]&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;4. We next need to register both of the .vmx files of the Packer virtual machines displayed above on the ESXi virtual machine.&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-esxi:/vmfs/volumes/] vim-cmd solo/registervm /vmfs
/volumes/datastore1/output-centos67/*.vmx
15
[root@packer-esxi:/vmfs/volumes/] vim-cmd solo/registervm /vmfs
/volumes/datastore1/output-centos67-pe-puppet-382/*.vmx
16
[root@packer-esxi:/vmfs/volumes/]&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You will notice that each of the vim-cmd commands returned a number. That number is the vmid of the virtual machine that was registered. Also if you check the embedded host client, you will see those virtual machines we just registered with the vim-cmd command listed under &amp;quot;Virtual Machines.&amp;quot;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/01-registered-virtual-machines.png&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;h2&gt;5. Once the Packer virtual machines have been registered, we can use ovftool on the CentOS virtual machine to export them as .ovf files.&lt;/h2&gt;

&lt;p&gt;After the files are exported, we can then compress the .ovf files into a vmware_ovf compatible .box file. Using the vmware_ovf format will provide a generic .box file that can be deployed to vCenter, vCloud Director or vCloud Air Vagrant providers.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos ~]# ovftool vi://root@192.168.1.51/centos67 /root/box_files/ovf/
Enter login information for source vi://192.168.1.51/
Username: root
Password: ********
Opening VI source: vi://root@192.168.1.51:443/centos67
Opening OVF target: /root/box_files/ovf/
Writing OVF package: /root/box_files/ovf/centos67/centos67.ovf
Transfer Completed
Completed successfully

[root@packer-centos ~]# ovftool vi://root@192.168.1.51/centos67-pe-puppet-382 /root/box_files/ovf/
Enter login information for source vi://192.168.1.51/
Username: root
Password: ********
Opening VI source: vi://root@192.168.1.51:443/centos67-pe-puppet-382
Opening OVF target: /root/box_files/ovf/
Writing OVF package: /root/box_files/ovf/centos67-pe-puppet-382/centos67-pe-puppet-382.ovf
Transfer Completed
Completed successfully

[root@packer-centos ~]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;6. We can also use ovftool to export the Packer virtual machines in a .vmx format for use with VMware Fusion or Workstation:&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos ~]# ovftool -tt=vmx vi://root@192.168.1.51/centos67 /root/box_files/vmx/Enter login information for source vi://192.168.1.51/
Username: root
Password: ********
Opening VI source: vi://root@192.168.1.51:443/centos67
Opening VMX target: /root/box_files/vmx/
Writing VMX file: /root/box_files/vmx/centos67/centos67.vmx
Transfer Completed
Completed successfully

[root@packer-centos ~]# ovftool -tt=vmx vi://root@192.168.1.51/centos67-pe-puppet-382 /root/box_files/vmx/
Enter login information for source vi://192.168.1.51/
Username: root
Password: ********
Opening VI source: vi://root@192.168.1.51:443/centos67-pe-puppet-382
Opening VMX target: /root/box_files/vmx/
Writing VMX file: /root/box_files/vmx/centos67-pe-puppet-382/centos67-pe-puppet-382.vmx
Transfer Completed
Completed successfully

[root@packer-centos ~]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;7. We will start with converting the .ovf exported templates.&lt;/h2&gt;

&lt;p&gt;Each of the exported template directories will need a metadata.js and Vagrantfile created. After creating the metadata.js and Vagrantfile files, we will tar all of the files in each directory into a .box file.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos ~]# cd /root/box_files/ovf/centos67
[root@packer-centos centos67]# echo &amp;#39;{&amp;quot;provider&amp;quot;:&amp;quot;vmware_ovf&amp;quot;}&amp;#39; &amp;gt;&amp;gt; metadata.json
[root@packer-centos centos67]# touch Vagrantfile
[root@packer-centos centos67]# tar cvzf /root/box_files/centos67-vmware_ovf-1.0.box ./*
./centos67-disk1.vmdk
./centos67.mf
./centos67.ovf
./metadata.json
./Vagrantfile

[root@packer-centos centos67]# cd /root/box_files/ovf/centos67-pe-puppet-382
[root@packer-centos centos67-pe-puppet-382]# echo &amp;#39;{&amp;quot;provider&amp;quot;:&amp;quot;vmware_ovf&amp;quot;}&amp;#39; &amp;gt;&amp;gt; metadata.json
[root@packer-centos centos67-pe-puppet-382]# touch Vagrantfile
[root@packer-centos centos67-pe-puppet-382]# tar cvzf /root/box_files/centos67-pe-puppet-382-vmware_ovf-1.0.box ./*
./centos67-pe-puppet-382-disk1.vmdk
./centos67-pe-puppet-382.mf
./centos67-pe-puppet-382.ovf
./metadata.json
./Vagrantfile

[root@packer-centos centos67-pe-puppet-382]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;8. Next we will convert the .vmx exported templates.&lt;/h2&gt;

&lt;p&gt;Each of the exported template directories will also need a slightly different metadata.js file created, Vagrantfile created and finally tar all of the files in each directory into a .box file.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos centos67-pe-puppet-382]# cd /root/box_files/vmx/centos67
[root@packer-centos centos67]# echo &amp;#39;{&amp;quot;provider&amp;quot;:&amp;quot;vmware_desktop&amp;quot;}&amp;#39; &amp;gt;&amp;gt; metadata.json
[root@packer-centos centos67]# touch Vagrantfile
[root@packer-centos centos67]# tar cvzf /root/box_files/centos67-vmware_desktop-1.0.box ./*
./centos67-disk1.vmdk
./centos67.vmx
./metadata.json
./Vagrantfile

[root@packer-centos centos67]# cd /root/box_files/vmx/centos67-pe-puppet-382/
[root@packer-centos centos67-pe-puppet-382]# echo &amp;#39;{&amp;quot;provider&amp;quot;:&amp;quot;vmware_desktop&amp;quot;}&amp;#39; &amp;gt;&amp;gt; metadata.json
[root@packer-centos centos67-pe-puppet-382]# touch Vagrantfile
[root@packer-centos centos67-pe-puppet-382]# tar cvzf /root/box_files/centos67-pe-puppet-382-vmware_desktop-1.0.box ./*
./centos67-pe-puppet-382-disk1.vmdk
./centos67-pe-puppet-382.vmx
./metadata.json
./Vagrantfile

[root@packer-centos centos67-pe-puppet-382]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;9. Finally we can use the tree command to see the overall directory structure of the .ovf and .vmx templates, as well as the list of the .box files:&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos centos67-pe-puppet-382]# tree /root/box_files/
/root/box_files/
├── centos67-pe-puppet-382-vmware_desktop-1.0.box
├── centos67-pe-puppet-382-vmware_ovf-1.0.box
├── centos67-vmware_desktop-1.0.box
├── centos67-vmware_ovf-1.0.box
├── ovf
│   ├── centos67
│   │   ├── centos67-disk1.vmdk
│   │   ├── centos67.mf
│   │   ├── centos67.ovf
│   │   ├── metadata.json
│   │   └── Vagrantfile
│   └── centos67-pe-puppet-382
│       ├── centos67-pe-puppet-382-disk1.vmdk
│       ├── centos67-pe-puppet-382.mf
│       ├── centos67-pe-puppet-382.ovf
│       ├── metadata.json
│       └── Vagrantfile
└── vmx
    ├── centos67
    │   ├── centos67-disk1.vmdk
    │   ├── centos67.vmx
    │   ├── metadata.json
    │   └── Vagrantfile
    └── centos67-pe-puppet-382
        ├── centos67-pe-puppet-382-disk1.vmdk
        ├── centos67-pe-puppet-382.vmx
        ├── metadata.json
        └── Vagrantfile

6 directories, 22 files
[root@packer-centos centos67-pe-puppet-382]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3&gt;This brings us to the end of this post. Again I&amp;#39;m sorry for this post not covering any automated ways of converting the Packer templates to .box files, but in the next post you&amp;#39;ll learn how we can ease this manual pain with some  scripts.&lt;/h3&gt;

&lt;h3&gt;Please provide any feedback or suggestions to my twitter account located on the about page.&lt;/h3&gt;
</content>
 </entry>
 
 <entry>
   <title>Copying our existing CentOS 6.7 template and adding the Puppet agent</title>
   <link href="http://0.0.0.0:4000/2015/12/26/copy-our-existing-template-and-add-the-puppet-agent/"/>
   <updated>2015-12-26T00:00:00+00:00</updated>
   <id>http://0.0.0.0:4000/2015/12/26/copy-our-existing-template-and-add-the-puppet-agent</id>
   <content type="html">&lt;p&gt;This is the fifth in a series of posts on &lt;a href=&quot;https://sdorsett.github.io/2015/12/22/pipeline-for-creating-packer-box-files/&quot;&gt;using a Packer pipeline to generate Vagrant .box files&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the last post we covered &lt;a href=&quot;https://sdorsett.github.io/2015/12/25/creating-a-packer-template-for-installing-centos-67/&quot;&gt;creating a Packer template for installing CentOS 6.7 with vmtools&lt;/a&gt;. In this post we will be basing a new Packer template on the one we created last post and add installing the Puppet Enterprise 3.8.2 agent.&lt;/p&gt;

&lt;p&gt;This is an older version of the Puppet Enterprise agent, but it will let us create a Vagrant .box file that can be used for testing Puppet 3.x code.&lt;/p&gt;

&lt;p&gt;Let&amp;#39;s get started...&lt;/p&gt;

&lt;h2&gt;1. Start off by connecting by SSH to CentOS virtual machine we have been using in previous posts.&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;sdorsett-mbp:~ sdorsett$ ssh root@192.168.1.52
root@192.168.1.52&amp;#39;s password:
Last login: Fri Dec 25 20:22:44 2015 from 192.168.1.253
[root@packer-centos ~]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;2. Create a new bash script named centos-install-pe-puppet-382.sh at packer-templates/scripts/ to install the Puppet Enterprise 3.8.2 agent:&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos ~]# cd ~/packer-templates/scripts/
[root@packer-centos scripts]# cat centos-install-pe-puppet-382.sh
#!/bin/bash

VERSION=&amp;#39;3.8.2&amp;#39;
PKGNAME=&amp;quot;puppet-enterprise-${VERSION}-el-6-x86_64&amp;quot;
TARFILE=&amp;quot;${PKGNAME}.tar.gz&amp;quot;
URL=&amp;quot;https://pm.puppetlabs.com/puppet-enterprise/${VERSION}/${TARFILE}&amp;quot;

TMPDIR=&amp;#39;/tmp&amp;#39;
HOSTNAME=`hostname`
cd $TMPDIR
echo &amp;quot;Fetching ${TARFILE}&amp;quot;
[ -e $TARFILE ] || curl -fLO $URL
echo &amp;quot;Extracting ${TARFILE}&amp;quot;
[ -d $PKGNAME ] || tar -xf $TARFILE
cd $PKGNAME

cat &amp;gt; agent.ans &amp;lt;&amp;lt;EOF
q_all_in_one_install=n
q_database_install=n
q_pe_database=n
q_puppetca_install=n
q_puppetdb_install=n
q_puppetmaster_install=n
q_puppet_enterpriseconsole_install=n
q_run_updtvpkg=n
q_continue_or_reenter_master_hostname=c
q_fail_on_unsuccessful_master_lookup=n
q_puppetagent_certname=$HOSTNAME
q_puppetagent_install=y
q_puppetagent_server=puppet
q_puppet_cloud_install=y
q_puppet_symlinks_install=y
q_vendor_packages_install=y
q_install=y
EOF

./puppet-enterprise-installer -a agent.ans

# Remove certname so the system will use host FQDN
sed -i &amp;#39;/certname =/d&amp;#39; /etc/puppetlabs/puppet/puppet.conf

# Symlink so puppet is in vagrant provisioner PATH
ln -s /opt/puppet/bin/facter /usr/bin/facter
ln -s /opt/puppet/bin/puppet /usr/bin/puppet

[root@packer-deploy scripts]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;3. Copy the Packer template we created previously, as the starting point for a new CentOS 6.7 template that will have Puppet Enterprise 3.8.2 installed&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos ~]# cd ~/packer-templates/templates/
[root@packer-centos templates]# ls
centos67.json
[root@packer-centos templates]# cp centos67.json centos67-pe-puppet-382.json
[root@packer-centos templates]#
[root@packer-deploy scripts]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;4. Update the provisioners blocker to include the &amp;quot;scripts/centos-install-pe-puppet-382.sh&amp;quot; bash script.&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos templates]# cat centos67-pe-puppet-382.json
{
  &amp;quot;variables&amp;quot;: {
    &amp;quot;version&amp;quot;: &amp;quot;1.0&amp;quot;
  },
  &amp;quot;builders&amp;quot;: [
    {
      &amp;quot;name&amp;quot;: &amp;quot;centos67-pe-puppet-382&amp;quot;,
      &amp;quot;vm_name&amp;quot;: &amp;quot;centos67-pe-puppet-382&amp;quot;,
      &amp;quot;vmdk_name&amp;quot;: &amp;quot;centos67-pe-puppet-382&amp;quot;,
      &amp;quot;type&amp;quot;: &amp;quot;vmware-iso&amp;quot;,
      &amp;quot;communicator&amp;quot;: &amp;quot;ssh&amp;quot;,
      &amp;quot;ssh_pty&amp;quot;: &amp;quot;true&amp;quot;,
      &amp;quot;headless&amp;quot;: false,
      &amp;quot;disk_size&amp;quot;: 16384,
      &amp;quot;guest_os_type&amp;quot;: &amp;quot;rhel6-64&amp;quot;,
      &amp;quot;iso_url&amp;quot;: &amp;quot;./iso/CentOS-6.7-x86_64-minimal.iso&amp;quot;,
      &amp;quot;iso_checksum&amp;quot;: &amp;quot;2ed5ea551dffc3e4b82847b3cee1f6cd748e8071&amp;quot;,
      &amp;quot;iso_checksum_type&amp;quot;: &amp;quot;sha1&amp;quot;,
      &amp;quot;shutdown_command&amp;quot;: &amp;quot;echo &amp;#39;vagrant&amp;#39; | sudo -S /sbin/shutdown -P now&amp;quot;,

      &amp;quot;remote_host&amp;quot;: &amp;quot;&amp;quot;,
      &amp;quot;remote_datastore&amp;quot;: &amp;quot;&amp;quot;,
      &amp;quot;remote_username&amp;quot;: &amp;quot;&amp;quot;,
      &amp;quot;remote_password&amp;quot;: &amp;quot;&amp;quot;,
      &amp;quot;remote_type&amp;quot;: &amp;quot;esx5&amp;quot;,
      &amp;quot;ssh_username&amp;quot;: &amp;quot;root&amp;quot;,
      &amp;quot;ssh_password&amp;quot;: &amp;quot;vagrant&amp;quot;,
      &amp;quot;ssh_wait_timeout&amp;quot;: &amp;quot;60m&amp;quot;,
      &amp;quot;tools_upload_flavor&amp;quot;: &amp;quot;linux&amp;quot;,
      &amp;quot;http_directory&amp;quot;: &amp;quot;.&amp;quot;,
      &amp;quot;boot_wait&amp;quot;: &amp;quot;5s&amp;quot;,
      &amp;quot;vmx_data&amp;quot;: {
        &amp;quot;memsize&amp;quot;: &amp;quot;4096&amp;quot;,
        &amp;quot;numvcpus&amp;quot;: &amp;quot;2&amp;quot;,
        &amp;quot;ethernet0.networkName&amp;quot;: &amp;quot;&amp;quot;,
        &amp;quot;ethernet0.present&amp;quot;: &amp;quot;TRUE&amp;quot;,
        &amp;quot;ethernet0.startConnected&amp;quot;: &amp;quot;TRUE&amp;quot;,
        &amp;quot;ethernet0.virtualDev&amp;quot;: &amp;quot;e1000&amp;quot;,
        &amp;quot;ethernet0.addressType&amp;quot;: &amp;quot;generated&amp;quot;,
        &amp;quot;ethernet0.generatedAddressOffset&amp;quot;: &amp;quot;0&amp;quot;,
        &amp;quot;ethernet0.wakeOnPcktRcv&amp;quot;: &amp;quot;FALSE&amp;quot;

      },
      &amp;quot;vmx_data_post&amp;quot;: {
        &amp;quot;ide1:0.startConnected&amp;quot;: &amp;quot;FALSE&amp;quot;,
        &amp;quot;ide1:0.clientDevice&amp;quot;: &amp;quot;TRUE&amp;quot;,
        &amp;quot;ide1:0.fileName&amp;quot;: &amp;quot;emptyBackingString&amp;quot;,
        &amp;quot;ethernet0.virtualDev&amp;quot;: &amp;quot;vmxnet3&amp;quot;
      },
      &amp;quot;boot_command&amp;quot;: [
        &amp;quot;&amp;lt;tab&amp;gt; text ks=http://:/scripts/centos-6-kickstart.cfg&amp;lt;enter&amp;gt;&amp;lt;wait&amp;gt;&amp;quot;
      ]
    }
  ],
  &amp;quot;provisioners&amp;quot;: [
    {
      &amp;quot;type&amp;quot;: &amp;quot;file&amp;quot;,
      &amp;quot;source&amp;quot;: &amp;quot;iso/vmware-tools-linux.iso&amp;quot;,
      &amp;quot;destination&amp;quot;: &amp;quot;/tmp/vmware-tools-linux.iso&amp;quot;
    },
    {
      &amp;quot;type&amp;quot;: &amp;quot;shell&amp;quot;,
      &amp;quot;execute_command&amp;quot;: &amp;quot;echo &amp;#39;vagrant&amp;#39; |  sudo -E -S sh &amp;#39;&amp;#39;&amp;quot;,
      &amp;quot;script&amp;quot;: &amp;quot;scripts/centos-vagrant-settings.sh&amp;quot;
    },
    {
      &amp;quot;type&amp;quot;: &amp;quot;shell&amp;quot;,
      &amp;quot;execute_command&amp;quot;: &amp;quot;echo &amp;#39;vagrant&amp;#39; |  sudo -E -S sh &amp;#39;&amp;#39;&amp;quot;,
      &amp;quot;script&amp;quot;: &amp;quot;scripts/centos-vmware-tools_install.sh&amp;quot;
    },
    {
      &amp;quot;type&amp;quot;: &amp;quot;shell&amp;quot;,
      &amp;quot;execute_command&amp;quot;: &amp;quot;echo &amp;#39;vagrant&amp;#39; |  sudo -E -S sh &amp;#39;&amp;#39;&amp;quot;,
      &amp;quot;script&amp;quot;: &amp;quot;scripts/centos-install-pe-puppet-382.sh&amp;quot;
    },
    {
      &amp;quot;type&amp;quot;: &amp;quot;shell&amp;quot;,
      &amp;quot;execute_command&amp;quot;: &amp;quot;echo &amp;#39;vagrant&amp;#39; |  sudo -E -S sh &amp;#39;&amp;#39;&amp;quot;,
      &amp;quot;script&amp;quot;: &amp;quot;scripts/centos-vmware-cleanup.sh&amp;quot;
    }
  ]
}&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;5. The packer-templates directory should now contain the following directories and files:&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos packer-templates]# tree /root/packer-templates/
/root/packer-templates/
├── iso
│   ├── CentOS-6.7-x86_64-minimal.iso
│   ├── linux.iso
│   └── vmware-tools-linux.iso -&amp;gt; ./linux.iso
├── packer_cache
├── packer-remote-info.json
├── README.md
├── scripts
│   ├── centos-6-kickstart.cfg
│   ├── centos-install-pe-puppet-382.sh
│   ├── centos-vagrant-settings.sh
│   ├── centos-vmware-cleanup.sh
│   └── centos-vmware-tools_install.sh
└── templates
    ├── centos67.json
    └── centos67-pe-puppet-382.json

4 directories, 12 files
[root@packer-centos packer-templates]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;6. Push the changes we have made to a github repository&lt;/h2&gt;

&lt;p&gt;During this step pushed the two new files to the &lt;a href=&quot;https://github.com/sdorsett/packer-templates/tree/adding-second-packer-template&quot;&gt;adding-second-packer-template branch&lt;/a&gt; of the github repository I have created for this series of blog posts:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos packer-templates]# git status
# On branch first-packer-template
# Untracked files:
#   (use &amp;quot;git add &amp;lt;file&amp;gt;...&amp;quot; to include in what will be committed)
#
#   scripts/centos-install-pe-puppet-382.sh
#   templates/centos67-pe-puppet-382.json
nothing added to commit but untracked files present (use &amp;quot;git add&amp;quot; to track)
[root@packer-centos packer-templates]# git add .
[root@packer-centos packer-templates]# git commit -m &amp;quot;adding templates/centos67-pe-puppet-382.json and scripts/centos-install-pe-puppet-382.sh&amp;quot;
[first-packer-template 75bb98e] adding templates/centos67-pe-puppet-382.json and scripts/centos-install-pe-puppet-382.sh
 2 files changed, 125 insertions(+), 0 deletions(-)
 create mode 100644 scripts/centos-install-pe-puppet-382.sh
 create mode 100644 templates/centos67-pe-puppet-382.json
[root@packer-centos packer-templates]# git checkout -b adding-second-packer-template
Switched to a new branch &amp;#39;adding-second-packer-template&amp;#39;
[root@packer-centos packer-templates]# git push origin adding-second-packer-template
Password:
Counting objects: 9, done.
Delta compression using up to 2 threads.
Compressing objects: 100% (6/6), done.
Writing objects: 100% (6/6), 1.95 KiB, done.
Total 6 (delta 1), reused 0 (delta 0)
To https://sdorsett@github.com/sdorsett/packer-templates.git
 * [new branch]      adding-second-packer-template -&amp;gt; adding-second-packer-template
[root@packer-centos packer-templates]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;13. Have Packer build the new template we created&lt;/h2&gt;

&lt;p&gt;Now that we have updated the new template file and created the script file that will install the Puppet Enterprise 3.8.2 agent, we can test our build. In order to use the values in the packer-remote-info.json file, we will use the -var-file parameter to specify this file.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;root@packer-centos packer-templates]# packer build -var-file=./packer-remote-info.json templates/centos67-pe-puppet-382.json
centos67-pe-puppet-382 output will be in this color.

==&amp;gt; centos67-pe-puppet-382: Downloading or copying ISO
    centos67-pe-puppet-382: Downloading or copying: file:///root/packer-templates/iso/CentOS-6.7-x86_64-minimal.iso
==&amp;gt; centos67-pe-puppet-382: Uploading ISO to remote machine...
==&amp;gt; centos67-pe-puppet-382: Creating virtual machine disk
==&amp;gt; centos67-pe-puppet-382: Building and writing VMX file
==&amp;gt; centos67-pe-puppet-382: Starting HTTP server on port 8019
==&amp;gt; centos67-pe-puppet-382: Registering remote VM...
==&amp;gt; centos67-pe-puppet-382: Starting virtual machine...
==&amp;gt; centos67-pe-puppet-382: Waiting 7s for boot...
==&amp;gt; centos67-pe-puppet-382: Connecting to VM via VNC
==&amp;gt; centos67-pe-puppet-382: Typing the boot command over VNC...
==&amp;gt; centos67-pe-puppet-382: Waiting for SSH to become available...&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;At this point you should see a centos67 virtual machine in the embedded host client of the ESXi virtual machine.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/01-centos-67-pe-puppet-382-virtual-machine.png&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;p&gt;Once the CentOS 6.7 install completes and the virtual machine reboots, you will see the packer build output continue with the provisioners block of the Packer template.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;==&amp;gt; centos67-pe-puppet-382: Connected to SSH!&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The first provisioner is the file provisioner that will copy iso/vmware-tools-linux.iso to /tmp/vmware-tools-linux.iso within the CentOS 6.7 virtual machine.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;==&amp;gt; centos67-pe-puppet-382: Uploading iso/vmware-tools-linux.iso =&amp;gt; /tmp/vmware-tools-linux.iso&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The second provisioner is a shell provisioner that will run the scripts/centos-vagrant-settings.sh bash script. This script will added the necessary changes for this Packer image to be used by Vagrant.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;==&amp;gt; centos67-pe-puppet-382: Provisioning with shell script: scripts/centos-vagrant-settings.sh&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The third provisioner is a shell provisioner that will run the scripts/centos-vmware-tools_install.sh bash script. This script will install all the needed dependencies for vmtools and then install vmtools using an answer file.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;==&amp;gt; centos67-pe-puppet-382: Provisioning with shell script: scripts/centos-vmware-tools_install.sh
    centos67-pe-puppet-382: Loaded plugins: fastestmirror
    centos67-pe-puppet-382: Setting up Upgrade Process
    centos67-pe-puppet-382: Determining fastest mirrors
    centos67-pe-puppet-382: * base: centos.unixheads.org
    centos67-pe-puppet-382: * extras: mirror.rackspace.com
    centos67-pe-puppet-382: * updates: centos-mirror.jchost.net
    centos67-pe-puppet-382: No Packages marked for Update
    centos67-pe-puppet-382: Loaded plugins: fastestmirror
    centos67-pe-puppet-382: Setting up Install Process
    centos67-pe-puppet-382: Loading mirror speeds from cached hostfile
    centos67-pe-puppet-382: * base: centos.unixheads.org
    centos67-pe-puppet-382: * extras: mirror.rackspace.com
    centos67-pe-puppet-382: * updates: centos-mirror.jchost.net
    centos67-pe-puppet-382: Package 4:perl-5.10.1-141.el6_7.1.x86_64 already installed and latest version
    centos67-pe-puppet-382: Resolving Dependencies
    centos67-pe-puppet-382: --&amp;gt; Running transaction check
    centos67-pe-puppet-382: ---&amp;gt; Package fuse-libs.x86_64 0:2.8.3-4.el6 will be installed
    centos67-pe-puppet-382: --&amp;gt; Finished Dependency Resolution
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Dependencies Resolved
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ========================================
    centos67-pe-puppet-382: Package  Arch   Version     Repository
    centos67-pe-puppet-382: Size
    centos67-pe-puppet-382: ========================================
    centos67-pe-puppet-382: Installing:
    centos67-pe-puppet-382: fuse-libs
    centos67-pe-puppet-382: x86_64 2.8.3-4.el6 base  74 k
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Transaction Summary
    centos67-pe-puppet-382: ========================================
    centos67-pe-puppet-382: Install       1 Package(s)
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Total download size: 74 k
    centos67-pe-puppet-382: Installed size: 251 k
    centos67-pe-puppet-382: Downloading Packages:
    centos67-pe-puppet-382: fuse-libs-2.8.3- |  74 kB     00:00
    centos67-pe-puppet-382: Running rpm_check_debug
    centos67-pe-puppet-382: Running Transaction Test
    centos67-pe-puppet-382: Transaction Test Succeeded
    centos67-pe-puppet-382: Running Transaction
    centos67-pe-puppet-382:   Installing : fuse-libs-2.8.3-4.   1/1
    centos67-pe-puppet-382: Verifying  : fuse-libs-2.8.3-4.   1/1
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Installed:
    centos67-pe-puppet-382: fuse-libs.x86_64 0:2.8.3-4.el6
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Complete!
    centos67-pe-puppet-382: total 488
    centos67-pe-puppet-382: drwxr-xr-x   7 root root   4096 Oct 30 07:10 .
    centos67-pe-puppet-382: drwxrwxrwt.  4 root root   4096 Dec 26 20:56 ..
    centos67-pe-puppet-382: drwxr-xr-x   2 root root   4096 Oct 30 07:10 bin
    centos67-pe-puppet-382: drwxr-xr-x   2 root root   4096 Oct 30 07:10 doc
    centos67-pe-puppet-382: drwxr-xr-x   5 root root   4096 Oct 30 07:10 etc
    centos67-pe-puppet-382: -rw-r--r--   1 root root 269196 Oct 30 07:10 FILES
    centos67-pe-puppet-382: -rw-r--r--   1 root root   2538 Oct 30 07:10 INSTALL
    centos67-pe-puppet-382: drwxr-xr-x   2 root root   4096 Oct 30 07:10 installer
    centos67-pe-puppet-382: drwxr-xr-x  15 root root   4096 Oct 30 07:10 lib
    centos67-pe-puppet-382: -rwxr-xr-x   1 root root 196237 Oct 30 07:10 vmware-install.pl
    centos67-pe-puppet-382: Creating a new VMware Tools installer database using the tar4 format.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Installing VMware Tools.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: In which directory do you want to install the binary files?
    centos67-pe-puppet-382: [/usr/bin]
    centos67-pe-puppet-382: The path &amp;quot;yes&amp;quot; is a relative path. Please enter an absolute path.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: In which directory do you want to install the binary files?
    centos67-pe-puppet-382: [/usr/bin]
    centos67-pe-puppet-382: What is the directory that contains the init directories (rc0.d/ to rc6.d/)?
    centos67-pe-puppet-382: [/etc/rc.d]
    centos67-pe-puppet-382: What is the directory that contains the init scripts?
    centos67-pe-puppet-382: [/etc/init.d]
    centos67-pe-puppet-382: In which directory do you want to install the daemon files?
    centos67-pe-puppet-382: [/usr/sbin]
    centos67-pe-puppet-382: In which directory do you want to install the library files?
    centos67-pe-puppet-382: [/usr/lib/vmware-tools]
    centos67-pe-puppet-382: The path &amp;quot;/usr/lib/vmware-tools&amp;quot; does not exist currently. This program is
    centos67-pe-puppet-382: going to create it, including needed parent directories. Is this what you want?
    centos67-pe-puppet-382: [yes]
    centos67-pe-puppet-382: In which directory do you want to install the documentation files?
    centos67-pe-puppet-382: [/usr/share/doc/vmware-tools]
    centos67-pe-puppet-382: The path &amp;quot;/usr/share/doc/vmware-tools&amp;quot; does not exist currently. This program
    centos67-pe-puppet-382: is going to create it, including needed parent directories. Is this what you
    centos67-pe-puppet-382: want? [yes]
    centos67-pe-puppet-382: The installation of VMware Tools 9.9.4 build-3193940 for Linux completed
    centos67-pe-puppet-382: successfully. You can decide to remove this software from your system at any
    centos67-pe-puppet-382: time by invoking the following command: &amp;quot;/usr/bin/vmware-uninstall-tools.pl&amp;quot;.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Before running VMware Tools for the first time, you need to configure it by
    centos67-pe-puppet-382: invoking the following command: &amp;quot;/usr/bin/vmware-config-tools.pl&amp;quot;. Do you want
    centos67-pe-puppet-382: this program to invoke the command for you now? [yes]
    centos67-pe-puppet-382: Initializing...
    centos67-pe-puppet-382:
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Making sure services for VMware Tools are stopped.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382:
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Found a compatible pre-built module for vmci.  Installing it...
    centos67-pe-puppet-382:
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Found a compatible pre-built module for vsock.  Installing it...
    centos67-pe-puppet-382:
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: The module vmxnet3 has already been installed on this system by another
    centos67-pe-puppet-382: installer or package and will not be modified by this installer.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: The module pvscsi has already been installed on this system by another
    centos67-pe-puppet-382: installer or package and will not be modified by this installer.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: The module vmmemctl has already been installed on this system by another
    centos67-pe-puppet-382: installer or package and will not be modified by this installer.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: The VMware Host-Guest Filesystem allows for shared folders between the host OS
    centos67-pe-puppet-382: and the guest OS in a Fusion or Workstation virtual environment.  Do you wish
    centos67-pe-puppet-382: to enable this feature? [no]
    centos67-pe-puppet-382: Found a compatible pre-built module for vmhgfs.  Installing it...
    centos67-pe-puppet-382:
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Found a compatible pre-built module for vmxnet.  Installing it...
    centos67-pe-puppet-382:
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: The vmblock enables dragging or copying files between host and guest in a
    centos67-pe-puppet-382: Fusion or Workstation virtual environment.  Do you wish to enable this feature?
    centos67-pe-puppet-382: [no]
    centos67-pe-puppet-382: VMware automatic kernel modules enables automatic building and installation of
    centos67-pe-puppet-382: VMware kernel modules at boot that are not already present. This feature can be
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: enabled/disabled by re-running vmware-config-tools.pl.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Would you like to enable VMware automatic kernel modules?
    centos67-pe-puppet-382: [no]
    centos67-pe-puppet-382: Do you want to enable Guest Authentication (vgauth)? [yes]
    centos67-pe-puppet-382: No X install found.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Creating a new initrd boot image for the kernel.
    centos67-pe-puppet-382: vmware-tools-thinprint start/running
    centos67-pe-puppet-382: vmware-tools start/running
    centos67-pe-puppet-382: The configuration of VMware Tools 9.9.4 build-3193940 for Linux for this
    centos67-pe-puppet-382: running kernel completed successfully.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: You must restart your X session before any mouse or graphics changes take
    centos67-pe-puppet-382: effect.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: You can now run VMware Tools by invoking &amp;quot;/usr/bin/vmware-toolbox-cmd&amp;quot; from the
    centos67-pe-puppet-382: command line.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: To enable advanced X features (e.g., guest resolution fit, drag and drop, and
    centos67-pe-puppet-382: file and text copy/paste), you will need to do one (or more) of the following:
    centos67-pe-puppet-382: 1. Manually start /usr/bin/vmware-user
    centos67-pe-puppet-382: 2. Log out and log back into your desktop session; and,
    centos67-pe-puppet-382: 3. Restart your X session.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Enjoy,
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: --the VMware team
    centos67-pe-puppet-382:&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The fourth provisioner is the shell provisioner that will run the scripts/centos-install-pe-puppet-382.sh bash script. This is the script we added to the Packer template to install the Puppet Enterprise 3.8.2 agent.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;==&amp;gt; centos67-pe-puppet-382: Provisioning with shell script: scripts/centos-install-pe-puppet-382.sh
    centos67-pe-puppet-382: Fetching puppet-enterprise-3.8.2-el-6-x86_64.tar.gz
    centos67-pe-puppet-382: % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
    centos67-pe-puppet-382: Dload  Upload   Total   Spent    Left  Speed
    centos67-pe-puppet-382: 100  437M  100  437M    0     0  6688k      0  0:01:06  0:01:06 --:--:-- 6879k
    centos67-pe-puppet-382: Extracting puppet-enterprise-3.8.2-el-6-x86_64.tar.gz
    centos67-pe-puppet-382: ========================================================================
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Puppet Enterprise v3.8.2 installer
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Puppet Enterprise documentation can be found at http://docs.puppetlabs.com/pe/3.8/
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ------------------------------------------------------------------------
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: STEP 1: READ ANSWERS FROM FILE
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ## Reading answers from file: ./agent.ans
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ------------------------------------------------------------------------
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: STEP 2: SELECT AND CONFIGURE ROLES
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: This installer lets you select and install the various roles
    centos67-pe-puppet-382: required in a Puppet Enterprise deployment: puppet master,
    centos67-pe-puppet-382: console, database, cloud provisioner, and puppet agent.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: NOTE: when specifying hostnames during installation, use the fully-qualified domain name (foo.example.com) rather than a shortened name (foo).
    centos67-pe-puppet-382:
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: -&amp;gt; puppet master
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: The puppet master serves configurations to a group of puppet
    centos67-pe-puppet-382: agent nodes. This role also provides MCollective&amp;#39;s message queue
    centos67-pe-puppet-382: and client interface. It should be installed on a robust,
    centos67-pe-puppet-382: dedicated server.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ?? Install puppet master? [y/N] n
    centos67-pe-puppet-382: ?? Puppet master hostname to connect to? [Default: puppet] puppet
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: -&amp;gt; database support
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: This role provides database support for PuppetDB and PE&amp;#39;s
    centos67-pe-puppet-382: console. PuppetDB is a centralized data service that caches data
    centos67-pe-puppet-382: generated by Puppet and provides access to it via a robust API.
    centos67-pe-puppet-382: The console uses data provided by a PostgreSQL server and
    centos67-pe-puppet-382: database both of which will be installed along with PuppetDB on
    centos67-pe-puppet-382: the node you specify.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: IMPORTANT: If you choose not to install PuppetDB at this time,
    centos67-pe-puppet-382: you will be prompted for the host name of the node you intend to
    centos67-pe-puppet-382: use to provide database services. Note that you must install
    centos67-pe-puppet-382: database support on that node for the console to function. When
    centos67-pe-puppet-382: using a separate node, you should install database support on it
    centos67-pe-puppet-382: BEFORE installing the console role.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ?? Install PuppetDB? [y/N] n
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: -&amp;gt; console
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: The console is a web interface where you can view reports,
    centos67-pe-puppet-382: classify nodes, control Puppet runs, and invoke MCollective
    centos67-pe-puppet-382: agents. It can be installed on the puppet master&amp;#39;s node, but for
    centos67-pe-puppet-382: performance considerations, especially in larger deployments, it
    centos67-pe-puppet-382: can also be installed on a separate node.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ?? Install the console? [y/N] n
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: -&amp;gt; cloud provisioner
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: The cloud provisioner can create and bootstrap new machine
    centos67-pe-puppet-382: instances and add them to your Puppet infrastructure. It should
    centos67-pe-puppet-382: be installed on a trusted node where site administrators have
    centos67-pe-puppet-382: shell access.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ?? Install the cloud provisioner? [y/N] y
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: -&amp;gt; puppet agent
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: The puppet agent role is automatically installed with the
    centos67-pe-puppet-382: console, puppet master, puppetdb, and cloud provisioner roles.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ?? Puppet agent needs a unique name (&amp;quot;certname&amp;quot;) for its
    centos67-pe-puppet-382: certificate; this can be an arbitrary string. Certname for this
    centos67-pe-puppet-382: node? [Default: localhost] localhost.localdomain
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: -&amp;gt; Vendor Packages
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: The installer has detected that Puppet Enterprise requires
    centos67-pe-puppet-382: additional packages from your operating system vendor&amp;#39;s
    centos67-pe-puppet-382: repositories, and can automatically install them. If you choose
    centos67-pe-puppet-382: not to install these packages automatically, the installer will
    centos67-pe-puppet-382: exit so you can install them manually.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Additional vendor packages required for installation:
    centos67-pe-puppet-382: * dmidecode
    centos67-pe-puppet-382: * libxslt
    centos67-pe-puppet-382: * pciutils
    centos67-pe-puppet-382:
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ?? Install these packages automatically? [Y/n] y
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ------------------------------------------------------------------------
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: STEP 3: CONFIRM PLAN
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: You have selected to install the following components (and their dependencies)
    centos67-pe-puppet-382: * Cloud Provisioner
    centos67-pe-puppet-382: * Puppet Agent
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ?? Perform installation? [Y/n] y
    centos67-pe-puppet-382: ## Answers saved in the following files: /tmp/puppet-enterprise-3.8.2-el-6-x86_64/answers.lastrun.localhost and /etc/puppetlabs/installer/answers.install
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ========================================================================
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ------------------------------------------------------------------------
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: STEP 4: INSTALL PACKAGES
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ## Installing packages from repositories...
    centos67-pe-puppet-382: Loaded plugins: fastestmirror
    centos67-pe-puppet-382: Setting up Install Process
    centos67-pe-puppet-382: Loading mirror speeds from cached hostfile
    centos67-pe-puppet-382: * base: centos.unixheads.org
    centos67-pe-puppet-382: * extras: mirror.rackspace.com
    centos67-pe-puppet-382: * updates: centos-mirror.jchost.net
    centos67-pe-puppet-382: Package zlib-1.2.3-29.el6.x86_64 already installed and latest version
    centos67-pe-puppet-382: Package which-2.19-6.el6.x86_64 already installed and latest version
    centos67-pe-puppet-382: Package net-tools-1.60-110.el6_2.x86_64 already installed and latest version
    centos67-pe-puppet-382: Resolving Dependencies
    centos67-pe-puppet-382: --&amp;gt; Running transaction check
    centos67-pe-puppet-382: ---&amp;gt; Package cronie.x86_64 0:1.4.4-15.el6 will be updated
    centos67-pe-puppet-382: --&amp;gt; Processing Dependency: cronie = 1.4.4-15.el6 for package: cronie-anacron-1.4.4-15.el6.x86_64
    centos67-pe-puppet-382: ---&amp;gt; Package cronie.x86_64 0:1.4.4-15.el6_7.1 will be an update
    centos67-pe-puppet-382: ---&amp;gt; Package dmidecode.x86_64 1:2.12-6.el6 will be installed
    centos67-pe-puppet-382: ---&amp;gt; Package libxml2.x86_64 0:2.7.6-20.el6 will be updated
    centos67-pe-puppet-382: ---&amp;gt; Package libxml2.x86_64 0:2.7.6-20.el6_7.1 will be an update
    centos67-pe-puppet-382: ---&amp;gt; Package libxslt.x86_64 0:1.1.26-2.el6_3.1 will be installed
    centos67-pe-puppet-382: ---&amp;gt; Package pciutils.x86_64 0:3.1.10-4.el6 will be installed
    centos67-pe-puppet-382: ---&amp;gt; Package pe-agent.noarch 0:3.8.2-1.pe.el6 will be installed
    centos67-pe-puppet-382: --&amp;gt; Processing Dependency: pe-virt-what &amp;gt;= 1.13-1.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&amp;gt; Processing Dependency: pe-rubygem-deep-merge &amp;gt;= 1.0.0-3.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&amp;gt; Processing Dependency: pe-ruby-stomp &amp;gt;= 1.3.3-1.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&amp;gt; Processing Dependency: pe-ruby-shadow &amp;gt;= 2.2.0-3.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&amp;gt; Processing Dependency: pe-ruby-selinux &amp;gt;= 2.0.94-4.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&amp;gt; Processing Dependency: pe-ruby-rgen &amp;gt;= 0.6.5-1.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&amp;gt; Processing Dependency: pe-ruby-augeas &amp;gt;= 0.5.0-7.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&amp;gt; Processing Dependency: pe-ruby &amp;gt;= 1.9.3.551-2.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&amp;gt; Processing Dependency: pe-puppet-enterprise-release &amp;gt;= 3.8.2.0-1.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&amp;gt; Processing Dependency: pe-puppet &amp;gt;= 3.8.2.0-1.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&amp;gt; Processing Dependency: pe-openssl &amp;gt;= 1.0.0s-1.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&amp;gt; Processing Dependency: pe-mcollective-common &amp;gt;= 2.7.0.1-1.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&amp;gt; Processing Dependency: pe-mcollective &amp;gt;= 2.7.0.1-1.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&amp;gt; Processing Dependency: pe-libyaml &amp;gt;= 0.1.6-5.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&amp;gt; Processing Dependency: pe-libldap &amp;gt;= 2.4.39-5.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&amp;gt; Processing Dependency: pe-hiera &amp;gt;= 1.3.4.5-1.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&amp;gt; Processing Dependency: pe-facter &amp;gt;= 2.4.4.0-1.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&amp;gt; Processing Dependency: pe-augeas &amp;gt;= 1.3.0-2.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: ---&amp;gt; Package pe-cloud-provisioner.noarch 0:1.2.0-1.el6 will be installed
    centos67-pe-puppet-382: ---&amp;gt; Package pe-cloud-provisioner-libs.x86_64 0:0.3.2-2.pe.el6 will be installed
    centos67-pe-puppet-382: ---&amp;gt; Package pe-ruby-ldap.x86_64 0:0.9.12-7.pe.el6 will be installed
    centos67-pe-puppet-382: ---&amp;gt; Package pe-rubygem-net-ssh.noarch 0:2.1.4-2.pe.el6 will be installed
    centos67-pe-puppet-382: --&amp;gt; Running transaction check
    centos67-pe-puppet-382: ---&amp;gt; Package cronie-anacron.x86_64 0:1.4.4-15.el6 will be updated
    centos67-pe-puppet-382: ---&amp;gt; Package cronie-anacron.x86_64 0:1.4.4-15.el6_7.1 will be an update
    centos67-pe-puppet-382: ---&amp;gt; Package pe-augeas.x86_64 0:1.3.0-2.pe.el6 will be installed
    centos67-pe-puppet-382: ---&amp;gt; Package pe-facter.x86_64 0:2.4.4.0-1.pe.el6 will be installed
    centos67-pe-puppet-382: ---&amp;gt; Package pe-hiera.noarch 0:1.3.4.5-1.pe.el6 will be installed
    centos67-pe-puppet-382: ---&amp;gt; Package pe-libldap.x86_64 0:2.4.39-5.pe.el6 will be installed
    centos67-pe-puppet-382: ---&amp;gt; Package pe-libyaml.x86_64 0:0.1.6-5.el6 will be installed
    centos67-pe-puppet-382: ---&amp;gt; Package pe-mcollective.noarch 0:2.7.0.1-1.pe.el6 will be installed
    centos67-pe-puppet-382: ---&amp;gt; Package pe-mcollective-common.noarch 0:2.7.0.1-1.pe.el6 will be installed
    centos67-pe-puppet-382: ---&amp;gt; Package pe-openssl.x86_64 0:1.0.0s-1.pe.el6 will be installed
    centos67-pe-puppet-382: ---&amp;gt; Package pe-puppet.noarch 0:3.8.2.0-1.pe.el6 will be installed
    centos67-pe-puppet-382: ---&amp;gt; Package pe-puppet-enterprise-release.noarch 0:3.8.2.0-1.pe.el6 will be installed
    centos67-pe-puppet-382: ---&amp;gt; Package pe-ruby.x86_64 0:1.9.3.551-2.pe.el6 will be installed
    centos67-pe-puppet-382: ---&amp;gt; Package pe-ruby-augeas.x86_64 0:0.5.0-7.pe.el6 will be installed
    centos67-pe-puppet-382: ---&amp;gt; Package pe-ruby-rgen.noarch 0:0.6.5-1.pe.el6 will be installed
    centos67-pe-puppet-382: ---&amp;gt; Package pe-ruby-selinux.x86_64 0:2.0.94-4.pe.el6 will be installed
    centos67-pe-puppet-382: ---&amp;gt; Package pe-ruby-shadow.x86_64 0:2.2.0-3.pe.el6 will be installed
    centos67-pe-puppet-382: ---&amp;gt; Package pe-ruby-stomp.noarch 0:1.3.3-1.pe.el6 will be installed
    centos67-pe-puppet-382: ---&amp;gt; Package pe-rubygem-deep-merge.noarch 0:1.0.0-3.pe.el6 will be installed
    centos67-pe-puppet-382: ---&amp;gt; Package pe-virt-what.x86_64 0:1.13-1.el6 will be installed
    centos67-pe-puppet-382: --&amp;gt; Finished Dependency Resolution
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Dependencies Resolved
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ================================================================================
    centos67-pe-puppet-382: Package            Arch   Version            Repository                   Size
    centos67-pe-puppet-382: ================================================================================
    centos67-pe-puppet-382: Installing:
    centos67-pe-puppet-382: dmidecode          x86_64 1:2.12-6.el6       base                         74 k
    centos67-pe-puppet-382: libxslt            x86_64 1.1.26-2.el6_3.1   base                        452 k
    centos67-pe-puppet-382: pciutils           x86_64 3.1.10-4.el6       base                         85 k
    centos67-pe-puppet-382: pe-agent           noarch 3.8.2-1.pe.el6     puppet-enterprise-installer 3.7 k
    centos67-pe-puppet-382: pe-cloud-provisioner
    centos67-pe-puppet-382: noarch 1.2.0-1.el6        puppet-enterprise-installer  51 k
    centos67-pe-puppet-382: pe-cloud-provisioner-libs
    centos67-pe-puppet-382: x86_64 0.3.2-2.pe.el6     puppet-enterprise-installer 4.6 M
    centos67-pe-puppet-382: pe-ruby-ldap       x86_64 0.9.12-7.pe.el6    puppet-enterprise-installer  46 k
    centos67-pe-puppet-382: pe-rubygem-net-ssh noarch 2.1.4-2.pe.el6     puppet-enterprise-installer 227 k
    centos67-pe-puppet-382: Updating:
    centos67-pe-puppet-382: cronie             x86_64 1.4.4-15.el6_7.1   updates                      74 k
    centos67-pe-puppet-382: libxml2            x86_64 2.7.6-20.el6_7.1   updates                     803 k
    centos67-pe-puppet-382: Installing for dependencies:
    centos67-pe-puppet-382: pe-augeas          x86_64 1.3.0-2.pe.el6     puppet-enterprise-installer 523 k
    centos67-pe-puppet-382: pe-facter          x86_64 2.4.4.0-1.pe.el6   puppet-enterprise-installer  99 k
    centos67-pe-puppet-382: pe-hiera           noarch 1.3.4.5-1.pe.el6   puppet-enterprise-installer  18 k
    centos67-pe-puppet-382: pe-libldap         x86_64 2.4.39-5.pe.el6    puppet-enterprise-installer 776 k
    centos67-pe-puppet-382: pe-libyaml         x86_64 0.1.6-5.el6        puppet-enterprise-installer 193 k
    centos67-pe-puppet-382: pe-mcollective     noarch 2.7.0.1-1.pe.el6   puppet-enterprise-installer 8.0 k
    centos67-pe-puppet-382: pe-mcollective-common
    centos67-pe-puppet-382: noarch 2.7.0.1-1.pe.el6   puppet-enterprise-installer 129 k
    centos67-pe-puppet-382: pe-openssl         x86_64 1.0.0s-1.pe.el6    puppet-enterprise-installer 6.7 M
    centos67-pe-puppet-382: pe-puppet          noarch 3.8.2.0-1.pe.el6   puppet-enterprise-installer 1.6 M
    centos67-pe-puppet-382: pe-puppet-enterprise-release
    centos67-pe-puppet-382: noarch 3.8.2.0-1.pe.el6   puppet-enterprise-installer  12 k
    centos67-pe-puppet-382: pe-ruby            x86_64 1.9.3.551-2.pe.el6 puppet-enterprise-installer 8.1 M
    centos67-pe-puppet-382: pe-ruby-augeas     x86_64 0.5.0-7.pe.el6     puppet-enterprise-installer  22 k
    centos67-pe-puppet-382: pe-ruby-rgen       noarch 0.6.5-1.pe.el6     puppet-enterprise-installer 238 k
    centos67-pe-puppet-382: pe-ruby-selinux    x86_64 2.0.94-4.pe.el6    puppet-enterprise-installer  57 k
    centos67-pe-puppet-382: pe-ruby-shadow     x86_64 2.2.0-3.pe.el6     puppet-enterprise-installer  11 k
    centos67-pe-puppet-382: pe-ruby-stomp      noarch 1.3.3-1.pe.el6     puppet-enterprise-installer  54 k
    centos67-pe-puppet-382: pe-rubygem-deep-merge
    centos67-pe-puppet-382: noarch 1.0.0-3.pe.el6     puppet-enterprise-installer  74 k
    centos67-pe-puppet-382: pe-virt-what       x86_64 1.13-1.el6         puppet-enterprise-installer  22 k
    centos67-pe-puppet-382: Updating for dependencies:
    centos67-pe-puppet-382: cronie-anacron     x86_64 1.4.4-15.el6_7.1   updates                      31 k
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Transaction Summary
    centos67-pe-puppet-382: ================================================================================
    centos67-pe-puppet-382: Install      26 Package(s)
    centos67-pe-puppet-382: Upgrade       3 Package(s)
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Total download size: 25 M
    centos67-pe-puppet-382: Downloading Packages:
    centos67-pe-puppet-382: --------------------------------------------------------------------------------
    centos67-pe-puppet-382: Total                                            11 MB/s |  25 MB     00:02
    centos67-pe-puppet-382: Running rpm_check_debug
    centos67-pe-puppet-382: Running Transaction Test
    centos67-pe-puppet-382: Transaction Test Succeeded
    centos67-pe-puppet-382: Running Transaction
    centos67-pe-puppet-382: Installing : pe-puppet-enterprise-release-3.8.2.0-1.pe.el6.noarch        1/32
    centos67-pe-puppet-382: Installing : pe-openssl-1.0.0s-1.pe.el6.x86_64                           2/32
    centos67-pe-puppet-382: Installing : pe-libldap-2.4.39-5.pe.el6.x86_64                           3/32
    centos67-pe-puppet-382: Installing : pe-libyaml-0.1.6-5.el6.x86_64                               4/32
    centos67-pe-puppet-382: Installing : pe-ruby-1.9.3.551-2.pe.el6.x86_64                           5/32
    centos67-pe-puppet-382: Installing : pe-rubygem-net-ssh-2.1.4-2.pe.el6.noarch                    6/32
    centos67-pe-puppet-382: Installing : pe-ruby-stomp-1.3.3-1.pe.el6.noarch                         7/32
    centos67-pe-puppet-382: Installing : pe-mcollective-common-2.7.0.1-1.pe.el6.noarch               8/32
    centos67-pe-puppet-382: Installing : pe-ruby-selinux-2.0.94-4.pe.el6.x86_64                      9/32
    centos67-pe-puppet-382: Installing : pe-ruby-rgen-0.6.5-1.pe.el6.noarch                         10/32
    centos67-pe-puppet-382: Installing : pe-rubygem-deep-merge-1.0.0-3.pe.el6.noarch                11/32
    centos67-pe-puppet-382: Installing : pe-hiera-1.3.4.5-1.pe.el6.noarch                           12/32
    centos67-pe-puppet-382: Installing : pe-augeas-1.3.0-2.pe.el6.x86_64                            13/32
    centos67-pe-puppet-382: Installing : pe-ruby-augeas-0.5.0-7.pe.el6.x86_64                       14/32
    centos67-pe-puppet-382: Updating   : libxml2-2.7.6-20.el6_7.1.x86_64                            15/32
    centos67-pe-puppet-382: Installing : 1:dmidecode-2.12-6.el6.x86_64                              16/32
    centos67-pe-puppet-382: Installing : pe-virt-what-1.13-1.el6.x86_64                             17/32
    centos67-pe-puppet-382: Installing : libxslt-1.1.26-2.el6_3.1.x86_64                            18/32
    centos67-pe-puppet-382: Installing : pe-cloud-provisioner-libs-0.3.2-2.pe.el6.x86_64            19/32
    centos67-pe-puppet-382: Installing : pe-mcollective-2.7.0.1-1.pe.el6.noarch                     20/32
    centos67-pe-puppet-382: Installing : pe-ruby-ldap-0.9.12-7.pe.el6.x86_64                        21/32
    centos67-pe-puppet-382: Installing : pe-ruby-shadow-2.2.0-3.pe.el6.x86_64                       22/32
    centos67-pe-puppet-382: Updating   : cronie-anacron-1.4.4-15.el6_7.1.x86_64                     23/32
    centos67-pe-puppet-382: Updating   : cronie-1.4.4-15.el6_7.1.x86_64                             24/32
    centos67-pe-puppet-382: Installing : pciutils-3.1.10-4.el6.x86_64                               25/32
    centos67-pe-puppet-382: Installing : pe-facter-2.4.4.0-1.pe.el6.x86_64                          26/32
    centos67-pe-puppet-382: Installing : pe-puppet-3.8.2.0-1.pe.el6.noarch                          27/32
    centos67-pe-puppet-382: Installing : pe-agent-3.8.2-1.pe.el6.noarch                             28/32
    centos67-pe-puppet-382: Installing : pe-cloud-provisioner-1.2.0-1.el6.noarch                    29/32
    centos67-pe-puppet-382: Cleanup    : cronie-anacron-1.4.4-15.el6.x86_64                         30/32
    centos67-pe-puppet-382: Cleanup    : cronie-1.4.4-15.el6.x86_64                                 31/32
    centos67-pe-puppet-382: Cleanup    : libxml2-2.7.6-20.el6.x86_64                                32/32
    centos67-pe-puppet-382: Verifying  : pe-puppet-enterprise-release-3.8.2.0-1.pe.el6.noarch        1/32
    centos67-pe-puppet-382: Verifying  : pe-hiera-1.3.4.5-1.pe.el6.noarch                            2/32
    centos67-pe-puppet-382: Verifying  : pe-mcollective-common-2.7.0.1-1.pe.el6.noarch               3/32
    centos67-pe-puppet-382: Verifying  : pciutils-3.1.10-4.el6.x86_64                                4/32
    centos67-pe-puppet-382: Verifying  : pe-libldap-2.4.39-5.pe.el6.x86_64                           5/32
    centos67-pe-puppet-382: Verifying  : pe-ruby-augeas-0.5.0-7.pe.el6.x86_64                        6/32
    centos67-pe-puppet-382: Verifying  : pe-agent-3.8.2-1.pe.el6.noarch                              7/32
    centos67-pe-puppet-382: Verifying  : pe-ruby-ldap-0.9.12-7.pe.el6.x86_64                         8/32
    centos67-pe-puppet-382: Verifying  : 1:dmidecode-2.12-6.el6.x86_64                               9/32
    centos67-pe-puppet-382: Verifying  : pe-mcollective-2.7.0.1-1.pe.el6.noarch                     10/32
    centos67-pe-puppet-382: Verifying  : pe-cloud-provisioner-libs-0.3.2-2.pe.el6.x86_64            11/32
    centos67-pe-puppet-382: Verifying  : cronie-1.4.4-15.el6_7.1.x86_64                             12/32
    centos67-pe-puppet-382: Verifying  : pe-rubygem-net-ssh-2.1.4-2.pe.el6.noarch                   13/32
    centos67-pe-puppet-382: Verifying  : pe-openssl-1.0.0s-1.pe.el6.x86_64                          14/32
    centos67-pe-puppet-382: Verifying  : pe-facter-2.4.4.0-1.pe.el6.x86_64                          15/32
    centos67-pe-puppet-382: Verifying  : pe-ruby-selinux-2.0.94-4.pe.el6.x86_64                     16/32
    centos67-pe-puppet-382: Verifying  : pe-ruby-stomp-1.3.3-1.pe.el6.noarch                        17/32
    centos67-pe-puppet-382: Verifying  : pe-ruby-shadow-2.2.0-3.pe.el6.x86_64                       18/32
    centos67-pe-puppet-382: Verifying  : libxslt-1.1.26-2.el6_3.1.x86_64                            19/32
    centos67-pe-puppet-382: Verifying  : pe-ruby-rgen-0.6.5-1.pe.el6.noarch                         20/32
    centos67-pe-puppet-382: Verifying  : pe-rubygem-deep-merge-1.0.0-3.pe.el6.noarch                21/32
    centos67-pe-puppet-382: Verifying  : pe-puppet-3.8.2.0-1.pe.el6.noarch                          22/32
    centos67-pe-puppet-382: Verifying  : cronie-anacron-1.4.4-15.el6_7.1.x86_64                     23/32
    centos67-pe-puppet-382: Verifying  : pe-libyaml-0.1.6-5.el6.x86_64                              24/32
    centos67-pe-puppet-382: Verifying  : pe-virt-what-1.13-1.el6.x86_64                             25/32
    centos67-pe-puppet-382: Verifying  : pe-augeas-1.3.0-2.pe.el6.x86_64                            26/32
    centos67-pe-puppet-382: Verifying  : pe-cloud-provisioner-1.2.0-1.el6.noarch                    27/32
    centos67-pe-puppet-382: Verifying  : pe-ruby-1.9.3.551-2.pe.el6.x86_64                          28/32
    centos67-pe-puppet-382: Verifying  : libxml2-2.7.6-20.el6_7.1.x86_64                            29/32
    centos67-pe-puppet-382: Verifying  : libxml2-2.7.6-20.el6.x86_64                                30/32
    centos67-pe-puppet-382: Verifying  : cronie-1.4.4-15.el6.x86_64                                 31/32
    centos67-pe-puppet-382: Verifying  : cronie-anacron-1.4.4-15.el6.x86_64                         32/32
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Installed:
    centos67-pe-puppet-382: dmidecode.x86_64 1:2.12-6.el6
    centos67-pe-puppet-382: libxslt.x86_64 0:1.1.26-2.el6_3.1
    centos67-pe-puppet-382: pciutils.x86_64 0:3.1.10-4.el6
    centos67-pe-puppet-382: pe-agent.noarch 0:3.8.2-1.pe.el6
    centos67-pe-puppet-382: pe-cloud-provisioner.noarch 0:1.2.0-1.el6
    centos67-pe-puppet-382: pe-cloud-provisioner-libs.x86_64 0:0.3.2-2.pe.el6
    centos67-pe-puppet-382: pe-ruby-ldap.x86_64 0:0.9.12-7.pe.el6
    centos67-pe-puppet-382: pe-rubygem-net-ssh.noarch 0:2.1.4-2.pe.el6
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Dependency Installed:
    centos67-pe-puppet-382: pe-augeas.x86_64 0:1.3.0-2.pe.el6
    centos67-pe-puppet-382: pe-facter.x86_64 0:2.4.4.0-1.pe.el6
    centos67-pe-puppet-382: pe-hiera.noarch 0:1.3.4.5-1.pe.el6
    centos67-pe-puppet-382: pe-libldap.x86_64 0:2.4.39-5.pe.el6
    centos67-pe-puppet-382: pe-libyaml.x86_64 0:0.1.6-5.el6
    centos67-pe-puppet-382: pe-mcollective.noarch 0:2.7.0.1-1.pe.el6
    centos67-pe-puppet-382: pe-mcollective-common.noarch 0:2.7.0.1-1.pe.el6
    centos67-pe-puppet-382: pe-openssl.x86_64 0:1.0.0s-1.pe.el6
    centos67-pe-puppet-382: pe-puppet.noarch 0:3.8.2.0-1.pe.el6
    centos67-pe-puppet-382: pe-puppet-enterprise-release.noarch 0:3.8.2.0-1.pe.el6
    centos67-pe-puppet-382: pe-ruby.x86_64 0:1.9.3.551-2.pe.el6
    centos67-pe-puppet-382: pe-ruby-augeas.x86_64 0:0.5.0-7.pe.el6
    centos67-pe-puppet-382: pe-ruby-rgen.noarch 0:0.6.5-1.pe.el6
    centos67-pe-puppet-382: pe-ruby-selinux.x86_64 0:2.0.94-4.pe.el6
    centos67-pe-puppet-382: pe-ruby-shadow.x86_64 0:2.2.0-3.pe.el6
    centos67-pe-puppet-382: pe-ruby-stomp.noarch 0:1.3.3-1.pe.el6
    centos67-pe-puppet-382: pe-rubygem-deep-merge.noarch 0:1.0.0-3.pe.el6
    centos67-pe-puppet-382: pe-virt-what.x86_64 0:1.13-1.el6
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Updated:
    centos67-pe-puppet-382: cronie.x86_64 0:1.4.4-15.el6_7.1       libxml2.x86_64 0:2.7.6-20.el6_7.1
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Dependency Updated:
    centos67-pe-puppet-382: cronie-anacron.x86_64 0:1.4.4-15.el6_7.1
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Complete!
    centos67-pe-puppet-382: Loaded plugins: fastestmirror
    centos67-pe-puppet-382: Cleaning repos: puppet-enterprise-installer
    centos67-pe-puppet-382: Cleaning up Everything
    centos67-pe-puppet-382: Cleaning up list of fastest mirrors
    centos67-pe-puppet-382: ## Checking the agent certificate name detection...
    centos67-pe-puppet-382: ## Setting up puppet agent...
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ------------------------------------------------------------------------
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: STEP 5: DONE
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Thanks for installing Puppet Enterprise!
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: To learn more and get started using Puppet Enterprise, refer to
    centos67-pe-puppet-382: the Puppet Enterprise Quick Start Guide
    centos67-pe-puppet-382: (http://docs.puppetlabs.com/pe/latest/quick_start.html) and the
    centos67-pe-puppet-382: Puppet Enterprise Deployment Guide
    centos67-pe-puppet-382: (http://docs.puppetlabs.com/guides/deployment_guide/index.html).
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ========================================================================
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ## NOTES
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Puppet Enterprise has been installed to &amp;quot;/opt/puppet,&amp;quot; and its
    centos67-pe-puppet-382: configuration files are located in &amp;quot;/etc/puppetlabs&amp;quot;.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Answers from this session saved to
    centos67-pe-puppet-382: &amp;#39;/tmp/puppet-enterprise-3.8.2-el-6-x86_64/answers.lastrun.localhost&amp;#39;
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: If you have a firewall running, please ensure outbound
    centos67-pe-puppet-382: connections are allowed to the following TCP ports: 8140, 61613
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ------------------------------------------------------------------------
    centos67-pe-puppet-382:&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;The final provisioner is a shell provisioner that will run the scripts/centos-vmware-cleanup.sh bash script. This script will clean up the centos67 virtual machine and zero out all unused disk space to reduce the size of the image.
&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;==&amp;gt; centos67-pe-puppet-382: Provisioning with shell script: scripts/centos-vmware-cleanup.sh
    centos67-pe-puppet-382: ==&amp;gt; Pausing for 0 seconds...
    centos67-pe-puppet-382: ==&amp;gt; erasing unused packages to free up space
    centos67-pe-puppet-382: Loaded plugins: fastestmirror
    centos67-pe-puppet-382: Setting up Remove Process
    centos67-pe-puppet-382: No Match for argument: gtk2
    centos67-pe-puppet-382: Determining fastest mirrors
    centos67-pe-puppet-382: * base: centos.unixheads.org
    centos67-pe-puppet-382: * extras: centos.unixheads.org
    centos67-pe-puppet-382: * updates: centos-mirror.jchost.net
    centos67-pe-puppet-382: Package(s) gtk2 available, but not installed.
    centos67-pe-puppet-382: No Match for argument: libX11
    centos67-pe-puppet-382: Package(s) libX11 available, but not installed.
    centos67-pe-puppet-382: No Match for argument: hicolor-icon-theme
    centos67-pe-puppet-382: Package(s) hicolor-icon-theme available, but not installed.
    centos67-pe-puppet-382: No Match for argument: avahi
    centos67-pe-puppet-382: Package(s) avahi available, but not installed.
    centos67-pe-puppet-382: No Match for argument: freetype
    centos67-pe-puppet-382: Package(s) freetype available, but not installed.
    centos67-pe-puppet-382: No Match for argument: bitstream-vera-fonts
    centos67-pe-puppet-382: No Packages marked for removal
    centos67-pe-puppet-382: ==&amp;gt; Cleaning up yum cache
    centos67-pe-puppet-382: Loaded plugins: fastestmirror
    centos67-pe-puppet-382: Cleaning repos: base extras updates
    centos67-pe-puppet-382: Cleaning up Everything
    centos67-pe-puppet-382: Cleaning up list of fastest mirrors
    centos67-pe-puppet-382: ==&amp;gt; Force logs to rotate
    centos67-pe-puppet-382: ==&amp;gt; Clear audit log and wtmp
    centos67-pe-puppet-382: ==&amp;gt; Cleaning up udev rules
    centos67-pe-puppet-382: ==&amp;gt; Remove the traces of the template MAC address and UUIDs
    centos67-pe-puppet-382: ==&amp;gt; Cleaning up tmp
    centos67-pe-puppet-382: ==&amp;gt; Remove the SSH host keys
    centos67-pe-puppet-382: ==&amp;gt; Remove the root user’s shell history
    centos67-pe-puppet-382: ==&amp;gt; yum -y clean all
    centos67-pe-puppet-382: Loaded plugins: fastestmirror
    centos67-pe-puppet-382: Cleaning repos: base extras updates
    centos67-pe-puppet-382: Cleaning up Everything
    centos67-pe-puppet-382: /
    centos67-pe-puppet-382: 24828344
    centos67-pe-puppet-382: /tmp/script_7943.sh: line 59: bc: command not found
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: dd: invalid number `&amp;#39;
    centos67-pe-puppet-382: /boot
    centos67-pe-puppet-382: 60502
    centos67-pe-puppet-382: /tmp/script_7943.sh: line 59: bc: command not found
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: dd: invalid number `&amp;#39;
    centos67-pe-puppet-382: /tmp/script_7943.sh: line 67: /usr/sbin/vgdisplay: No such file or directory
    centos67-pe-puppet-382: ==&amp;gt; Zero out the free space to save space in the final image
    centos67-pe-puppet-382: dd: writing `/EMPTY&amp;#39;: No space left on device
    centos67-pe-puppet-382: 12783+0 records in
    centos67-pe-puppet-382: 12782+0 records out
    centos67-pe-puppet-382: 13403570176 bytes (13 GB) copied, 428.651 s, 31.3 MB/s&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;With the provisioners block having been completed, Packer will now shutdown the centos67-pe-puppet-382 virtual machine and unregister it from the ESXi virtual machine.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;==&amp;gt; centos67-pe-puppet-382: Gracefully halting virtual machine...
    centos67-pe-puppet-382: Waiting for VMware to clean up after itself...
==&amp;gt; centos67-pe-puppet-382: Deleting unnecessary VMware files...
    centos67-pe-puppet-382: Deleting: /vmfs/volumes/datastore1/output-centos67-pe-puppet-382/vmware.log
==&amp;gt; centos67-pe-puppet-382: Cleaning VMX prior to finishing up...
    centos67-pe-puppet-382: Unmounting floppy from VMX...
    centos67-pe-puppet-382: Detaching ISO from CD-ROM device...
    centos67-pe-puppet-382: Disabling VNC server...
==&amp;gt; centos67-pe-puppet-382: Compacting the disk image
==&amp;gt; centos67-pe-puppet-382: Unregistering virtual machine...
Build &amp;#39;centos67-pe-puppet-382&amp;#39; finished.

==&amp;gt; Builds finished. The artifacts of successful builds are:
--&amp;gt; centos67-pe-puppet-382: VM files in directory: /vmfs/volumes/datastore1/output-centos67-pe-puppet-382
[root@packer-centos packer-templates]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The output of the packer build command shows that the template was successfully created at /vmfs/volumes/datastore1/output-centos67-pe-puppet-382 on the ESXi virtual machine.&lt;/p&gt;

&lt;h3&gt;This brings us to the end of this post. I think it is pretty powerful that we only had to copy/update the template and add another provisioner script, in order to modify the Packer template we created last time to install Puppet Enterprise 3.8.2.&lt;/h3&gt;

&lt;h3&gt;If you would like to not have to manually create the two files covered in this post, you can clone down &lt;a href=&quot;https://github.com/sdorsett/packer-templates/tree/adding-second-packer-template&quot;&gt;this github repository&lt;/a&gt; by running the following command:&lt;/h3&gt;

&lt;pre&gt;
git clone -b &quot;adding-second-packer-template&quot; https://github.com/sdorsett/packer-templates.git
&lt;/pre&gt;  

&lt;p&gt;If you cloned the packer-templates repo in the last post, you can pull down the updates by running the following command:&lt;/p&gt;

&lt;pre&gt;
git fetch --all
git pull origin &quot;adding-second-packer-template&quot;
&lt;/pre&gt;  

&lt;h3&gt;In the next post we will extend what was covered in this post by installing the Puppet agent in Packer image.&lt;/h3&gt;

&lt;h3&gt;Please provide any feedback or suggestions to my twitter account located on the about page.&lt;/h3&gt;
</content>
 </entry>
 
 <entry>
   <title>Creating our first Packer template for installing CentOS 6.7 with vmtools</title>
   <link href="http://0.0.0.0:4000/2015/12/25/creating-a-packer-template-for-installing-centos-67/"/>
   <updated>2015-12-25T00:00:00+00:00</updated>
   <id>http://0.0.0.0:4000/2015/12/25/creating-a-packer-template-for-installing-centos-67</id>
   <content type="html">&lt;p&gt;This is the fourth in a series of posts on &lt;a href=&quot;https://sdorsett.github.io/2015/12/22/pipeline-for-creating-packer-box-files/&quot;&gt;using a Packer pipeline to generate Vagrant .box files&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the last two posts we covered &lt;a href=&quot;https://sdorsett.github.io/2015/12/23/installing-esxi-virtual-machine-for-packer-depolyment/&quot;&gt;installing a ESXi virtual machine for use with Packer&lt;/a&gt; and &lt;a href=&quot;https://sdorsett.github.io/2015/12/24/installing-packer-and-ovftool-on-centos/&quot;&gt;Setting up Packer, ovftool and Apache web server on a CentOS virtual machine&lt;/a&gt;. In this post we will be putting all this prep work to use in order to install a CentOS 6.7 image using Packer.&lt;/p&gt;

&lt;p&gt;Before we get started I would like to mentioned that many of the configuration files I am using have been influenced by or directly copied from the following github repositories:  &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/frapposelli/packer-templates/&quot;&gt;https://github.com/frapposelli/packer-templates&lt;/a&gt;&lt;br&gt;
&lt;a href=&quot;https://github.com/nanliu/packer-templates/&quot;&gt;https://github.com/nanliu/packer-templates/&lt;/a&gt;  &lt;/p&gt;

&lt;p&gt;These public repositories by Fabio and Nan have been extremely helpful in my own understanding of how Packer works.
 Well...enough talking, let&amp;#39;s get started...&lt;/p&gt;

&lt;h2&gt;1. Start off by connecting by SSH to CentOS virtual machine we created in the previous post.&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;sdorsett-mbp:~ sdorsett$ ssh root@192.168.1.52
root@192.168.1.52&amp;#39;s password:
Last login: Wed Dec 23 16:58:41 2015 from 192.168.1.163
[root@packer-centos ~]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;2. Create a new folder for our packer-templates and run &amp;quot;git init&amp;quot; to start tracking changes.&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos ~]# mkdir packer-templates
[root@packer-centos ~]# cd packer-templates/
[root@packer-centos packer-templates]# git init
Initialized empty Git repository in /root/packer-templates/.git/
[root@packer-centos packer-templates]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;3. Create directory structure for the .iso files, scripts and templates we will be using with Packer.&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos packer-templates]# mkdir -p ~/packer-templates/{iso,scripts,templates}
[root@packer-centos packer-templates]# ls
iso  scripts  templates
[root@packer-centos packer-templates]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;4. Download the CentOS 6.7 minimal .iso and place it in the packer-templates/iso/ folder&lt;/h2&gt;

&lt;p&gt;Open &lt;a href=&quot;http://isoredirect.centos.org/centos/6/isos/x86_64/&quot;&gt;http://isoredirect.centos.org/centos/6/isos/x86_64/&lt;/a&gt; in a browser and pick a mirror to download the .iso from. Copy the URL to the CentOS-6.7-x86_64-minimal.iso image file and use wget to pull the image down in the CentOS virtual machine&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos packer-templates]# cd ~/packer-templates/iso/
[root@packer-centos iso]# wget http://mirror.cs.pitt.edu/centos/6.7/isos/x86_64/CentOS-6.7-x86_64-minimal.iso
--2015-12-23 20:04:30--  http://mirror.cs.pitt.edu/centos/6.7/isos/x86_64/CentOS-6.7-x86_64-minimal.iso
Resolving mirror.cs.pitt.edu... 136.142.23.206
Connecting to mirror.cs.pitt.edu|136.142.23.206|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 414187520 (395M) [application/octet-stream]
Saving to: “CentOS-6.7-x86_64-minimal.iso”

100%[========================================================&amp;gt;] 414,187,520 7.01M/s   in 58s

2015-12-23 20:05:28 (6.77 MB/s) - “CentOS-6.7-x86_64-minimal.iso” saved [414187520/414187520]

[root@packer-centos iso]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;5. Compare the sha1sum of the downloaded CentOS-6.7-x86_64-minimal.iso file to what is listed on the CentOS mirror.&lt;/h2&gt;

&lt;p&gt;Open the sha1sum.txt file from the CentOS mirror you chose in your browser. It should show the following:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;7bb8c1c23a4fdef93e6f0a6347d570e5764d0b38  CentOS-6.7-x86_64-bin-DVD1.iso
79f58df5723f723fc62d5e8831ada38072096a46  CentOS-6.7-x86_64-bin-DVD2.iso
2ed5ea551dffc3e4b82847b3cee1f6cd748e8071  CentOS-6.7-x86_64-minimal.iso
c3678c6b72cbf2ed9b4e8a1ddb190fd262db8b7f  CentOS-6.7-x86_64-netinstall.iso
a7c05a7c0a4c6bdef8dd1cf8310613c154c2b9d5  CentOS-6.7-x86_64-LiveCD.iso
aebd4c2e81ace92ebda2bc909aca34a139ff2cea  CentOS-6.7-x86_64-LiveDVD.iso&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Compare the sha1sum from above to the actual sha1sum of the downloaded .iso&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos iso]# sha1sum CentOS-6.7-x86_64-minimal.iso
2ed5ea551dffc3e4b82847b3cee1f6cd748e8071  CentOS-6.7-x86_64-minimal.iso
[root@packer-centos iso]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The two hashes match so we know the file was downloaded without any corruption.&lt;/p&gt;

&lt;h2&gt;6. Download patched vmtools from &lt;a href=&quot;https://github.com/rasa/vmware-tools-patches&quot;&gt;https://github.com/rasa/vmware-tools-patches&lt;/a&gt; and place the .iso file in the packer-templates/iso/ folder.&lt;/h2&gt;

&lt;p&gt;I have had issues with the VMware issued vmtools, specifically around CentOS 7 and trying to get hgfs (host guest file system) functional on Fusion. During that troubleshooting I discovered the patched vmware tools listed above and found installing those, rather than the VMware issue vmtools installer, resolved my issues.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos ~]# wget https://softwareupdate.vmware.com/cds/vmw-desktop/fusion/7.1.3/3204469/packages/com.vmware.fusion.tools.linux.zip.tar
--2015-12-23 21:20:55--  https://softwareupdate.vmware.com/cds/vmw-desktop/fusion/7.1.3/3204469/packages/com.vmware.fusion.tools.linux.zip.tar
Resolving softwareupdate.vmware.com... 23.64.167.143, 2600:1404:a:589::2ef, 2600:1404:a:59a::2ef
Connecting to softwareupdate.vmware.com|23.64.167.143|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 62074880 (59M) [application/x-tar]
Saving to: “com.vmware.fusion.tools.linux.zip.tar”

100%[========================================================&amp;gt;] 62,074,880  6.95M/s   in 8.6s

2015-12-23 21:21:04 (6.91 MB/s) - “com.vmware.fusion.tools.linux.zip.tar” saved [62074880/62074880]

[root@packer-centos ~]# tar xf com.vmware.fusion.tools.linux.zip.tar
[root@packer-centos ~]# unzip com.vmware.fusion.tools.linux.zip
Archive:  com.vmware.fusion.tools.linux.zip
  inflating: manifest.plist
   creating: payload/
  inflating: payload/linux.iso
 extracting: payload/linux.iso.sig
  inflating: payload/tools-linux.plist
[root@packer-centos ~]# cp payload/linux.iso ~/packer-templates/iso/
[root@packer-centos ~]# cd ~/packer-templates/iso/
[root@packer-centos iso]# ln -s ./linux.iso vmware-tools-linux.iso
[root@packer-centos iso]# ls -la
total 466960
drwxr-xr-x 2 root root      4096 Dec 23 21:22 .
drwxr-xr-x 6 root root      4096 Dec 23 21:22 ..
-rw-r--r-- 1 root root 414187520 Aug  4 21:59 CentOS-6.7-x86_64-minimal.iso
-rw-r--r-- 1 root root  63971328 Dec 23 21:21 linux.iso
lrwxrwxrwx 1 root root        11 Dec 23 21:22 vmware-tools-linux.iso -&amp;gt; ./linux.iso
[root@packer-centos iso]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;7. Create a packer .json file in the template/ folder for our image&lt;/h2&gt;

&lt;p&gt;Create the following file in the packer-templates/templates/ directory. The centos67.json file contains the instructions Packer will use to install the CentOS 6.7 image.&lt;/p&gt;

&lt;pre&gt;
[root@packer-centos packer-templates]# cat templates/centos67.json
{
  &quot;variables&quot;: {
    &quot;version&quot;: &quot;1.0&quot;
  },
  &quot;builders&quot;: [
    {
      &quot;name&quot;: &quot;centos67&quot;,
      &quot;vm_name&quot;: &quot;centos67&quot;,
      &quot;vmdk_name&quot;: &quot;centos67&quot;,
      &quot;type&quot;: &quot;vmware-iso&quot;,
      &quot;communicator&quot;: &quot;ssh&quot;,
      &quot;ssh_pty&quot;: &quot;true&quot;,
      &quot;headless&quot;: false,
      &quot;disk_size&quot;: 16384,
      &quot;guest_os_type&quot;: &quot;rhel6-64&quot;,
      &quot;iso_url&quot;: &quot;./iso/CentOS-6.7-x86_64-minimal.iso&quot;,
      &quot;iso_checksum&quot;: &quot;2ed5ea551dffc3e4b82847b3cee1f6cd748e8071&quot;,
      &quot;iso_checksum_type&quot;: &quot;sha1&quot;,
      &quot;shutdown_command&quot;: &quot;echo 'vagrant' | sudo -S /sbin/shutdown -P now&quot;,

      &quot;remote_host&quot;: &quot;{{user `packer_remote_host`}}&quot;,
      &quot;remote_datastore&quot;: &quot;{{user `packer_remote_datastore`}}&quot;,
      &quot;remote_username&quot;: &quot;{{user `packer_remote_username`}}&quot;,
      &quot;remote_password&quot;: &quot;{{user `packer_remote_password`}}&quot;,
      &quot;remote_type&quot;: &quot;esx5&quot;,
      &quot;ssh_username&quot;: &quot;root&quot;,
      &quot;ssh_password&quot;: &quot;vagrant&quot;,
      &quot;ssh_wait_timeout&quot;: &quot;60m&quot;,
      &quot;tools_upload_flavor&quot;: &quot;linux&quot;,
      &quot;http_directory&quot;: &quot;.&quot;,
      &quot;boot_wait&quot;: &quot;5s&quot;,
      &quot;vmx_data&quot;: {
        &quot;memsize&quot;: &quot;4096&quot;,
        &quot;numvcpus&quot;: &quot;2&quot;,
        &quot;ethernet0.networkName&quot;: &quot;{{user `packer_remote_network`}}&quot;,
        &quot;ethernet0.present&quot;: &quot;TRUE&quot;,
        &quot;ethernet0.startConnected&quot;: &quot;TRUE&quot;,
        &quot;ethernet0.virtualDev&quot;: &quot;e1000&quot;,
        &quot;ethernet0.addressType&quot;: &quot;generated&quot;,
        &quot;ethernet0.generatedAddressOffset&quot;: &quot;0&quot;,
        &quot;ethernet0.wakeOnPcktRcv&quot;: &quot;FALSE&quot;

      },
      &quot;vmx_data_post&quot;: {
        &quot;ide1:0.startConnected&quot;: &quot;FALSE&quot;,
        &quot;ide1:0.clientDevice&quot;: &quot;TRUE&quot;,
        &quot;ide1:0.fileName&quot;: &quot;emptyBackingString&quot;,
        &quot;ethernet0.virtualDev&quot;: &quot;vmxnet3&quot;
      },
      &quot;boot_command&quot;: [
        &quot;&lt;tab&gt; text ks=http://:/scripts/centos-6-kickstart.cfg&lt;enter&gt;&lt;wait&gt;&quot;
      ]
    }
  ],
  &quot;provisioners&quot;: [
    {
      &quot;type&quot;: &quot;file&quot;,
      &quot;source&quot;: &quot;iso/vmware-tools-linux.iso&quot;,
      &quot;destination&quot;: &quot;/tmp/vmware-tools-linux.iso&quot;
    },
    {
      &quot;type&quot;: &quot;shell&quot;,
      &quot;execute_command&quot;: &quot;echo 'vagrant' |  sudo -E -S sh ''&quot;,
      &quot;script&quot;: &quot;scripts/centos-vagrant-settings.sh&quot;
    },
    {
      &quot;type&quot;: &quot;shell&quot;,
      &quot;execute_command&quot;: &quot;echo 'vagrant' |  sudo -E -S sh ''&quot;,
      &quot;script&quot;: &quot;scripts/centos-vmware-tools_install.sh&quot;
    },
    {
      &quot;type&quot;: &quot;shell&quot;,
      &quot;execute_command&quot;: &quot;echo 'vagrant' |  sudo -E -S sh ''&quot;,
      &quot;script&quot;: &quot;scripts/centos-vmware-cleanup.sh&quot;
    }
  ]
}
&lt;/pre&gt;

&lt;p&gt;Let us go over some of what&amp;#39;s in this .json file&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the builders block&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This section contains builder specific instruction. We are using the vmware-iso builder so much of this is describing how we want to VMware backed virtual machine configured.&lt;/p&gt;

&lt;p&gt;You might notice something a little different about remote_host, remote_datastore, remote_username, remote_password and ethernet0.networkName parameters. These parameters contain information about the ESXi host we are using to create the Packer image, so rather than hard coding this info into each and every template these values can be pulled from user defined variables.&lt;/p&gt;

&lt;p&gt;The benefit of separating &amp;quot;data from code&amp;quot; is two fold. First, this prevents sensitive datacenter specific information from being included in our template, which could end up in github or some other git repository. Secondly, we only need to update one location if the password of our remote_host ever needs to be changed.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the provisioners block&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This section contains instruction on what to do after the OS is successfully installed packer. You can use &amp;quot;file&amp;quot; provisioners to copy files into the Packer built OS, as well as &amp;quot;shell&amp;quot; provisioners to run scripts to perform additional configuration. You will get a closer look at &amp;quot;shell&amp;quot; provisioner scripts shortly&lt;/p&gt;

&lt;h2&gt;8. Create the script/ files that Packer will use to install the CentOS 6.7 image&lt;/h2&gt;

&lt;p&gt;The following files are referenced in the centos67.json template and need to be created in the packer-templates/scripts/ folder&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;scripts/centos-6-kickstart.cfg - this is the CentOS 6 kickstart file that describes how CentOS 6 should be installed&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;
[root@packer-centos packer-templates]# cat scripts/centos-6-kickstart.cfg
firewall --disabled

install
cdrom

lang en_US.UTF-8
keyboard us
timezone  Europe/Rome

network --bootproto=dhcp
rootpw vagrant
authconfig --enableshadow --passalgo=sha512

selinux —-disabled
bootloader --location=mbr
text
skipx

logging --level=info
zerombr

clearpart --all --initlabel
autopart

auth  --useshadow  --enablemd5
firstboot --disabled
reboot

%packages --ignoremissing
@Base
@Core
%end

%post
yum install wget git -y
VAGRANT_USER=vagrant
/usr/sbin/groupadd vagrant
/usr/sbin/useradd vagrant -g vagrant -G wheel
echo &quot;vagrant&quot;|passwd --stdin vagrant
echo &quot;vagrant        ALL=(ALL)       NOPASSWD: ALL&quot; &gt;&gt; /etc/sudoers

sed -i &quot;s/^.*requiretty/#Defaults requiretty/&quot; /etc/sudoers
[root@packer-centos packer-templates]#
&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;scripts/centos-vagrant-settings.sh - this is a bash script that describes how vagrant speciffied settings should be configured&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;
[root@packer-centos packer-templates]# cat scripts/centos-vagrant-settings.sh
#!/bin/bash -eux

# Vagrant specific
date &gt; /etc/vagrant_box_build_time

VAGRANT_USER=vagrant
VAGRANT_HOME=/home/$VAGRANT_USER
VAGRANT_KEY_URL=https://raw.github.com/mitchellh/vagrant/master/keys/vagrant.pub

# Add vagrant user (if it doesn't already exist)
if ! id -u $VAGRANT_USER &gt;/dev/null 2&gt;&amp;1; then
    /usr/sbin/groupadd $VAGRANT_USER
    /usr/sbin/useradd $VAGRANT_USER -g $VAGRANT_USER -G wheel
    echo &quot;${VAGRANT_USER}&quot;|passwd --stdin $VAGRANT_USER
    echo &quot;${VAGRANT_USER}        ALL=(ALL)       NOPASSWD: ALL&quot; &gt;&gt; /etc/sudoers
fi

# Installing vagrant keys
mkdir -pm 700 $VAGRANT_HOME/.ssh
echo &quot;ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA6NF8iallvQVp22WDkTkyrtvp9eWW6A8YVr+kz4TjGYe7gHzIw+niNltGEFHzD8+v1I2YJ6oXevct1YeS0o9HZyN1Q9qgCgzUFtdOKLv6IedplqoPkcmF0aYet2PkEDo3MlTBckFXPITAMzF8dJSIFo9D8HfdOV0IAdx4O7PtixWKn5y2hMNG0zQPyUecp4pzC6kivAIhyfHilFR61RGL+GPXQ2MWZWFYbAGjyiYJnAmCP3NOTd0jMZEnDkbUvxhMmBYSdETk1rRgm+R4LOzFUGaHqHDLKLX+FIPKcF96hrucXzcWyLbIbEgE98OHlnVYCzRdK8jlqm8tehUc9c9WhQ== vagrant insecure public key&quot; &gt;&gt; $VAGRANT_HOME/.ssh/authorized_keys
chmod 0600 $VAGRANT_HOME/.ssh/authorized_keys
chown -R $VAGRANT_USER:$VAGRANT_USER $VAGRANT_HOME/.ssh
[root@packer-centos packer-templates]#
&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;scripts/centos-vmware-tools_install.sh - this is a bash script that describes how vmtools should be installed&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;
[root@packer-centos packer-templates]# cat scripts/centos-vmware-tools_install.sh
#!/bin/bash
sudo yum upgrade ca-certificates -y
sudo yum install perl fuse-libs -y
sudo mkdir /mnt/cdrom
sudo mount -o loop /tmp/vmware-tools-linux.iso /mnt/cdrom
sudo cp /mnt/cdrom/VMwareTools*.tar.gz /tmp/
cd /tmp
tar xfz VMwareTools*.tar.gz
cd /tmp/vmware-tools-distrib
ls -la

# from http://www.virtuallyghetto.com/2015/06/automating-silent-installation-of-vmware-tools-on-linux-wautomatic-kernel-modules.html
# If you wish to change which Kernel modules get installed
# The last four entries (yes,no,no,no) map to the following:
#   VMware Host-Guest Filesystem
#   vmblock enables dragging or copying files
#   VMware automatic kernel modules
#   Guest Authentication
# and you can also change the other params as well
sudo cat &gt; /tmp/answer &lt;&lt; __ANSWER__
yes
/usr/bin
/etc
/etc/init.d
/usr/sbin
/usr/lib/vmware-tools
yes
/usr/share/doc/vmware-tools
yes
yes
yes
no
yes
no

__ANSWER__

sudo /tmp/vmware-tools-distrib/vmware-install.pl &lt; /tmp/answer

sudo umount /mnt/cdrom
cd /tmp
rm -rf vmware-tools-distrib
rm -f VMwareTools*.tar.gz
rm /tmp/vmware-tools-linux.iso
[root@packer-centos packer-templates]#
&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;scripts/centos-vmware-cleanup.sh - a bash script that cleans up the Packer image prior to being exported&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;
[root@packer-centos packer-templates]# cat scripts/centos-vmware-cleanup.sh
#!/bin/bash -eux
# Based off the great &quot;Preparing Linux Template VMs&quot;
# (http://lonesysadmin.net/2013/03/26/preparing-linux-template-vms/) article
# by Bob Plankers, thanks Bob!

CLEANUP_PAUSE=${CLEANUP_PAUSE:-0}
echo &quot;==&gt; Pausing for ${CLEANUP_PAUSE} seconds...&quot;
sleep ${CLEANUP_PAUSE}

echo &quot;==&gt; erasing unused packages to free up space&quot;
/usr/bin/yum -y erase gtk2 libX11 hicolor-icon-theme avahi freetype bitstream-vera-fonts

echo &quot;==&gt; Cleaning up yum cache&quot;
/usr/bin/yum clean all

echo &quot;==&gt; Force logs to rotate&quot;
/usr/sbin/logrotate -f /etc/logrotate.conf
/bin/rm -f /var/log/*-???????? /var/log/*.gz

echo &quot;==&gt; Clear audit log and wtmp&quot;
/bin/cat /dev/null &gt; /var/log/audit/audit.log
/bin/cat /dev/null &gt; /var/log/wtmp

echo &quot;==&gt; Cleaning up udev rules&quot;
/bin/rm -f /etc/udev/rules.d/70*

echo &quot;==&gt; Remove the traces of the template MAC address and UUIDs&quot;
/bin/sed -i '/^\(HWADDR\|UUID\)=/d' /etc/sysconfig/network-scripts/ifcfg-eth0

echo &quot;==&gt; Cleaning up tmp&quot;
/bin/rm -rf /tmp/*
/bin/rm -rf /var/tmp/*

echo &quot;==&gt; Remove the SSH host keys&quot;
/bin/rm -f /etc/ssh/*key*

echo &quot;==&gt; Remove the root user’s shell history&quot;
/bin/rm -f ~root/.bash_history
unset HISTFILE

echo &quot;==&gt; yum -y clean all&quot;
yum -y clean all

# Determine the version of RHEL
COND=`grep -i Taroon /etc/redhat-release`
if [ &quot;$COND&quot; = &quot;&quot; ]; then
        export PREFIX=&quot;/usr/sbin&quot;
else
        export PREFIX=&quot;/sbin&quot;
fi

FileSystem=`grep ext /etc/mtab| awk -F&quot; &quot; '{ print $2 }'`

for i in $FileSystem
do
        echo $i
        number=`df -B 512 $i | awk -F&quot; &quot; '{print $3}' | grep -v Used`
        echo $number
        percent=$(echo &quot;scale=0; $number * 98 / 100&quot; | bc )
        echo $percent
        dd count=`echo $percent` if=/dev/zero of=`echo $i`/zf
        /bin/sync
        sleep 15
        rm -f $i/zf
done

VolumeGroup=`$PREFIX/vgdisplay | grep Name | awk -F&quot; &quot; '{ print $3 }'`

for j in $VolumeGroup
do
        echo $j
        $PREFIX/lvcreate -l `$PREFIX/vgdisplay $j | grep Free | awk -F&quot; &quot; '{ print $5 }'` -n zero $j
        if [ -a /dev/$j/zero ]; then
                cat /dev/zero &gt; /dev/$j/zero
                /bin/sync
                sleep 15
                $PREFIX/lvremove -f /dev/$j/zero
        fi
done

echo &quot;==&gt; Zero out the free space to save space in the final image&quot;
dd if=/dev/zero of=/EMPTY bs=1M
rm -f /EMPTY

# Make sure we wait until all the data is written to disk, otherwise
# Packer might quit too early before the large files are deleted
sync
[root@packer-centos packer-templates]#
&lt;/pre&gt;

&lt;h2&gt;9. Create packer-remote-info.json file that contains info on our ESXi virtual machine&lt;/h2&gt;

&lt;p&gt;Create the following file that will contain the ESXi virtual machine specific information used by Packer. We will also add this file to .gitignore in a future step, to ensure this information does not get stored in a git repository.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos packer-templates]# cat &amp;lt;&amp;lt; EOF &amp;gt; ~/packer-templates/packer-remote-info.json
&amp;gt; {
&amp;gt;   &amp;quot;packer_remote_host&amp;quot;: &amp;quot;192.168.1.51&amp;quot;,
&amp;gt;   &amp;quot;packer_remote_username&amp;quot;: &amp;quot;root&amp;quot;,
&amp;gt;   &amp;quot;packer_remote_password&amp;quot;: &amp;quot;password&amp;quot;,
&amp;gt;   &amp;quot;packer_remote_datastore&amp;quot;: &amp;quot;datastore1&amp;quot;,
&amp;gt;   &amp;quot;packer_remote_network&amp;quot;: &amp;quot;VM Network&amp;quot;
&amp;gt; }
&amp;gt; EOF
[root@packer-centos packer-templates]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;10. Use yum to install the tree command so we can visualize the packer-templates directory structure&lt;/h2&gt;

&lt;p&gt;The tree command is helpful by allowing you to display the files and folders from a specific point in the file system. We will install this now to assist in showing the directory structure of the packer-templates directory.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos packer-templates]# yum install -y tree
Loaded plugins: fastestmirror
Setting up Install Process
Loading mirror speeds from cached hostfile
 * base: yum.tamu.edu
 * epel: mirror.utexas.edu
 * extras: centos.firehosted.com
 * updates: mirrors.adams.net
Resolving Dependencies
--&amp;gt; Running transaction check
---&amp;gt; Package tree.x86_64 0:1.5.3-3.el6 will be installed
--&amp;gt; Finished Dependency Resolution

Dependencies Resolved

==========================================================
 Package      Arch      Version         Repository   Size
==========================================================
Installing:
 tree         x86_64    1.5.3-3.el6     base         36 k

Transaction Summary
==========================================================
Install       1 Package(s)

Total download size: 36 k
Installed size: 65 k
Downloading Packages:
tree-1.5.3-3.el6.x86_64.rpm              |  36 kB    00:00
Running rpm_check_debug
Running Transaction Test
Transaction Test Succeeded
Running Transaction
  Installing : tree-1.5.3-3.el6.x86_64                 1/1
  Verifying  : tree-1.5.3-3.el6.x86_64                 1/1

Installed:
  tree.x86_64 0:1.5.3-3.el6

Complete!
[root@packer-centos packer-templates]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Run the tree command to see the directory structure:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos packer-templates]# tree
.
├── iso
│   ├── CentOS-6.7-x86_64-minimal.iso
│   ├── linux.iso
│   └── vmware-tools-linux.iso -&amp;gt; ./linux.iso
├── packer-remote-info.json
├── scripts
│   ├── centos-6-kickstart.cfg
│   ├── centos-vagrant-settings.sh
│   ├── centos-vmware-cleanup.sh
│   └── centos-vmware-tools_install.sh
└── templates
    └── centos67.json

3 directories, 9 files
[root@packer-centos packer-templates]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;11. Create .gitignore file that contains everything we don&amp;#39;t have to stored in git&lt;/h2&gt;

&lt;p&gt;Any files or folders listed in the .gitignore will be ignored by git. We will be adding the packer-remote-info.json file, since it contains username and password information, and the iso/ directory, since we don&amp;#39;t want to upload the large .iso files to our git repository.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos packer-templates]# cat .gitignore
iso/
packer-remote-info.json
[root@packer-centos packer-templates]# git status
# On branch master
#
# Initial commit
#
# Untracked files:
#   (use &amp;quot;git add &amp;lt;file&amp;gt;...&amp;quot; to include in what will be committed)
#
#   .gitignore
#   scripts/
#   templates/
nothing added to commit but untracked files present (use &amp;quot;git add&amp;quot; to track)
[root@packer-centos packer-templates]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;12. Push the changes we have made to a github repository&lt;/h2&gt;

&lt;p&gt;I created a public github repository for storing all the files created during this Packer series. During this step I will be pushing the current state of the files to the &lt;a href=&quot;https://github.com/sdorsett/packer-templates/tree/first-packer-template&quot;&gt;&amp;quot;first-packer-template&amp;quot; branch&lt;/a&gt; of this github repository:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos packer-templates]# git remote add origin https://sdorsett@github.com/sdorsett/packer-templates.git
[root@packer-centos packer-templates]# git add .
[root@packer-centos packer-templates]# git commit -m &amp;quot;first-packer-template&amp;quot;
[master (root-commit) 3fba017] first-packer-template
 6 files changed, 277 insertions(+), 0 deletions(-)
 create mode 100644 .gitignore
 create mode 100644 scripts/centos-6-kickstart.cfg
 create mode 100644 scripts/centos-vagrant-settings.sh
 create mode 100644 scripts/centos-vmware-cleanup.sh
 create mode 100644 scripts/centos-vmware-tools_install.sh
 create mode 100644 templates/centos67.json
[root@packer-centos packer-templates]# git checkout -b first-packer-template
Switched to a new branch &amp;#39;first-packer-template&amp;#39;
[root@packer-centos packer-templates]# git push origin first-packer-template
Password:
Counting objects: 10, done.
Delta compression using up to 2 threads.
Compressing objects: 100% (8/8), done.
Writing objects: 100% (10/10), 3.90 KiB, done.
Total 10 (delta 0), reused 0 (delta 0)
To https://sdorsett@github.com/sdorsett/packer-templates.git
 * [new branch]      first-packer-template -&amp;gt; first-packer-template
[root@packer-centos packer-templates]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;13. packer build the template we created&lt;/h2&gt;

&lt;p&gt;Now that we have all the necessary .iso files downloaded and template/script files created, we can test our build. In order to use the values in the packer-remote-info.json file, we will use the -var-file parameter to specify this file.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos packer-templates]# packer build -var-file=./packer-remote-info.json templates/centos67.json
centos67 output will be in this color.

==&amp;gt; centos67: Downloading or copying ISO
    centos67: Downloading or copying: file:///root/packer-templates/iso/CentOS-6.7-x86_64-minimal.iso
==&amp;gt; centos67: Uploading ISO to remote machine...
==&amp;gt; centos67: Creating virtual machine disk
==&amp;gt; centos67: Building and writing VMX file
==&amp;gt; centos67: Starting HTTP server on port 8448
==&amp;gt; centos67: Registering remote VM...
==&amp;gt; centos67: Starting virtual machine...
==&amp;gt; centos67: Waiting 5s for boot...
==&amp;gt; centos67: Connecting to VM via VNC
==&amp;gt; centos67: Typing the boot command over VNC...
==&amp;gt; centos67: Waiting for SSH to become available...&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;At this point you should see a centos67 virtual machine in the embedded host client of the ESXi virtual machine.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/01-centos67-virtual-machine.png&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;p&gt;If you open the console of the centos67 virtual machine, you can watch the automated install as it progresses.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/02-centos67-automated-install.png&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/03-centos67-automated-install.png&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/04-centos67-automated-install.png&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/05-centos67-install-completed.png&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;p&gt;Once the CentOS 6.7 install completes and the virtual machine reboots, you will see the packer build output continue with the provisioners block of the Packer template.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;==&amp;gt; centos67: Connected to SSH!&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The first provisioner is the file provisioner that will copy iso/vmware-tools-linux.iso to /tmp/vmware-tools-linux.iso within the CentOS 6.7 virtual machine.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;==&amp;gt; centos67: Uploading iso/vmware-tools-linux.iso =&amp;gt; /tmp/vmware-tools-linux.iso&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The second provisioner is a shell provisioner that will run the scripts/centos-vagrant-settings.sh bash script. This script will added the necessary changes for this Packer image to be used by Vagrant.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;==&amp;gt; centos67: Provisioning with shell script: scripts/centos-vagrant-settings.sh&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The third provisioner is a shell provisioner that will run the scripts/centos-vmware-tools_install.sh bash script. This script will install all the needed dependencies for vmtools and then install vmtools using an answer file.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;==&amp;gt; centos67: Provisioning with shell script: scripts/centos-vmware-tools_install.sh
    centos67: Loaded plugins: fastestmirror
    centos67: Setting up Upgrade Process
    centos67: Determining fastest mirrors
    centos67: * base: dallas.tx.mirror.xygenhosting.com
    centos67: * extras: mirror.us.oneandone.net
    centos67: * updates: dallas.tx.mirror.xygenhosting.com
    centos67: No Packages marked for Update
    centos67: Loaded plugins: fastestmirror
    centos67: Setting up Install Process
    centos67: Loading mirror speeds from cached hostfile
    centos67: * base: dallas.tx.mirror.xygenhosting.com
    centos67: * extras: mirror.us.oneandone.net
    centos67: * updates: dallas.tx.mirror.xygenhosting.com
    centos67: Resolving Dependencies
    centos67: --&amp;gt; Running transaction check
    centos67: ---&amp;gt; Package fuse-libs.x86_64 0:2.8.3-4.el6 will be installed
    centos67: ---&amp;gt; Package perl.x86_64 4:5.10.1-141.el6_7.1 will be installed
    centos67: --&amp;gt; Processing Dependency: perl-libs = 4:5.10.1-141.el6_7.1 for package: 4:perl-5.10.1-141.el6_7.1.x86_64
    centos67: --&amp;gt; Processing Dependency: perl-libs for package: 4:perl-5.10.1-141.el6_7.1.x86_64
    centos67: --&amp;gt; Processing Dependency: perl(version) for package: 4:perl-5.10.1-141.el6_7.1.x86_64
    centos67: --&amp;gt; Processing Dependency: perl(Pod::Simple) for package: 4:perl-5.10.1-141.el6_7.1.x86_64
    centos67: --&amp;gt; Processing Dependency: perl(Module::Pluggable) for package: 4:perl-5.10.1-141.el6_7.1.x86_64
    centos67: --&amp;gt; Processing Dependency: libperl.so()(64bit) for package: 4:perl-5.10.1-141.el6_7.1.x86_64
    centos67: --&amp;gt; Running transaction check
    centos67: ---&amp;gt; Package perl-Module-Pluggable.x86_64 1:3.90-141.el6_7.1 will be installed
    centos67: ---&amp;gt; Package perl-Pod-Simple.x86_64 1:3.13-141.el6_7.1 will be installed
    centos67: --&amp;gt; Processing Dependency: perl(Pod::Escapes) &amp;gt;= 1.04 for package: 1:perl-Pod-Simple-3.13-141.el6_7.1.x86_64
    centos67: ---&amp;gt; Package perl-libs.x86_64 4:5.10.1-141.el6_7.1 will be installed
    centos67: ---&amp;gt; Package perl-version.x86_64 3:0.77-141.el6_7.1 will be installed
    centos67: --&amp;gt; Running transaction check
    centos67: ---&amp;gt; Package perl-Pod-Escapes.x86_64 1:1.04-141.el6_7.1 will be installed
    centos67: --&amp;gt; Finished Dependency Resolution
    centos67:
    centos67: Dependencies Resolved
    centos67:
    centos67: ========================================
    centos67: Package         Arch   Version
    centos67: Repository
    centos67: Size
    centos67: ========================================
    centos67: Installing:
    centos67: fuse-libs       x86_64 2.8.3-4.el6
    centos67: base     74 k
    centos67: perl            x86_64 4:5.10.1-141.el6_7.1
    centos67: updates  10 M
    centos67: Installing for dependencies:
    centos67: perl-Module-Pluggable
    centos67: x86_64 1:3.90-141.el6_7.1
    centos67: updates  40 k
    centos67: perl-Pod-Escapes
    centos67: x86_64 1:1.04-141.el6_7.1
    centos67: updates  33 k
    centos67: perl-Pod-Simple x86_64 1:3.13-141.el6_7.1
    centos67: updates 213 k
    centos67: perl-libs       x86_64 4:5.10.1-141.el6_7.1
    centos67: updates 579 k
    centos67: perl-version    x86_64 3:0.77-141.el6_7.1
    centos67: updates  52 k
    centos67:
    centos67: Transaction Summary
    centos67: ========================================
    centos67: Install       7 Package(s)
    centos67:
    centos67: Total size: 11 M
    centos67: Total download size: 74 k
    centos67: Installed size: 36 M
    centos67: Downloading Packages:
    centos67: fuse-libs-2.8.3- |  74 kB     00:00
    centos67: warning: rpmts_HdrFromFdno: Header V3 RSA/SHA1 Signature, key ID c105b9de: NOKEY
    centos67: Retrieving key from file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6
    centos67: Importing GPG key 0xC105B9DE:
    centos67: Userid : CentOS-6 Key (CentOS 6 Official Signing Key) &amp;lt;centos-6-key@centos.org&amp;gt;
    centos67: Package: centos-release-6-7.el6.centos.12.3.x86_64 (@anaconda-CentOS-201508042137.x86_64/6.7)
    centos67: From   : /etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6
    centos67: Running rpm_check_debug
    centos67: Running Transaction Test
    centos67: Transaction Test Succeeded
    centos67: Running Transaction
    centos67:   Installing : 1:perl-Pod-Escapes   1/7
    centos67:   Installing : 1:perl-Module-Plug   2/7
    centos67:   Installing : 3:perl-version-0.7   3/7
    centos67:   Installing : 4:perl-libs-5.10.1   4/7
    centos67:   Installing : 1:perl-Pod-Simple-   5/7
    centos67:   Installing : 4:perl-5.10.1-141.   6/7
    centos67:   Installing : fuse-libs-2.8.3-4.   7/7
    centos67: Verifying  : 1:perl-Pod-Simple-   1/7
    centos67: Verifying  : 1:perl-Pod-Escapes   2/7
    centos67: Verifying  : fuse-libs-2.8.3-4.   3/7
    centos67: Verifying  : 1:perl-Module-Plug   4/7
    centos67: Verifying  : 3:perl-version-0.7   5/7
    centos67: Verifying  : 4:perl-libs-5.10.1   6/7
    centos67: Verifying  : 4:perl-5.10.1-141.   7/7
    centos67:
    centos67: Installed:
    centos67: fuse-libs.x86_64 0:2.8.3-4.el6
    centos67: perl.x86_64 4:5.10.1-141.el6_7.1
    centos67:
    centos67: Dependency Installed:
    centos67: perl-Module-Pluggable.x86_64 1:3.90-141.el6_7.1
    centos67: perl-Pod-Escapes.x86_64 1:1.04-141.el6_7.1
    centos67: perl-Pod-Simple.x86_64 1:3.13-141.el6_7.1
    centos67: perl-libs.x86_64 4:5.10.1-141.el6_7.1
    centos67: perl-version.x86_64 3:0.77-141.el6_7.1
    centos67:
    centos67: Complete!
    centos67: total 488
    centos67: drwxr-xr-x   7 root root   4096 Oct 30 07:10 .
    centos67: drwxrwxrwt.  4 root root   4096 Dec 24 02:34 ..
    centos67: drwxr-xr-x   2 root root   4096 Oct 30 07:10 bin
    centos67: drwxr-xr-x   2 root root   4096 Oct 30 07:10 doc
    centos67: drwxr-xr-x   5 root root   4096 Oct 30 07:10 etc
    centos67: -rw-r--r--   1 root root 269196 Oct 30 07:10 FILES
    centos67: -rw-r--r--   1 root root   2538 Oct 30 07:10 INSTALL
    centos67: drwxr-xr-x   2 root root   4096 Oct 30 07:10 installer
    centos67: drwxr-xr-x  15 root root   4096 Oct 30 07:10 lib
    centos67: -rwxr-xr-x   1 root root 196237 Oct 30 07:10 vmware-install.pl
    centos67: Creating a new VMware Tools installer database using the tar4 format.
    centos67:
    centos67: Installing VMware Tools.
    centos67:
    centos67: In which directory do you want to install the binary files?
    centos67: [/usr/bin]
    centos67: The path &amp;quot;yes&amp;quot; is a relative path. Please enter an absolute path.
    centos67:
    centos67: In which directory do you want to install the binary files?
    centos67: [/usr/bin]
    centos67: What is the directory that contains the init directories (rc0.d/ to rc6.d/)?
    centos67: [/etc/rc.d]
    centos67: What is the directory that contains the init scripts?
    centos67: [/etc/init.d]
    centos67: In which directory do you want to install the daemon files?
    centos67: [/usr/sbin]
    centos67: In which directory do you want to install the library files?
    centos67: [/usr/lib/vmware-tools]
    centos67: The path &amp;quot;/usr/lib/vmware-tools&amp;quot; does not exist currently. This program is
    centos67: going to create it, including needed parent directories. Is this what you want?
    centos67: [yes]
    centos67: In which directory do you want to install the documentation files?
    centos67: [/usr/share/doc/vmware-tools]
    centos67: The path &amp;quot;/usr/share/doc/vmware-tools&amp;quot; does not exist currently. This program
    centos67: is going to create it, including needed parent directories. Is this what you
    centos67: want? [yes]
    centos67: The installation of VMware Tools 9.9.4 build-3193940 for Linux completed
    centos67: successfully. You can decide to remove this software from your system at any
    centos67: time by invoking the following command: &amp;quot;/usr/bin/vmware-uninstall-tools.pl&amp;quot;.
    centos67:
    centos67: Before running VMware Tools for the first time, you need to configure it by
    centos67: invoking the following command: &amp;quot;/usr/bin/vmware-config-tools.pl&amp;quot;. Do you want
    centos67: this program to invoke the command for you now? [yes]
    centos67: Initializing...
    centos67:
    centos67:
    centos67: Making sure services for VMware Tools are stopped.
    centos67:
    centos67:
    centos67:
    centos67: Found a compatible pre-built module for vmci.  Installing it...
    centos67:
    centos67:
    centos67: Found a compatible pre-built module for vsock.  Installing it...
    centos67:
    centos67:
    centos67: The module vmxnet3 has already been installed on this system by another
    centos67: installer or package and will not be modified by this installer.
    centos67:
    centos67: The module pvscsi has already been installed on this system by another
    centos67: installer or package and will not be modified by this installer.
    centos67:
    centos67: The module vmmemctl has already been installed on this system by another
    centos67: installer or package and will not be modified by this installer.
    centos67:
    centos67: The VMware Host-Guest Filesystem allows for shared folders between the host OS
    centos67: and the guest OS in a Fusion or Workstation virtual environment.  Do you wish
    centos67: to enable this feature? [no]
    centos67: Found a compatible pre-built module for vmhgfs.  Installing it...
    centos67:
    centos67:
    centos67: Found a compatible pre-built module for vmxnet.  Installing it...
    centos67:
    centos67:
    centos67: The vmblock enables dragging or copying files between host and guest in a
    centos67: Fusion or Workstation virtual environment.  Do you wish to enable this feature?
    centos67: [no]
    centos67: VMware automatic kernel modules enables automatic building and installation of
    centos67: VMware kernel modules at boot that are not already present. This feature can be
    centos67:
    centos67: enabled/disabled by re-running vmware-config-tools.pl.
    centos67:
    centos67: Would you like to enable VMware automatic kernel modules?
    centos67: [no]
    centos67: Do you want to enable Guest Authentication (vgauth)? [yes]
    centos67: No X install found.
    centos67:
    centos67: Creating a new initrd boot image for the kernel.
    centos67: vmware-tools-thinprint start/running
    centos67: vmware-tools start/running
    centos67: The configuration of VMware Tools 9.9.4 build-3193940 for Linux for this
    centos67: running kernel completed successfully.
    centos67:
    centos67: You must restart your X session before any mouse or graphics changes take
    centos67: effect.
    centos67:
    centos67: You can now run VMware Tools by invoking &amp;quot;/usr/bin/vmware-toolbox-cmd&amp;quot; from the
    centos67: command line.
    centos67:
    centos67: To enable advanced X features (e.g., guest resolution fit, drag and drop, and
    centos67: file and text copy/paste), you will need to do one (or more) of the following:
    centos67: 1. Manually start /usr/bin/vmware-user
    centos67: 2. Log out and log back into your desktop session; and,
    centos67: 3. Restart your X session.
    centos67:
    centos67: Enjoy,
    centos67:
    centos67: --the VMware team
    centos67:&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The final provisioner is a shell provisioner that will run the scripts/centos-vmware-cleanup.sh bash script. This script will clean up the centos67 virtual machine and zero out all unused disk space to reduce the size of the image.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;==&amp;gt; centos67: Provisioning with shell script: scripts/centos-vmware-cleanup.sh
    centos67: ==&amp;gt; Pausing for 0 seconds...
    centos67: ==&amp;gt; erasing unused packages to free up space
    centos67: Loaded plugins: fastestmirror
    centos67: Setting up Remove Process
    centos67: No Match for argument: gtk2
    centos67: Loading mirror speeds from cached hostfile
    centos67: * base: dallas.tx.mirror.xygenhosting.com
    centos67: * extras: mirror.us.oneandone.net
    centos67: * updates: dallas.tx.mirror.xygenhosting.com
    centos67: Package(s) gtk2 available, but not installed.
    centos67: No Match for argument: libX11
    centos67: Package(s) libX11 available, but not installed.
    centos67: No Match for argument: hicolor-icon-theme
    centos67: Package(s) hicolor-icon-theme available, but not installed.
    centos67: No Match for argument: avahi
    centos67: Package(s) avahi available, but not installed.
    centos67: No Match for argument: freetype
    centos67: Package(s) freetype available, but not installed.
    centos67: No Match for argument: bitstream-vera-fonts
    centos67: No Packages marked for removal
    centos67: ==&amp;gt; Cleaning up yum cache
    centos67: Loaded plugins: fastestmirror
    centos67: Cleaning repos: base extras updates
    centos67: Cleaning up Everything
    centos67: Cleaning up list of fastest mirrors
    centos67: ==&amp;gt; Force logs to rotate
    centos67: ==&amp;gt; Clear audit log and wtmp
    centos67: ==&amp;gt; Cleaning up udev rules
    centos67: ==&amp;gt; Remove the traces of the template MAC address and UUIDs
    centos67: ==&amp;gt; Cleaning up tmp
    centos67: ==&amp;gt; Remove the SSH host keys
    centos67: ==&amp;gt; Remove the root user’s shell history
    centos67: ==&amp;gt; yum -y clean all
    centos67: Loaded plugins: fastestmirror
    centos67: Cleaning repos: base extras updates
    centos67: Cleaning up Everything
    centos67: /
    centos67: 25211696
    centos67: /tmp/script_7943.sh: line 62: bc: command not found
    centos67:
    centos67: dd: invalid number `&amp;#39;
    centos67: /boot
    centos67: 60474
    centos67: /tmp/script_7943.sh: line 62: bc: command not found
    centos67:
    centos67: dd: invalid number `&amp;#39;
    centos67: /tmp/script_7943.sh: line 70: /usr/sbin/vgdisplay: No such file or directory
    centos67: ==&amp;gt; Zero out the free space to save space in the final image
    centos67: dd: writing `/EMPTY&amp;#39;: No space left on device
    centos67: 12970+0 records in
    centos67: 12969+0 records out
    centos67: 13599232000 bytes (14 GB) copied, 413.4 s, 32.9 MB/s&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;With the provisioners block having been completed, Packer will now shutdown the centos67 virtual machine and unregister it from the ESXi virtual machine.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;==&amp;gt; centos67: Gracefully halting virtual machine...
    centos67: Waiting for VMware to clean up after itself...
==&amp;gt; centos67: Deleting unnecessary VMware files...
    centos67: Deleting: /vmfs/volumes/datastore1/output-centos67/vmware.log
==&amp;gt; centos67: Cleaning VMX prior to finishing up...
    centos67: Unmounting floppy from VMX...
    centos67: Detaching ISO from CD-ROM device...
    centos67: Disabling VNC server...
==&amp;gt; centos67: Compacting the disk image
==&amp;gt; centos67: Unregistering virtual machine...
Build &amp;#39;centos67&amp;#39; finished.

==&amp;gt; Builds finished. The artifacts of successful builds are:
--&amp;gt; centos67: VM files in directory: /vmfs/volumes/datastore1/output-centos67
[root@packer-centos packer-templates]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The output of the packer build command shows that the template was successfully created at /vmfs/volumes/datastore1/output-centos67 on the ESXi virtual machine.&lt;/p&gt;

&lt;h3&gt;This seems like a good point to take break. We covered a lot of ground in this post, but as a result of all our work we have a working Packer template to build on.&lt;/h3&gt;

&lt;h3&gt;If you would like to not have to manually create the files covered in this post, you can clone down &lt;a href=&quot;https://github.com/sdorsett/packer-templates/tree/first-packer-template&quot;&gt;this github repository&lt;/a&gt; by running the following command:&lt;/h3&gt;

&lt;pre&gt;
git clone -b &quot;first-packer-template&quot; https://github.com/sdorsett/packer-templates.git
&lt;/pre&gt;  

&lt;h3&gt;You will still need to create the iso/ directory, download the needed .iso files and create the packer-remote-info.json file as covered in this post.&lt;/h3&gt;

&lt;h3&gt;In the next post we will extend what was covered in this post by installing the Puppet agent in Packer image.&lt;/h3&gt;

&lt;h3&gt;Please provide any feedback or suggestions to my twitter account located on the about page.&lt;/h3&gt;
</content>
 </entry>
 
 <entry>
   <title>Setting up Packer, ovftool and Apache web server on a CentOS virtual machine</title>
   <link href="http://0.0.0.0:4000/2015/12/24/installing-packer-and-ovftool-on-centos/"/>
   <updated>2015-12-24T00:00:00+00:00</updated>
   <id>http://0.0.0.0:4000/2015/12/24/installing-packer-and-ovftool-on-centos</id>
   <content type="html">&lt;p&gt;This is the third in a series of posts on &lt;a href=&quot;https://sdorsett.github.io/2015/12/22/pipeline-for-creating-packer-box-files/&quot;&gt;using a Packer pipeline to generate Vagrant .box files&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the &lt;a href=&quot;https://sdorsett.github.io/2015/12/23/installing-esxi-virtual-machine-for-packer-depolyment/&quot;&gt;last post&lt;/a&gt; we setup a ESXi virtual machine that would be the target for creating Packer images. In order to follow along with this post you will need two things:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A fresh CentOS virtual machine on which we will install Packer - I&amp;#39;m using CentOS 6.6 minimal install named &amp;quot;packer-centos&amp;quot; with 2 vCPU, 4GB of memory and a 100GB virtual hard drive. I also gave this virtual machine the IP address of 192.168.1.52&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://my.vmware.com/web/vmware/details?productId=491&amp;downloadGroup=OVFTOOL410&quot;&gt;VMware ovftool&lt;/a&gt; - if you followed the previous post and are using ESXi 6.0 you will need a copy of ovftool 4.1.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&amp;#39;s get started...&lt;/p&gt;

&lt;h2&gt;1. SSH to CentOS virtual machine you created.&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;sdorsett-mbp:~ sdorsett$ ssh root@192.168.1.52
The authenticity of host &amp;#39;192.168.1.52 (192.168.1.52)&amp;#39; cant be established.
RSA key fingerprint is 58:f5:22:2e:f6:64:04:59:6b:0b:76:2f:33:6f:03:85.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added &amp;#39;192.168.1.52&amp;#39; (RSA) to the list of known hosts.
root@192.168.1.52&amp;#39;s password:
Last login: Thu Oct 22 17:52:17 2015 from 192.168.1.1
[root@packer-centos ~]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;2. Install the EPEL repository&lt;/h2&gt;

&lt;p&gt;There will be several package we need to install that are coming from the CentOS EPEL repository, so we will need to install it.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos ~]# yum install -y epel-release
Loaded plugins: fastestmirror
Setting up Install Process
Determining fastest mirrors
 * base: yum.tamu.edu
 * extras: centos.firehosted.com
 * updates: mirrors.adams.net
base
base/primary_db
extras
extras/primary_db
updates
updates/primary_db
Resolving Dependencies
--&amp;gt; Running transaction check
---&amp;gt; Package epel-release.noarch 0:6-8 will be installed
--&amp;gt; Finished Dependency Resolution

Dependencies Resolved

=============================================================
 Package          Arch       Version     Repository     Size
=============================================================
Installing:
 epel-release     noarch     6-8         extras         14 k

Transaction Summary
=============================================================
Install       1 Package(s)

Total download size: 14 k
Installed size: 22 k
Downloading Packages:
epel-release-6-8.noarch.rpm     |  14 kB     00:00
Running rpm_check_debug
Running Transaction Test
Transaction Test Succeeded
Running Transaction
  Installing : epel-release-6-8.noarch     1/1
  Verifying  : epel-release-6-8.noarch     1/1

Installed:
  epel-release.noarch 0:6-8

Complete!

[root@packer-centos ~]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;3. Use yum to make sure the following packages are installed:&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;git - for version control and the ability to pull down git repositories&lt;/li&gt;
&lt;li&gt;wget - provides the ability to download packages from URLs&lt;/li&gt;
&lt;li&gt;unzip - needed to unzip Packer, since it comes as a .zip&lt;/li&gt;
&lt;li&gt;sshpass - used for running SSH commands, but it allows us to specify a password to use&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos ~]# yum install -y git wget unzip sshpass
Loaded plugins: fastestmirror
Setting up Install Process
Loading mirror speeds from cached hostfile
 * base: yum.tamu.edu
 * epel: fedora-epel.mirror.lstn.net
 * extras: centos.firehosted.com
 * updates: mirrors.adams.net
Package git-1.7.1-3.el6_4.1.x86_64 already installed and latest version
Package wget-1.12-5.el6_6.1.x86_64 already installed and latest version
Resolving Dependencies
--&amp;gt; Running transaction check
---&amp;gt; Package sshpass.x86_64 0:1.05-1.el6 will be installed
---&amp;gt; Package unzip.x86_64 0:6.0-2.el6_6 will be installed
--&amp;gt; Finished Dependency Resolution

Dependencies Resolved

============================================================
 Package     Arch       Version        Repository     Size
============================================================
Installing:
 sshpass     x86_64     1.05-1.el6     epel           19 k
 unzip       x86_64     6.0-2.el6_6    base           149 k

Transaction Summary
============================================================
Install       2 Package(s)

Total download size: 168 k
Installed size: 346 k
Downloading Packages:
(1/2): sshpass-1.05-1.el6.x86_64.rpm     |  19 kB     00:00
(2/2): unzip-6.0-2.el6_6.x86_64.rpm      | 149 kB     00:00
------------------------------------------------------------
Total                                                                                                                                                                     593 kB/s | 168 kB     00:00
warning: rpmts_HdrFromFdno: Header V3 RSA/SHA256 Signature, key ID 0608b895: NOKEY
Retrieving key from file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6
Importing GPG key 0x0608B895:
 Userid : EPEL (6) &amp;lt;epel@fedoraproject.org&amp;gt;
 Package: epel-release-6-8.noarch (@extras)
 From   : /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-6
Running rpm_check_debug
Running Transaction Test
Transaction Test Succeeded
Running Transaction
  Installing : sshpass-1.05-1.el6.x86_64     1/2
  Installing : unzip-6.0-2.el6_6.x86_64      2/2
  Verifying  : unzip-6.0-2.el6_6.x86_64      1/2
  Verifying  : sshpass-1.05-1.el6.x86_64     2/2

Installed:
  sshpass.x86_64 0:1.05-1.el6     unzip.x86_64 0:6.0-2.el6_6

Complete!
[root@packer-centos ~]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;4. Install Packer&lt;/h2&gt;

&lt;p&gt;Go to &lt;a href=&quot;https://packer.io/downloads.html&quot;&gt;https://packer.io/downloads.html&lt;/a&gt; and copy the URL for the Linux 64-bit package. In the SSH session to our CentOS virtual machine, perform the following steps:&lt;/p&gt;

&lt;p&gt;Create a new directory to install packer into&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos ~]# mkdir /usr/local/packer_0.8.6
[root@packer-centos ~]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Download Packer using the URL you copied from the packer.io download page:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos ~]# wget https://releases.hashicorp.com/packer/0.8.6/packer_0.8.6_linux_amd64.zip
--2015-12-23 17:22:03--  https://releases.hashicorp.com/packer/0.8.6/packer_0.8.6_linux_amd64.zip
Resolving releases.hashicorp.com... 23.235.44.69
Connecting to releases.hashicorp.com|23.235.44.69|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 132616691 (126M) [application/zip]
Saving to: “packer_0.8.6_linux_amd64.zip”

100%[====================================&amp;gt;] 132,616,691 6.97M/s   in 18s

2015-12-23 17:22:22 (6.95 MB/s) - “packer_0.8.6_linux_amd64.zip” saved [132616691/132616691]

[root@packer-centos ~]# unzip packer_0.8.6_linux_amd64.zip -d /usr/local/packer_8.6/
Archive:  packer_0.8.6_linux_amd64.zip
  inflating: /usr/local/packer_8.6/packer
  inflating: /usr/local/packer_8.6/packer-builder-amazon-chroot
  inflating: /usr/local/packer_8.6/packer-builder-amazon-ebs
  inflating: /usr/local/packer_8.6/packer-builder-amazon-instance
  inflating: /usr/local/packer_8.6/packer-builder-digitalocean
  inflating: /usr/local/packer_8.6/packer-builder-docker
  inflating: /usr/local/packer_8.6/packer-builder-file
  inflating: /usr/local/packer_8.6/packer-builder-googlecompute
  inflating: /usr/local/packer_8.6/packer-builder-null
  inflating: /usr/local/packer_8.6/packer-builder-openstack
  inflating: /usr/local/packer_8.6/packer-builder-parallels-iso
  inflating: /usr/local/packer_8.6/packer-builder-parallels-pvm
  inflating: /usr/local/packer_8.6/packer-builder-qemu
  inflating: /usr/local/packer_8.6/packer-builder-virtualbox-iso
  inflating: /usr/local/packer_8.6/packer-builder-virtualbox-ovf
  inflating: /usr/local/packer_8.6/packer-builder-vmware-iso
  inflating: /usr/local/packer_8.6/packer-builder-vmware-vmx
  inflating: /usr/local/packer_8.6/packer-post-processor-artifice
  inflating: /usr/local/packer_8.6/packer-post-processor-atlas
  inflating: /usr/local/packer_8.6/packer-post-processor-compress
  inflating: /usr/local/packer_8.6/packer-post-processor-docker-import
  inflating: /usr/local/packer_8.6/packer-post-processor-docker-push
  inflating: /usr/local/packer_8.6/packer-post-processor-docker-save
  inflating: /usr/local/packer_8.6/packer-post-processor-docker-tag
  inflating: /usr/local/packer_8.6/packer-post-processor-vagrant
  inflating: /usr/local/packer_8.6/packer-post-processor-vagrant-cloud
  inflating: /usr/local/packer_8.6/packer-post-processor-vsphere
  inflating: /usr/local/packer_8.6/packer-provisioner-ansible-local
  inflating: /usr/local/packer_8.6/packer-provisioner-chef-client
  inflating: /usr/local/packer_8.6/packer-provisioner-chef-solo
  inflating: /usr/local/packer_8.6/packer-provisioner-file
  inflating: /usr/local/packer_8.6/packer-provisioner-powershell
  inflating: /usr/local/packer_8.6/packer-provisioner-puppet-masterless
  inflating: /usr/local/packer_8.6/packer-provisioner-puppet-server
  inflating: /usr/local/packer_8.6/packer-provisioner-salt-masterless
  inflating: /usr/local/packer_8.6/packer-provisioner-shell
  inflating: /usr/local/packer_8.6/packer-provisioner-shell-local
  inflating: /usr/local/packer_8.6/packer-provisioner-windows-restart
  inflating: /usr/local/packer_8.6/packer-provisioner-windows-shell
[root@packer-centos ~]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Update ~/.bashrc to add &amp;quot;/usr/local/packer_8.6&amp;quot; to our path. Run &amp;quot;source ~/.bashrc&amp;quot; to re-read ~/.bashrc and validate $PATH has been updated.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos ~]# echo &amp;#39;export PATH=&amp;quot;/usr/local/packer_8.6:$PATH&amp;quot;&amp;#39; &amp;gt;&amp;gt; ~/.bashrc
[root@packer-centos ~]# source ~/.bashrc
[root@packer-centos ~]# echo $PATH
/usr/local/packer_8.6:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin
[root@packer-centos ~]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Ensure the packer binary can be found and is showing the proper version&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos ~]# which packer
/usr/local/packer_8.6/packer
[root@packer-centos ~]# packer -v
0.8.6
[root@packer-centos ~]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;5. Stop the iptables firewall and disable it from started on reboot of the virtual machine&lt;/h2&gt;

&lt;p&gt;Packer will start a web server service for providing kickstart and script files to Packer images during install. This service will use a random port within a range, so we will need to stop iptables in order to allow the Packer images to connect.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos ~]# service iptables stop
[root@packer-centos ~]# chkconfig iptables off
[root@packer-centos ~]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;6. Install ovftool on the CentOS virtual machine&lt;/h2&gt;

&lt;p&gt;Download linux 64bit version of ovftool 4.1 on your local machine&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/01-download-ovftool-linux-64bit.png&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;p&gt;SCP the downloaded file to the CentOS virtual machine&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;sdorsett-mbp:~ sdorsett$ scp ~/Downloads/VMware-ovftool-4.1.0-2459827-lin.x86_64.bundle root@192.168.1.52:/root/
root@192.168.1.52&amp;#39;s password:
VMware-ovftool-4.1.0-2459827-lin.x86_64.bundle                    100%   37MB  12.4MB/s   00:03
sdorsett-mbp:~ sdorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Install ovftool on the CentOS virtual machine and validate the version&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos ~]# ls
anaconda-ks.cfg  install.log  install.log.syslog  packer_0.8.6_linux_amd64.zip  VMware-ovftool-4.1.0-2459827-lin.x86_64.bundle
[root@packer-centos ~]# ./VMware-ovftool-4.1.0-2459827-lin.x86_64.bundle
Extracting VMware Installer...done.
You must accept the VMware OVF Tool component for Linux End User
License Agreement to continue.  Press Enter to proceed.
VMWARE END USER LICENSE AGREEMENT

PLEASE NOTE THAT THE TERMS OF THIS END USER LICENSE AGREEMENT SHALL GOVERN YOUR
USE OF THE SOFTWARE, REGARDLESS OF ANY TERMS THAT MAY APPEAR DURING THE
INSTALLATION OF THE SOFTWARE.

IMPORTANT-READ CAREFULLY:   BY DOWNLOADING, INSTALLING, OR USING THE SOFTWARE,
YOU (THE INDIVIDUAL OR LEGAL ENTITY) AGREE TO BE BOUND BY THE TERMS OF THIS END
USER LICENSE AGREEMENT (&amp;quot;EULA&amp;quot;).  IF YOU DO NOT AGREE TO THE TERMS OF THIS
EULA, YOU MUST NOT DOWNLOAD, INSTALL, OR USE THE SOFTWARE, AND YOU MUST DELETE
OR RETURN THE UNUSED SOFTWARE TO THE VENDOR FROM WHICH YOU ACQUIRED IT WITHIN
THIRTY (30) DAYS AND REQUEST A REFUND OF THE LICENSE FEE, IF ANY, THAT YOU PAID
FOR THE SOFTWARE.
...

Do you agree? [yes/no]: yes

The product is ready to be installed.  Press Enter to begin
installation or Ctrl-C to cancel.

Installing VMware OVF Tool component for Linux 4.1.0
    Configuring...
[######################################################################] 100%
Installation was successful.
[root@packer-centos ~]# which ovftool
/usr/bin/ovftool
[root@packer-centos ~]# ovftool -v
VMware ovftool 4.1.0 (build-2459827)
[root@packer-centos ~]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;7. Install apache web server on the CentOS virtual machine&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos ~]# yum install -y httpd
Loaded plugins: fastestmirror
Setting up Install Process
Loading mirror speeds from cached hostfile
 * base: yum.tamu.edu
 * epel: fedora-epel.mirror.lstn.net
 * extras: centos.firehosted.com
 * updates: mirrors.adams.net
Resolving Dependencies
--&amp;gt; Running transaction check
---&amp;gt; Package httpd.x86_64 0:2.2.15-47.el6.centos.1 will be installed
--&amp;gt; Processing Dependency: httpd-tools = 2.2.15-47.el6.centos.1 for package: httpd-2.2.15-47.el6.centos.1.x86_64
--&amp;gt; Processing Dependency: apr-util-ldap for package: httpd-2.2.15-47.el6.centos.1.x86_64
--&amp;gt; Processing Dependency: /etc/mime.types for package: httpd-2.2.15-47.el6.centos.1.x86_64
--&amp;gt; Processing Dependency: libaprutil-1.so.0()(64bit) for package: httpd-2.2.15-47.el6.centos.1.x86_64
--&amp;gt; Processing Dependency: libapr-1.so.0()(64bit) for package: httpd-2.2.15-47.el6.centos.1.x86_64
--&amp;gt; Running transaction check
---&amp;gt; Package apr.x86_64 0:1.3.9-5.el6_2 will be installed
---&amp;gt; Package apr-util.x86_64 0:1.3.9-3.el6_0.1 will be installed
---&amp;gt; Package apr-util-ldap.x86_64 0:1.3.9-3.el6_0.1 will be installed
---&amp;gt; Package httpd-tools.x86_64 0:2.2.15-47.el6.centos.1 will be installed
---&amp;gt; Package mailcap.noarch 0:2.1.31-2.el6 will be installed
--&amp;gt; Finished Dependency Resolution

Dependencies Resolved

======================================================================
 Package        Arch       Version                  Repository   Size
======================================================================
Installing:
 httpd          x86_64     2.2.15-47.el6.centos.1   updates      830 k
Installing for dependencies:
 apr            x86_64     1.3.9-5.el6_2            base         123 k
 apr-util       x86_64     1.3.9-3.el6_0.1          base         87 k
 apr-util-ldap  x86_64     1.3.9-3.el6_0.1          base         15 k
 httpd-tools    x86_64     2.2.15-47.el6.centos.1   updates      77 k
 mailcap        noarch     2.1.31-2.el6             base         27 k

Transaction Summary
======================================================================
Install       6 Package(s)

Total download size: 1.1 M
Installed size: 3.6 M
Downloading Packages:
(1/6): apr-1.3.9-5.el6_2.x86_64.rpm                   | 123 kB  00:00
(2/6): apr-util-1.3.9-3.el6_0.1.x86_64.rpm            |  87 kB  00:00
(3/6): apr-util-ldap-1.3.9-3.el6_0.1.x86_64.rpm       |  15 kB  00:00
(4/6): httpd-2.2.15-47.el6.centos.1.x86_64.rpm        | 830 kB  00:00
(5/6): httpd-tools-2.2.15-47.el6.centos.1.x86_64.rpm  |  77 kB  00:00
(6/6): mailcap-2.1.31-2.el6.noarch.rpm                |  27 kB  00:00
----------------------------------------------------------------------
Total                                        711 kB/s | 1.1 MB  00:01
Running rpm_check_debug
Running Transaction Test
Transaction Test Succeeded
Running Transaction
  Installing : apr-1.3.9-5.el6_2.x86_64                         1/6
  Installing : apr-util-1.3.9-3.el6_0.1.x86_64                  2/6
  Installing : httpd-tools-2.2.15-47.el6.centos.1.x86_64        3/6
  Installing : apr-util-ldap-1.3.9-3.el6_0.1.x86_64             4/6
  Installing : mailcap-2.1.31-2.el6.noarch                      5/6
  Installing : httpd-2.2.15-47.el6.centos.1.x86_64              6/6
  Verifying  : httpd-tools-2.2.15-47.el6.centos.1.x86_64        1/6
  Verifying  : httpd-2.2.15-47.el6.centos.1.x86_64              2/6
  Verifying  : apr-util-ldap-1.3.9-3.el6_0.1.x86_64             3/6
  Verifying  : apr-1.3.9-5.el6_2.x86_64                         4/6
  Verifying  : mailcap-2.1.31-2.el6.noarch                      5/6
  Verifying  : apr-util-1.3.9-3.el6_0.1.x86_64                  6/6

Installed:
  httpd.x86_64 0:2.2.15-47.el6.centos.1

Dependency Installed:
  apr.x86_64 0:1.3.9-5.el6_2    apr-util.x86_64 0:1.3.9-3.el6_0.1    
  apr-util-ldap.x86_64 0:1.3.9-3.el6_0.1  httpd-tools.x86_64 0:2.2.15-47.el6.centos.1     
  mailcap.noarch 0:2.1.31-2.el6

Complete!
[root@packer-centos ~]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;7. Remove the apache welcome.conf, to enable directory browsing, and start the httpd service&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos ~]# rm /etc/httpd/conf.d/welcome.conf
rm: remove regular file &amp;#39;/etc/httpd/conf.d/welcome.conf&amp;#39;? y
[root@packer-centos ~]# service httpd start
Starting httpd:                                            [  OK  ]
[root@packer-centos ~]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;8. create the directory /var/www/html/box-files and test that the directory is visible in a browser&lt;/h2&gt;

&lt;p&gt;Create the /var/www/hmlt/box-files/ directory and an empty test-file.txt&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos ~]# mkdir /var/www/html/box-files
[root@packer-centos ~]# touch /var/www/html/box-files/test-file.txt
[root@packer-centos ~]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Open the ip address of the CentOS virtual machine in a browser and verify that the test-file.txt is visible&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/02-browse-apache-box-file-directory.png&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;p&gt;Delete /var/www/html/box-files/test-file.txt, since it will not be needed:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-centos ~]# rm /var/www/html/box-files/test-file.txt
rm: remove regular empty file &amp;#39;/var/www/html/box-files/test-file.txt&amp;#39;? y
[root@packer-centos ~]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3&gt;That all for this post covering how to install Packer, ovftool and Apache web server on a CentOS virtual machine. In the next post we will create a Packer template that uses what we have setup so far.&lt;/h3&gt;

&lt;h3&gt;Please provide any feedback or suggestions to my twitter account located on the about page.&lt;/h3&gt;
</content>
 </entry>
 
 <entry>
   <title>Installing a ESXi 6.0 virtual machine for use with Packer</title>
   <link href="http://0.0.0.0:4000/2015/12/23/installing-esxi-virtual-machine-for-packer-depolyment/"/>
   <updated>2015-12-23T00:00:00+00:00</updated>
   <id>http://0.0.0.0:4000/2015/12/23/installing-esxi-virtual-machine-for-packer-depolyment</id>
   <content type="html">&lt;p&gt;This is the second in a series of posts on &lt;a href=&quot;https://sdorsett.github.io/2015/12/22/pipeline-for-creating-packer-box-files/&quot;&gt;using a Packer pipeline to generate Vagrant .box files&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In order to begin using Packer to create images, we will first need to lay the &amp;quot;virtual&amp;quot; ground work. Packer can create virtual machine images on a wide variety of virtualization or cloud platforms, but since I work for VMware I have been using the ESXi hypervisor.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;This post will be covering installing ESXi as a virtual machine on a vSphere cluster. There is no reason that you couldn&amp;#39;t use Packer with a stand alone physical server that has ESXi installed as well.&lt;/li&gt;
&lt;li&gt;These steps were performed using the vCenter 6.0 web client. You could just as well use ESXi 5 with the C# client, but the steps for setting up nested virtualization will be slightly different.&lt;/li&gt;
&lt;li&gt;There is nothing new about the steps being covered, but I figured it would be best to go ahead and document them.&lt;/li&gt;
&lt;li&gt;If you are looking for the best resources regarding running ESXi in a virtual machine, I would suggest taking a look at &lt;a href=&quot;http://www.virtuallyghetto.com/&quot;&gt;William Lam&amp;#39;s blog&lt;/a&gt; which covers this subject in great detail. Williams site also provides more details about the embedded host client we will be using.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&amp;#39;s get started...&lt;/p&gt;

&lt;h2&gt;1. Log into the web client of your vCenter instance and create a new virtual machine.&lt;/h2&gt;

&lt;p&gt;We first need to create a new virtual machine inside of which we will install the ESXi 6.0 hypervisor.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/01-new-virtual-machine.jpg&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;h2&gt;2. Step through the new virtual machine wizard:&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/02-create-new-virtual-machine.jpg&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;p&gt;Name the virtual machine what you would like and click next. I named mine &amp;quot;packer-esxi&amp;quot;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/03-virtual-machine-name.jpg&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;p&gt;Select the vCenter cluster or resource pool the virtual machine will reside in and click next.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/04-select-compute-resource.jpg&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;p&gt;Select the datastore you want the ESXi virtual machine to be created on and click next. I&amp;#39;m using a local datastore on one of my physical ESXi hosts, to prevent Packer from using storage shared across the entire cluster.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/05-select-storage.jpg&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;p&gt;Select the compatibility (virtual hardware level) for the virtual machine and click next. I kept with the default of version 11.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/06-select-compatibility.jpg&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;p&gt;Select the guest operating system. vSphere 6.0 or newer will allow you to select ESXi 6.0 as you guest OS. Click next. This is where things will be different if you are using vSphere 5.0 and the c# client since you will only have ESXi 5.0 listed as an option.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/07-select-guest-os.jpg&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;p&gt;Customize to virtual hardware to have the necessary resources. I had created my ESXi vurtual machine with:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;2 vCPU&lt;/li&gt;
&lt;li&gt;16 GB of memory&lt;/li&gt;
&lt;li&gt;100GB virtual hard drive&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08-customize-hardware.jpg&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;p&gt;Make sure you expand CPU and enable &amp;quot;Hardware virtualization.&amp;quot; The ESXi installer will fail if this is not enabled.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/09-customize-hardware.jpg&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;p&gt;Click finish to create the virtual machine.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/10-ready-to-complete.jpg&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;h2&gt;3. Enable promiscuous mode on virtual portgroup being used by ESXi virtual machine&lt;/h2&gt;

&lt;p&gt;You will need to ensure the portgroup (virtual network) you are connecting the ESXi virtual machine to has promiscuous mode enabled. Enabling promiscuous mode is required for the ESXi virtual machine to pass traffic to the child virtual machines running on it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/29-enable-protgroup-promiscuous-mode.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;h2&gt;4. Connect the virtual CDROM of the ESXi virtual machine to the ESXi 6.0 installer .iso and power it on.&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/11-start-esxi-vm.jpg&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;h2&gt;5. Connect to the console of the ESXi virtual machine and step through the installer&lt;/h2&gt;

&lt;p&gt;Press enter to begin&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/12-esxi-installer.jpg&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;p&gt;Press F11 to accept the EULA&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/13-esxi-installer.jpg&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;p&gt;Select the 100GB virtual hard drive we created with the virtual machine&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/14-esxi-installer.jpg&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;p&gt;Select your keyboard layout and press enter&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/15-esxi-installer.jpg&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;p&gt;Enter the root password twice and press enter&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/16-esxi-installer.jpg&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/17-esxi-installer.jpg&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;p&gt;Press F11 to begin the install process&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/18-esxi-installer.jpg&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;p&gt;Disconnect the .iso file from the virtual CDROM before rebooting the virtual machine&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/19-esxi-installer.jpg&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/20-esxi-installer.jpg&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/21-esxi-post-install.jpg&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;h2&gt;6. Press F2 to log into ESXi and make the following changes:&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Set the management IP address, subnet mask and gateway&lt;/li&gt;
&lt;li&gt;Set the hostname, dns servers and search domain&lt;/li&gt;
&lt;li&gt;Enable ssh&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/22-set-ip-address.jpg&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/23-apply-network-config.jpg&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/24-enable-ssh.jpg&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;h2&gt;7. On your local machine download the embedded host client .vib&lt;/h2&gt;

&lt;p&gt;The embedded host client is a VMware fling that allows you to manage an ESXi host from a browser, without needing the #c client or vCenter. You can download the embedded host client .vib from &lt;a href=&quot;https://labs.vmware.com/flings/esxi-embedded-host-client&quot;&gt;this link&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/25-download-embedded-host-client.jpg&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;h2&gt;8. SCP the downloaded .vib to the ESXi virtual machine&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;sdorsett-mbp:~ sdorsett$ scp ~/Downloads/esxui_signed.vib root@192.168.1.51:/tmp/
The authenticity of host &amp;#39;192.168.1.51 (192.168.1.51)&amp;#39; cant be established.
RSA key fingerprint is 82:e9:6b:9e:9d:ac:d7:8a:65:e2:9e:bf:60:fc:2b:df.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added &amp;#39;192.168.1.51&amp;#39; (RSA) to the list of known hosts.
Password: ********
esxui_signed.vib                                                                                                                                100% 2805KB   2.7MB/s   00:00
sdorsett-mbp:~ sdorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;9. SSH to the ESXi virtual machine and install the embedded host client .vib&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;sdorsett-mbp:~ sdorsett$ ssh root@192.168.1.51
Password: ********
The time and date of this login have been sent to the system logs.

VMware offers supported, powerful system administration tools.  Please
see www.vmware.com/go/sysadmintools for details.

The ESXi Shell can be disabled by an administrative user. See the
vSphere Security documentation for more information.

[root@packer-esxi:~] cd tmp
[root@packer-esxi:/tmp] ls
esxui_signed.vib  nfsgssd_krb5cc    probe.session     vmware-root

[root@packer-esxi:/tmp] esxcli software vib install -v /tmp/esxui_signed.vib
Installation Result
   Message: Operation finished successfully.
   Reboot Required: false
   VIBs Installed: VMware_bootbank_esx-ui_0.0.2-0.1.3357452
   VIBs Removed:
   VIBs Skipped:

[root@packer-esxi:/tmp]&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;10. Enable guest ip hack on the ESXi virtual machine&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;https://www.packer.io/docs/builders/vmware-iso.html&quot;&gt;Packer VMware .iso builder documentation&lt;/a&gt; lists the following esxcli command as needing to be run on the ESXi virtual machine:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/28-enable-guest-ip-hack.jpg&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@packer-esxi:/tmp] esxcli system settings advanced set -o /Net/GuestIPHack -i 1
[root@packer-esxi:/tmp] exit
Connection to 192.168.1.51 closed.
sdorsett-mbp:~ sdorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;11. Log into the embedded host client to validate that it it working properly&lt;/h2&gt;

&lt;p&gt;Open a browser on your local machine and go to https://[ESXi-virtual-machine-ip-address]/ui&lt;br&gt;
Accept any certificate warnings and log in using root as the username and the password you entered while installing ESXi.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/26-embedded-host-client.jpg&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;p&gt;Under Storage | Datastores we can see a datastore name &amp;quot;datastore1&amp;quot; was automatically created using the extra space of the virtual hard drive.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/27-embedded-host-client.jpg&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;h3&gt;That all for this post covering how to create a ESXi virtual machine that we will use to create Packer images.&lt;/h3&gt;

&lt;h3&gt;Please provide any feedback or suggestions to my twitter account located on the about page.&lt;/h3&gt;
</content>
 </entry>
 
 <entry>
   <title>Setting up a pipeline for creating Packer .box files</title>
   <link href="http://0.0.0.0:4000/2015/12/22/pipeline-for-creating-packer-box-files/"/>
   <updated>2015-12-22T00:00:00+00:00</updated>
   <id>http://0.0.0.0:4000/2015/12/22/pipeline-for-creating-packer-box-files</id>
   <content type="html">&lt;p&gt;Recently at work, the vCloud Air Zombie team has been using Packer to generate Vagrant templates for use in development and testing.  I have previously covered &lt;a href=&quot;https://sdorsett.github.io/2015/01/03/using-packer-on-centos/&quot;&gt;how to use Packer to create create a .box template for use with Vagrant&lt;/a&gt;, but I thought it might be useful to others to demonstrate how we are using Packer to create images.&lt;/p&gt;

&lt;p&gt;This will be the first of several blog posts in which I intend to cover:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://sdorsett.github.io/2015/12/23/installing-esxi-virtual-machine-for-packer-depolyment/&quot;&gt;Installing a ESXi 6.0 virtual machine for use with Packer&lt;/a&gt; - this is where Packer will be creating the virtual machine image.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://sdorsett.github.io/2015/12/24/installing-packer-and-ovftool-on-centos/&quot;&gt;Setting up Packer, ovftool and Apache web server on a CentOS virtual machine&lt;/a&gt; - this will be where we will be editing and running the Packer templates.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://sdorsett.github.io/2015/12/25/creating-a-packer-template-for-installing-centos-67/&quot;&gt;Creating our first Packer template for installing CentOS 6.7 with vmtools&lt;/a&gt; - templates are the instructions for how a Packer image should be built.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://sdorsett.github.io/2015/12/26/copy-our-existing-template-and-add-the-puppet-agent/&quot;&gt;Copying our existing CentOS 6.7 template and adding the Puppet agent&lt;/a&gt; - having the puppet agent in an image allows us to use puppet to describe configurations using puppet in either Packer and Vagrant.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://sdorsett.github.io/2015/12/27/using-ovftool-to-export-packer-generated-virtual-machines/&quot;&gt;Using ovftool to convert Packer generated virtual machines into Vagrant .box files&lt;/a&gt; - ovftool allows you to export the Packer created images in either a Fusion or vSphere compatible format.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://sdorsett.github.io/2015/12/28/scripted-packer-build-and-export/&quot;&gt;Scripted Packer build, ovftool export and Vagrant .box file creation&lt;/a&gt; - pulling everything we have done with Packer template creation and ovftool export into a single script&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I have also created a github repository to contain the Packer configuration files using during this series. You can access the github repository at &lt;a href=&quot;https://github.com/sdorsett/packer-templates&quot;&gt;https://github.com/sdorsett/packer-templates&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;These posts will hopefully be helpful by providing details on the process of using Packer to generate Vagrant .box files.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Creating a vCSA 5.5 .box template on CentOS 6.5 for vagrant deployment</title>
   <link href="http://0.0.0.0:4000/2015/01/06/creating-vcsa-box-manually/"/>
   <updated>2015-01-06T00:00:00+00:00</updated>
   <id>http://0.0.0.0:4000/2015/01/06/creating-vcsa-box-manually</id>
   <content type="html">&lt;p&gt;In the last few blogs post we have created ESXi .box templates, but in order to create a complete virtual lab using vagrant we will also need a vCenter Server Appliance virtual machine. The vCSA comes as a .ova template, so we will need to convert it to a vagrant-vmware-ovf .box template before we can use it with vagrant.&lt;/p&gt;

&lt;p&gt;In this post we will need several packages installed, that we have covered in the last few posts:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;vagrant - I used vagrant 1.7.1 for these steps&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/gosddc/vagrant-vcenter&quot;&gt;gosddc/vagrant-vcenter&lt;/a&gt;.
This repo contains a vagrant plugin (provider) that will deploy a vagrant-vmware-ovf generated .box template to a vcenter instance. This provider allows vagrant to seamlessly deploy to vCenter.&lt;/li&gt;
&lt;li&gt;ovftool - I have ovftool 3.5.1 installed on my CentOS vm.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I will be using the same CentOS virtual machine we used &lt;a href=&quot;https://sdorsett.github.io/2015/01/04/installing-vagrant-on-centos/&quot;&gt;in the last post&lt;/a&gt; since it already has all the packages we will be needing.&lt;/p&gt;

&lt;h2&gt;1. Download the vCenter Server Appliance .iso from the &lt;a href=&quot;https://my.vmware.com/web/vmware/details?downloadGroup=VC55U2&amp;productId=353&quot;&gt;vmware.com download page&lt;/a&gt;.&lt;/h2&gt;

&lt;p&gt;I will be using &amp;quot;VMware-vCenter-Server-Appliance-5.5.0.20000-2063318_OVF10.ova&amp;quot;, which is the vCSA 5.5 U2 .ova. &lt;/p&gt;

&lt;h2&gt;2. Use ovftool to upload the vCSA .ova to vCenter:&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# ovftool --disableVerification  --noSSLVerify --datastore=esx01-local-sata --name=vcsa-5.5.0.20000-2063318 --network=vlan2 ~/VMware-vCenter-Server-Appliance-5.5.0.20000-2063318_OVF10.ova vi://root@192.168.1.201
Opening OVA source: VMware-vCenter-Server-Appliance-5.5.0.20000-2063318_OVF10.ova
The manifest validates
Enter login information for target vi://192.168.1.201/
Username: root
Password: **********
Opening VI target: vi://root@192.168.1.201:443/
Deploying to VI: vi://root@192.168.1.201:443/
Transfer Completed
Completed successfully
[root@vagrant ~]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You will need to modify the following parameters in the ovftool command:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;--datastore - this is the datastore that ovftool will deploy the virtual machine to&lt;/li&gt;
&lt;li&gt;--name - this is the name the deployed virtual machine will be given&lt;/li&gt;
&lt;li&gt;--network - this is the portgroup the deployed virtual machine will be connected to&lt;/li&gt;
&lt;li&gt;vi://root://[esxi_host_ip_address] - update this part of the command with the IP address of one of your ESXi servers&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;3. Modify the vCSA virtual machine for vagrant deployment.&lt;/h2&gt;

&lt;h4&gt;A. Power on the vcsa-5.5.0.20000-2063318 virtual machine in the VI or vSphere web client.&lt;/h4&gt;

&lt;h4&gt;B. Once the vCSA virtual machine is up and running, SSH to the IP address that is acquired by DHCP.&lt;/h4&gt;

&lt;h5&gt;C. Create the ~/.ssh directory&lt;/h5&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;localhost:~ # mkdir ~/.ssh&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;D. Add the vagrant public ssh key to ~/.ssh/authorized_keys:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;localhost:~ # cd .ssh
localhost:~/.ssh # echo &amp;#39;ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA6NF8iallvQVp22WDkTkyrtvp9eWW6A8YVr+kz4TjGYe7gHzIw+niNltGEFHzD8+v1I2YJ6oXevct1YeS0o9HZyN1Q9qgCgzUFtdOKLv6IedplqoPkcmF0aYet2PkEDo3MlTBckFXPITAMzF8dJSIFo9D8HfdOV0IAdx4O7PtixWKn5y2hMNG0zQPyUecp4pzC6kivAIhyfHilFR61RGL+GPXQ2MWZWFYbAGjyiYJnAmCP3NOTd0jMZEnDkbUvxhMmBYSdETk1rRgm+R4LOzFUGaHqHDLKLX+FIPKcF96hrucXzcWyLbIbEgE98OHlnVYCzRdK8jlqm8tehUc9c9WhQ== vagrant insecure public key&amp;#39; &amp;gt; ~/.ssh/authorized_keys&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;E. Modify the permissions on ~/.ssh &amp;amp; ~/.ssh/authorized_keys since openssh is particular about file permissions:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;localhost:~/.ssh # chmod 700 ~/.ssh
localhost:~/.ssh # chmod 600 ~/.ssh/authorized_keys&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;F. Modify the following lines in /etc/ssh/sshd_config:&lt;/h4&gt;

&lt;p&gt;Uncomment and update the following lines:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;PubkeyAuthentication yes                  # line 47
AuthorizedKeysFile ~/.ssh/authorized_keys # line 48&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Comment out the following line:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;#MaxSessions 1                            # line 44&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;G. Power down the vcsa-5.5.0.20000-2063318 virtual machine in the VI or vSphere web client.&lt;/h4&gt;

&lt;h2&gt;4. Use ovftool to export the vcsa-5.5.0.20000-2063318 virtual machine.&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# ovftool vi://root@192.168.1.201/vcsa-5.5.0.20000-2063318 ./
Enter login information for source vi://192.168.1.201/
Username: root
Password: **********
Opening VI source: vi://root@192.168.1.201:443/vcsa-5.5.0.20000-2063318
Opening OVF target: ./
Writing OVF package: ./vcsa-5.5.0.20000-2063318/vcsa-5.5.0.20000-2063318.ovf
Transfer Completed
Completed successfully&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&amp;quot;vi://root@192.168.1.201&amp;quot; will need to be updated to use the IP address of the ESXi host that has your vcsa-5.5.0.20000-2063318 virtual machine.&lt;/p&gt;

&lt;h2&gt;5. add the additional vagrant-vmware-ovf files and tar up the vcsa-5.5.0.20000-2063318 .ovf as a .box file:&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# cd vcsa-5.5.0.20000-2063318/
[root@vagrant vcsa-5.5.0.20000-2063318]# echo &amp;#39;{&amp;quot;provider&amp;quot;:&amp;quot;vmware_ovf&amp;quot;}&amp;#39; &amp;gt;&amp;gt; metadata.json
[root@vagrant vcsa-5.5.0.20000-2063318]# touch Vagrantfile
[root@vagrant vcsa-5.5.0.20000-2063318]# tar cvzf vcsa-5.5.0.20000-2063318-vmware_ovf-1.0.box ./*
./metadata.json
./Vagrantfile
./vcsa-5.5.0.20000-2063318-disk1.vmdk
./vcsa-5.5.0.20000-2063318-disk2.vmdk
./vcsa-5.5.0.20000-2063318.mf
./vcsa-5.5.0.20000-2063318.ovf
[root@vagrant vcsa-5.5.0.20000-2063318]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;6. Create a directory to keep our vagrant-vmware-ovf .box template in and move the vCSA template to it:&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vcsa-5.5.0.20000-2063318]# mkdir ~/box-files/
[root@vagrant vcsa-5.5.0.20000-2063318]# mv vcsa-5.5.0.20000-2063318-vmware_ovf-1.0.box ~/box-files/&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;C. Create directory and Vagrantfile for the vCSA virtual machines we will deploy with vagrant:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vcsa-5.5.0.20000-2063318]# mkdir -p ~/vagrant-vms/vcsa-test/
[root@vagrant vcsa-5.5.0.20000-2063318]# cd ~/vagrant-vms/vcsa-test/
[root@vagrant vcsa-test]# vi Vagrantfile
[root@vagrant vcsa-test]# cat Vagrantfile
vcsa_box_url = &amp;#39;/root/box-files/vcsa-5.5.0.20000-2063318-vmware_ovf-1.0.box&amp;#39;

$script = &amp;lt;&amp;lt;SCRIPT
#!/bin/bash
# Commands to configure all the necessary services to start the vCenter Service, thanks @lamw ! see:
# http://www.virtuallyghetto.com/2012/02/automating-vcenter-server-appliance.html
 
echo &amp;quot;Accepting EULA ...&amp;quot;
/usr/sbin/vpxd_servicecfg eula accept
 
echo &amp;quot;Configuring Embedded DB ...&amp;quot;
/usr/sbin/vpxd_servicecfg db write embedded
 
echo &amp;quot;Configuring SSO...&amp;quot;
/usr/sbin/vpxd_servicecfg sso write embedded
 
echo &amp;quot;Starting VCSA ...&amp;quot;
/usr/sbin/vpxd_servicecfg service start

# http://www.virtuallyghetto.com/2014/02/how-to-automate-ntp-configurations-on.html
/usr/sbin/vpxd_servicecfg timesync write ntp &amp;#39;pool.ntp.org&amp;#39; &amp;#39;&amp;#39;

# Command to set IP address to 192.168.1.81:
# http://www.virtuallyghetto.com/2013/02/automating-vcsa-network-configurations.html
echo &amp;quot;Setting static IP address to 192.168.1.81&amp;quot;
/opt/vmware/share/vami/vami_set_network eth0 STATICV4 192.168.1.81 255.255.255.0 192.168.1.1 &amp;gt;&amp;amp;- 2&amp;gt;&amp;amp;- &amp;lt;&amp;amp;- &amp;amp;
SCRIPT

nodes = [
  { :hostname =&amp;gt; &amp;#39;vcsa-01a&amp;#39;, :box =&amp;gt; &amp;#39;vcsa-5.5.0.20000-2063318&amp;#39;, :box_url =&amp;gt; vcsa_box_url}
]
 
Vagrant.configure(&amp;#39;2&amp;#39;) do |config|
 
  config.vm.provider :vcenter do |vcenter|
    vcenter.hostname = &amp;#39;192.168.1.195&amp;#39;
    vcenter.username = &amp;#39;root&amp;#39;
    vcenter.password = &amp;#39;mySecretP@ssw0rd&amp;#39;
    vcenter.folder_name = &amp;#39;Vagrant/Deployed&amp;#39;
    vcenter.datacenter_name = &amp;#39;datacenter-01&amp;#39;
    vcenter.computer_name = &amp;#39;cluster-01&amp;#39;
    vcenter.datastore_name = &amp;#39;vsanDatastore&amp;#39;
    vcenter.template_folder_name = &amp;#39;Vagrant/Templates&amp;#39;
    vcenter.network_name = &amp;#39;vlan2&amp;#39;
    vcenter.linked_clones = true
    vcenter.enable_vm_customization = false
  end
 
  # Go through nodes and configure each of them.j
  nodes.each do |node|
    config.vm.define node[:hostname] do |node_config|
 
      if node[:hostname].include? &amp;#39;vcsa-&amp;#39;
        node_config.ssh.username = &amp;#39;root&amp;#39;
        node_config.ssh.insert_key = false
        node_config.vm.synced_folder &amp;#39;.&amp;#39;, &amp;#39;/vagrant&amp;#39;, disabled: true
        node_config.vm.provision &amp;quot;shell&amp;quot;, inline: $script
      end 
      node_config.vm.box = node[:box]
      node_config.vm.hostname = node[:hostname]
      node_config.vm.box_url = node[:box_url]
    end
  end
end&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This Vagrantfile is specifying vagrant to use the vagrant-vcenter profiler to create one virtual machine (vcsa-01a) from the vcsa-5.5.0.20000-2063318-vmware_ovf-1.0.box file. You will need update the following properties to reflect your own vCenter configuration:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;vcenter.hostname = the IP address of your vCenter server&lt;/li&gt;
&lt;li&gt;vcenter.username = the username used to connect to your vCenter server&lt;/li&gt;
&lt;li&gt;vcenter.password = the password used to connect to your vCenter server&lt;/li&gt;
&lt;li&gt;vcenter.datacenter_name = the vCenter virtual datacenter to use for virtual machine deployment&lt;/li&gt;
&lt;li&gt;vcenter.computer_name = the vCenter host or cluster to use for virtual machine deployment. I&amp;#39;ve used the cluster in my example.&lt;/li&gt;
&lt;li&gt;vcenter.datastore_name = the vCenter datastore to use for virtual machine deployment&lt;/li&gt;
&lt;li&gt;vcenter.folder_name = the vm folder that the virtual machines will be deployed to&lt;/li&gt;
&lt;li&gt;vcenter.template_folder_name = the vm folder that the vCSA template will be created in&lt;/li&gt;
&lt;li&gt;vcenter.network_name = the vCenter portgroup to connect the vCSA template to. There should be a DHCP server on this portgroup to provide IP addresses to the deployed vCSA template.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There is also a script section at the top of the Vagrantfile I would like to point out. William Lam has posted several posts documenting how to automate vCSA configuration, and several of his suggestions have been included in this section. Namely those are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Accepting the EULA&lt;/li&gt;
&lt;li&gt;Configuring the vCSA to use the internal database&lt;/li&gt;
&lt;li&gt;Configuring internal SSO&lt;/li&gt;
&lt;li&gt;Starting the vCenter services&lt;/li&gt;
&lt;li&gt;Setting NTP&lt;/li&gt;
&lt;li&gt;Assigning a static IP address using vami_set_network command&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You might want to adjust the last part of the script, since it will set vCSA to use 192.168.1.81 as it&amp;#39;s static IP address. &lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;echo &amp;quot;Setting static IP address to 192.168.1.81&amp;quot;
/opt/vmware/share/vami/vami_set_network eth0 STATICV4 192.168.1.81 255.255.255.0 192.168.1.1 &amp;gt;&amp;amp;- 2&amp;gt;&amp;amp;- &amp;lt;&amp;amp;- &amp;amp;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;I prefer to set a static address for the vCSA, so that any scripts that follow the vCSA deployment will know exactly what IP address to use to connect to it. Also you might be wondering about extra characters following the VAMI command. Since this command is modifying the IP address of the virtual machine, we&amp;#39;re need to run this command in the background without expecting a response. Here the breakdown of what is being done:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&amp;gt;&amp;amp;- means close stdout.&lt;/li&gt;
&lt;li&gt;2&amp;gt;&amp;amp;- means close stderr.&lt;/li&gt;
&lt;li&gt;&amp;lt;&amp;amp;- means close stdin.&lt;/li&gt;
&lt;li&gt;&amp;amp; means run in the background&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If we didn&amp;#39;t pipe the vami_set_network command through these commands, vagrant would keep waiting on output in the SSH session that the command was successfully run, but the IP address had been changed so it would never receive the expected response. &lt;/p&gt;

&lt;h2&gt;4. &amp;quot;vagrant up&amp;quot;&lt;/h2&gt;

&lt;h4&gt;A. Now that we have created a Vagrantfile config file and updated it with our vCenter information we can bring up our virtual machine by running &amp;quot;vagrant up&amp;quot; in the directory with the Vagrantfile.&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vcsa-test]# vagrant up
Bringing machine &amp;#39;vcsa-01a&amp;#39; up with &amp;#39;vcenter&amp;#39; provider...
==&amp;gt; vcsa-01a: Creating VM...
==&amp;gt; vcsa-01a: Powering on VM...
==&amp;gt; vcsa-01a: Running provisioner: shell...
    vcsa-01a: Running: inline script
==&amp;gt; vcsa-01a: Accepting EULA ...
==&amp;gt; vcsa-01a: VC_CFG_RESULT=0
==&amp;gt; vcsa-01a: Configuring Embedded DB ...
==&amp;gt; vcsa-01a: insserv: Service network is missed in the runlevels 4 to use service postgresql
==&amp;gt; vcsa-01a: insserv: Service syslog is missed in the runlevels 4 to use service postgresql
==&amp;gt; vcsa-01a: VC_DB_SCHEMA_VERSION=VirtualCenter Database 5.5
==&amp;gt; vcsa-01a: VC_DB_SCHEMA_INITIALIZED=1
==&amp;gt; vcsa-01a: VC_CFG_RESULT=0
==&amp;gt; vcsa-01a: Configuring SSO...
==&amp;gt; vcsa-01a: VC_CFG_RESULT=0
==&amp;gt; vcsa-01a: Starting VCSA ...
==&amp;gt; vcsa-01a: VC_CFG_RESULT=0
==&amp;gt; vcsa-01a: insserv: Service network is missed in the runlevels 4 to use service postgresql
==&amp;gt; vcsa-01a: insserv: Service syslog is missed in the runlevels 4 to use service postgresql
==&amp;gt; vcsa-01a: VC_CFG_RESULT=0
==&amp;gt; vcsa-01a: Setting static IP address to 192.168.1.81
[root@vagrant vcsa-test]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
 

&lt;h4&gt;B. Once the &amp;quot;vagrant up&amp;quot; command completes, you can check the status of the vCSA vm using &amp;quot;vagrant status&amp;quot;:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vcsa-test]# vagrant status
Current machine states:

vcsa-01a                  running (vcenter)

The VM is running. To stop this VM, you can run `vagrant halt` to
shut it down forcefully, or you can run `vagrant suspend` to simply
suspend the virtual machine. In either case, to restart it again,
simply run `vagrant up`.
[root@vagrant vcsa-test]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;D. You can now ssh into the vCSA virtual machine using &amp;quot;vagrant ssh&amp;quot;:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vcsa-test]# vagrant ssh
==&amp;gt; vcsa-01a: External IP for vcsa-01a: 192.168.1.81
Last login: Sat Jan 10 20:08:07 UTC 2015 from 192.168.1.24 on pts/0
Last login: Sat Jan 10 20:09:36 2015 from 192.168.1.24
localhost:~ # service vmware-vpxd status
vmware-vpxd is running
tomcat is running
localhost:~ # exit
logout
Connection to 192.168.1.81 closed.
[root@vagrant vcsa-test]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;I wanted to point out that vagrant automatically detected up the IP address change we made through vmtools, so we didn&amp;#39;t have to perform any additional steps for &amp;quot;vagrant ssh&amp;quot; to work.&lt;/p&gt;

&lt;h4&gt;E. Once we are done using our virtual machine, it can be destroyed with a &amp;quot;vagrant destroy&amp;quot; command:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vcsa-test]# vagrant destroy
    vcsa-01a: Are you sure you want to destroy the &amp;#39;vcsa-01a&amp;#39; VM? [y/N] y
==&amp;gt; vcsa-01a: Powering off VM...
==&amp;gt; vcsa-01a: Destroying VM...
[root@vagrant vcsa-test]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;F. If you are ever unsure of the state of the vagrant virtual machine you can run &amp;quot;vagrant status&amp;quot; to check:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vcsa-test]# vagrant status
Current machine states:

vcsa-01a                  not created (vcenter)

The environment has not yet been created. Run `vagrant up` to
create the environment. If a machine is not created, only the
default provider will be shown. So if a provider is not listed,
then the machine is not created for that environment.
[root@vagrant vcsa-test]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3&gt;That all for this post covering how to create a vCSA .box template and deploy it using the vagrant-vcenter provider.&lt;/h3&gt;

&lt;h3&gt;Please provide any feedback or suggestions to my twitter account located on the about page.&lt;/h3&gt;
</content>
 </entry>
 
 <entry>
   <title>Installing vagrant and the vagrant-vcenter provider on CentOS 6.5</title>
   <link href="http://0.0.0.0:4000/2015/01/04/installing-vagrant-on-centos/"/>
   <updated>2015-01-04T00:00:00+00:00</updated>
   <id>http://0.0.0.0:4000/2015/01/04/installing-vagrant-on-centos</id>
   <content type="html">&lt;p&gt;If you followed the steps in one of the two previous posts, you have a ESXi .box template in the vagrant-vmware-ovf format. This format allows for deploying the exact same template to vCenter, vCloud Director or vCloud Air, by simply specifying a different provider in vagrant. This post will cover deploying to vCenter, since that is the most readily available of the three. &lt;/p&gt;

&lt;p&gt;In this post we will again talk about the following helpful gosddc project:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/gosddc/vagrant-vcenter&quot;&gt;gosddc/vagrant-vcenter&lt;/a&gt;.
This repo contains a vagrant plugin (provider) that will deploy a vagrant-vmware-ovf generated .box template to a vcenter instance. This provider allows vagrant to seamlessly deploy to vCenter.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We will need a virtual machine with a minimal install of CentOS 6.5 to install vagrant or you can use the same CentOS virtual machine we used &lt;a href=&quot;https://sdorsett.github.io/2015/01/03/using-packer-on-centos/&quot;&gt;in the last post&lt;/a&gt;. For convenience I will be using the same virtual machine.&lt;/p&gt;

&lt;h2&gt;1. Let&amp;#39;s get started by installing the necessary dependancies.&lt;/h2&gt;

&lt;h4&gt;A. Run the following command to install the some needed packages:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# yum install -y gcc-c++ glibc-headers openssl-devel readline libyaml-devel readline-devel zlib zlib-devel iconv-devel libxml2 libxml2-devel libxslt libxslt-devel wget git unzip&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;B. Download and install ruby-build:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# git clone https://github.com/sstephenson/ruby-build.git
Initialized empty Git repository in /root/ruby-build/.git/
remote: Counting objects: 4212, done.
remote: Compressing objects: 100% (16/16), done.
remote: Total 4212 (delta 4), reused 2 (delta 0)
Receiving objects: 100% (4212/4212), 761.14 KiB | 730 KiB/s, done.
Resolving deltas: 100% (2142/2142), done.
[root@vagrant ~]# cd ruby-build/
[root@vagrant ruby-build]# ./install.sh&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;C. Running the &amp;quot;ruby-build --definitions&amp;quot; command will provide an output of the versions of Ruby that ruby-buid supports:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ruby-build]# ruby-build --definitions
1.8.6-p383
1.8.6-p420
1.8.7-p249
1.8.7-p302
1.8.7-p334
1.8.7-p352
1.8.7-p357
1.8.7-p358
1.8.7-p370
1.8.7-p371
1.8.7-p374
1.8.7-p375
1.9.1-p378
1.9.1-p430
1.9.2-p0
1.9.2-p180
1.9.2-p290
1.9.2-p318
1.9.2-p320
1.9.2-p326
1.9.2-p330
1.9.3-dev
1.9.3-preview1
1.9.3-rc1
1.9.3-p0
1.9.3-p125
1.9.3-p194
1.9.3-p286
1.9.3-p327
1.9.3-p362
1.9.3-p374
1.9.3-p385
1.9.3-p392
1.9.3-p429
1.9.3-p448
1.9.3-p484
1.9.3-p545
1.9.3-p547
1.9.3-p550
1.9.3-p551
2.0.0-dev
2.0.0-preview1
2.0.0-preview2
...&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;D. Install ruby 1.9.3 build 551 using the following command:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ruby-build]# ruby-build 1.9.3-p551 /usr/local/
Downloading yaml-0.1.6.tar.gz...
-&amp;gt; http://dqw8nmjcqpjn7.cloudfront.net/7da6971b4bd08a986dd2a61353bc422362bd0edcc67d7ebaac68c95f74182749
Installing yaml-0.1.6...
Installed yaml-0.1.6 to /usr/local/

Downloading ruby-1.9.3-p551.tar.gz...
-&amp;gt; http://dqw8nmjcqpjn7.cloudfront.net/bb5be55cd1f49c95bb05b6f587701376b53d310eb1bb7c76fbd445a1c75b51e8
Installing ruby-1.9.3-p551...
Installed ruby-1.9.3-p551 to /usr/local/&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;E. Download and install the latest version of ruby gems. You can always find the URL to the most recent version at &lt;a href=&quot;https://rubygems.org/pages/download&quot;&gt;rubygems.org&lt;/a&gt;.&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ruby-build]# cd ~/
[root@vagrant ~]# wget http://production.cf.rubygems.org/rubygems/rubygems-2.4.5.tgz
--2015-01-02 21:14:39-- http://production.cf.rubygems.org/rubygems/rubygems-2.4.5.tgz
Resolving production.cf.rubygems.org... 54.230.6.155, 54.230.6.120, 54.230.7.122, ...
Connecting to production.cf.rubygems.org|54.230.6.155|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 446665 (436K) [application/x-tar]
Saving to: &amp;quot;rubygems-2.4.5.tgz&amp;quot;

100%[==================================================================================================&amp;gt;] 446,665 1.78M/s in 0.2s 

2015-01-02 21:14:45 (1.78 MB/s) - &amp;quot;rubygems-2.4.5.tgz&amp;quot; saved [446665/446665]

[root@vagrant ~]# tar -zxf rubygems-2.4.5.tgz
[root@vagrant ~]# cd rubygems-2.4.5
[root@vagrant rubygems-2.4.5]# ruby setup.rb
RubyGems 2.4.5 installed
Installing ri documentation for rubygems-2.4.5&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;2. Create a directory to keep our vagrant-vmware-ovf .box template to and copy the template we previously created to it:&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant esxi55]# mkdir ~/box-files/
[root@vagrant esxi55]# cp /root/packer/packer-templates/esxi55/esxi55-vmware_ovf-1.1.box ~/box-files/&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;3. The next thing we need to do is download and install vagrant and the vagrant-vcenter provider:&lt;/h2&gt;

&lt;h4&gt;A. Download and install the latest version of vagrant. You can find the URL to the latest versions of the linux 64bit .rpm at &lt;a href=&quot;https://www.vagrantup.com/downloads.html&quot;&gt;vagrantup.com&lt;/a&gt;.&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant rubygems-2.4.5]# cd ~/
[root@vagrant ~]# wget https://dl.bintray.com/mitchellh/vagrant/vagrant_1.7.1_x86_64.rpm
--2015-01-03 16:02:34-- https://dl.bintray.com/mitchellh/vagrant/vagrant_1.7.1_x86_64.rpm
Resolving dl.bintray.com... 108.168.194.92, 108.168.194.91
Connecting to dl.bintray.com|108.168.194.92|:443... connected.
HTTP request sent, awaiting response... 302 
Resolving d29vzk4ow07wi7.cloudfront.net... 54.230.7.101, 54.230.5.129, 54.230.6.222, ...
Connecting to d29vzk4ow07wi7.cloudfront.net|54.230.7.101|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 65197711 (62M) [application/unknown]
Saving to: &amp;quot;vagrant_1.7.1_x86_64.rpm&amp;quot;

100%[==================================================================================================&amp;gt;] 65,197,711 1.92M/s in 33s 

2015-01-03 16:03:18 (1.87 MB/s) - &amp;quot;vagrant_1.7.1_x86_64.rpm&amp;quot; saved [65197711/65197711]

[root@vagrant ~]# rpm -i vagrant_1.7.1_x86_64.rpm
[root@vagrant ~]# vagrant -v
Vagrant 1.7.1&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;B. Install the vagrant-vcenter plugin using the following commands:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# vagrant plugin list
vagrant-share (1.1.4, system)
[root@vagrant ~]# vagrant plugin install vagrant-vcenter
Installing the &amp;#39;vagrant-vcenter&amp;#39; plugin. This can take a few minutes...
Installed the plugin &amp;#39;vagrant-vcenter (0.3.2)&amp;#39;!
[root@vagrant ~]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;C. Create directory and Vagrantfile for the ESXi virtual machines we will deploy with vagrant:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# mkdir -p ~/vagrant-vms/esxi-test/
[root@vagrant ~]# cd vagrant-vms/esxi-test/
[root@vagrant esxi-test]# vi Vagrantfile
[root@vagrant esxi-test]# cat Vagrantfile
esxi_box_url = &amp;#39;/root/box-files/esxi55-vmware_ovf-1.1.box&amp;#39;
 
nodes = [
  { :hostname =&amp;gt; &amp;#39;esx-01a&amp;#39;, :box =&amp;gt; &amp;#39;esxi55&amp;#39;, :box_url =&amp;gt; esxi_box_url},
  { :hostname =&amp;gt; &amp;#39;esx-02a&amp;#39;, :box =&amp;gt; &amp;#39;esxi55&amp;#39;, :box_url =&amp;gt; esxi_box_url},
]
 
Vagrant.configure(&amp;#39;2&amp;#39;) do |config|
 
  config.vm.provider :vcenter do |vcenter|
    vcenter.hostname = &amp;#39;192.168.1.195&amp;#39;
    vcenter.username = &amp;#39;root&amp;#39;
    vcenter.password = &amp;#39;mySecretP@ssw0rd&amp;#39;
    vcenter.folder_name = &amp;#39;Vagrant/Deployed&amp;#39;
    vcenter.datacenter_name = &amp;#39;datacenter-01&amp;#39;
    vcenter.computer_name = &amp;#39;cluster-01&amp;#39;
    vcenter.datastore_name = &amp;#39;vsanDatastore&amp;#39;
    vcenter.template_folder_name = &amp;#39;Vagrant/Templates&amp;#39;
    vcenter.network_name = &amp;#39;vlan2&amp;#39;
    vcenter.linked_clones = true
    vcenter.enable_vm_customization = false
  end
 
  # Go through nodes and configure each of them.j
  nodes.each do |node|
    config.vm.define node[:hostname] do |node_config|
 
      if node[:hostname].include? &amp;#39;esx-&amp;#39;
        node_config.ssh.username = &amp;#39;root&amp;#39;
        node_config.ssh.shell = &amp;#39;sh&amp;#39;
        node_config.ssh.insert_key = false
        node_config.vm.synced_folder &amp;#39;.&amp;#39;, &amp;#39;/vagrant&amp;#39;, disabled: true
      end
 
      node_config.vm.box = node[:box]
      node_config.vm.hostname = node[:hostname]
      node_config.vm.box_url = node[:box_url]
    end
  end
end&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This Vagrantfile is specifying vagrant to use the vagrant-vcenter profiler to create two virtual machines (esx-01a &amp;amp; esx-02a) from the esx55-vmware_ovf-1.1.box file. You will need update the following properties to reflect your own vCenter configuration:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;vcenter.hostname = the IP address of your vCenter server&lt;/li&gt;
&lt;li&gt;vcenter.username = the username used to connect to your vCenter server&lt;/li&gt;
&lt;li&gt;vcenter.password = the password used to connect to your vCenter server&lt;/li&gt;
&lt;li&gt;vcenter.datacenter_name = the vCenter virtual datacenter to use for virtual machine deployment&lt;/li&gt;
&lt;li&gt;vcenter.computer_name = the vCenter host or cluster to use for virtual machine deployment. I&amp;#39;ve used the cluster in my example.&lt;/li&gt;
&lt;li&gt;vcenter.datastore_name = the vCenter datastore to use for virtual machine deployment&lt;/li&gt;
&lt;li&gt;vcenter.folder_name = the vm folder that the virtual machines will be deployed to&lt;/li&gt;
&lt;li&gt;vcenter.template_folder_name = the vm folder that the ESXi template will be created in&lt;/li&gt;
&lt;li&gt;vcenter.network_name = the vCenter portgroup to connect the ESXi templates to. There should be a DHCP server on this portgroup to provide IP addresses to the deployed ESXi templates.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;4. &amp;quot;vagrant up&amp;quot;&lt;/h2&gt;

&lt;h4&gt;A. Now that we have created a Vagrantfile config file and updated it with our vCenter information we can bring up our virtual machine by running &amp;quot;vagrant up&amp;quot; in the directory with the Vagrantfile.&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant esxi-test]# vagrant up
Bringing machine &amp;#39;esx-01a&amp;#39; up with &amp;#39;vcenter&amp;#39; provider...
Bringing machine &amp;#39;esx-02a&amp;#39; up with &amp;#39;vcenter&amp;#39; provider...
==&amp;gt; esx-01a: Box &amp;#39;esxi55&amp;#39; could not be found. Attempting to find and install...
 esx-01a: Box Provider: vmware_ovf, vcloud, vcenter
 esx-01a: Box Version: &amp;gt;= 0
==&amp;gt; esx-01a: Adding box &amp;#39;esxi55&amp;#39; (v0) for provider: vmware_ovf, vcloud, vcenter
 esx-01a: Downloading: file:///root/box-files/esxi55-vmware_ovf-1.1.box
==&amp;gt; esx-01a: Successfully added box &amp;#39;esxi55&amp;#39; (v0) for &amp;#39;vmware_ovf&amp;#39;!
==&amp;gt; esx-02a: Box &amp;#39;esxi55&amp;#39; could not be found. Attempting to find and install...
 esx-02a: Box Provider: vmware_ovf, vcloud, vcenter
 esx-02a: Box Version: &amp;gt;= 0
==&amp;gt; esx-02a: Adding box &amp;#39;esxi55&amp;#39; (v0) for provider: vmware_ovf, vcloud, vcenter
==&amp;gt; esx-02a: Uploading [esxi55]...
==&amp;gt; esx-02a: Adding [esxi55]
2015-01-03 16:28:34 -0600: networks: vlan2 = vlan2
2015-01-03 16:28:34 -0600: Uploading OVF to esx02.tyrell.corp...
==&amp;gt; esx-01a: Uploading [esxi55]...
==&amp;gt; esx-01a: Adding [esxi55]
2015-01-03 16:28:35 -0600: networks: vlan2 = vlan2
2015-01-03 16:28:35 -0600: Uploading OVF to esx01.tyrell.corp...
DEBUG: Timeout: 300
Iteration 1: Trying to get host&amp;#39;s IP address ...
 % Total % Received % Xferd Average Speed Time Time Time Current
 Dload Upload Total Spent Left Speed
 15 473M 15 75.3M 0 0 4554k 0 0:01:46 0:00:16 0:01:30 4552k2015-01-03 16:29:02 -0600: Template already exists, waiting for it to be ready
 20 473M 20 98.8M 0 0 5122k 0 0:01:34 0:00:19 0:01:15 7489k2015-01-03 16:29:05 -0600: Template VM found
100 473M 100 473M 0 0 9245k 0 0:00:52 0:00:52 --:--:-- 9.9M
Iteration 1: Trying to access nfcLease.info.entity ...
HttpNfcLeaseComplete succeeded
esxi_box_url = &amp;#39;/root/box-files/esxi55-vmware_ovf-1.1.box&amp;#39;
==&amp;gt; esx-02a: Creating VM...
2015-01-03 16:29:42 -0600: Template fully prepared and ready to be cloned
==&amp;gt; esx-01a: Creating VM...
==&amp;gt; esx-01a: Powering on VM...
==&amp;gt; esx-02a: Powering on VM...&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;
 

&lt;h4&gt;B. Once the &amp;quot;vagrant up&amp;quot; command completes, you can check the status of the vms using &amp;quot;vagrant status&amp;quot;:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant esxi-test]# vagrant status
Current machine states:

esx-01a running (vcenter)
esx-02a running (vcenter)

This environment represents multiple VMs. The VMs are all listed
above with their current state. For more information about a specific
VM, run `vagrant status NAME`.&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;C. Looking in vCenter you will see the virtual machines that vagrant deployed:&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/01-esx-01a.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;h4&gt;D. You can now ssh into either of the ESXi virtual machines using &amp;quot;vagrant ssh [vm_name]&amp;quot;:&lt;/h4&gt;

&lt;p&gt;To connect to esx-01a you simply run &amp;quot;vagrant ssh esx-01a&amp;quot;:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant esxi-test]# vagrant ssh esx-01a
==&amp;gt; esx-01a: External IP for esx-01a: 192.168.1.177
The time and date of this login have been sent to the system logs.

VMware offers supported, powerful system administration tools. Please
see www.vmware.com/go/sysadmintools for details.

The ESXi Shell can be disabled by an administrative user. See the
vSphere Security documentation for more information.

~ # esxcli network ip interface list
vmk0
 Name: vmk0
 MAC Address: 00:50:56:6f:9d:66
 Enabled: true
 Portset: vSwitch0
 Portgroup: Management Network
 Netstack Instance: defaultTcpipStack
 VDS Name: N/A
 VDS UUID: N/A
 VDS Port: N/A
 VDS Connection: -1
 MTU: 1500
 TSO MSS: 65535
 Port ID: 33554437
~ # exit
Connection to 192.168.1.177 closed.
[root@vagrant esxi-test]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The same can be done for esx-02a:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant esxi-test]# vagrant ssh esx-02a
==&amp;gt; esx-02a: External IP for esx-02a: 192.168.1.178
The time and date of this login have been sent to the system logs.

VMware offers supported, powerful system administration tools. Please
see www.vmware.com/go/sysadmintools for details.

The ESXi Shell can be disabled by an administrative user. See the
vSphere Security documentation for more information.
~ # esxcli network ip interface list
vmk0
 Name: vmk0
 MAC Address: 00:50:56:67:d5:b7
 Enabled: true
 Portset: vSwitch0
 Portgroup: Management Network
 Netstack Instance: defaultTcpipStack
 VDS Name: N/A
 VDS UUID: N/A
 VDS Port: N/A
 VDS Connection: -1
 MTU: 1500
 TSO MSS: 65535
 Port ID: 33554437
~ # exit
Connection to 192.168.1.178 closed.
[root@vagrant esxi-test]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;E. Once we are done using our virtual machines they can be destroyed with a &amp;quot;vagrant destroy&amp;quot; command:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant esxi-test]# vagrant destroy
    esx-02a: Are you sure you want to destroy the &amp;#39;esx-02a&amp;#39; VM? [y/N] y
==&amp;gt; esx-02a: Powering off VM...
==&amp;gt; esx-02a: Destroying VM...
    esx-01a: Are you sure you want to destroy the &amp;#39;esx-01a&amp;#39; VM? [y/N] y
==&amp;gt; esx-01a: Powering off VM...
==&amp;gt; esx-01a: Destroying VM...
[root@vagrant esxi-test]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;F. If you are ever unsure of the state of the vagrant virtual machines you can run &amp;quot;vagrant status&amp;quot; to check:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant esxi-test]# vagrant status
Current machine states:

esx-01a                   not created (vcenter)
esx-02a                   not created (vcenter)

This environment represents multiple VMs. The VMs are all listed
above with their current state. For more information about a specific
VM, run `vagrant status NAME`.
[root@vagrant esxi-test]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3&gt;That all for this post covering the basics of getting vagrant and the vagrant-vcenter provider installed/configured on CentOS.&lt;/h3&gt;

&lt;h3&gt;Please provide any feedback or suggestions to my twitter account located on the about page.&lt;/h3&gt;
</content>
 </entry>
 
 <entry>
   <title>Using packer on CentOS 6.5 to create an ESXi .box template for vagrant deployment</title>
   <link href="http://0.0.0.0:4000/2015/01/03/using-packer-on-centos/"/>
   <updated>2015-01-03T00:00:00+00:00</updated>
   <id>http://0.0.0.0:4000/2015/01/03/using-packer-on-centos</id>
   <content type="html">&lt;p&gt;In the previous post I demonstrated using packer to create a ESXi .box template on OS X with fusion and the vagrant vmware provider. Both of these pieces of software have a cost associated with their usage, so in this post I will demonstrate how to use CentOS 6.5 and ESXi for the same results.  &lt;/p&gt;

&lt;p&gt;In this post we will again talk about two helpful gosddc projects:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/gosddc/packer-post-processor-vagrant-vmware-ovf&quot;&gt;gosddc/packer-post-processor-vagrant-vmware-ovf&lt;/a&gt;.
This repo contains a packer post processor that leverages VMware OVF Tool to create a vmware_ovf Vagrant box that is compatible with vagrant-vcloud, vagrant-vcenter and vagrant-vcloudair vagrant providers. It is only compatible with the packer VMware builder. This project is a post processor that makes the generation of the .box file seemless. Unfortunately there currently seems to be &lt;a href=&quot;https://github.com/mitchellh/packer/issues/1457&quot;&gt;a bug with packer&lt;/a&gt; exporting the virtual machine artifact from a remote ESXi server. Hopefully this issue will be resolved with an upcoming release of packer which will allow this post processor to be used with remote ESXi.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/gosddc/packer-templates&quot;&gt;gosddc/packer-templates&lt;/a&gt;.
This repo contains Packer templates for boxes available at https://vagrantcloud.com/gosddc, they only work with VMware and require the packer-post-processor-vagrant-vmware-ovf post-processor to work. These templates are a good starting point for generating packer templates on VMware products.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We will be using a virtual machine with a minimal install of CentOS 6.5 to install everything we need to build the packer template.&lt;/p&gt;

&lt;h2&gt;1. Let&amp;#39;s get started by installing packer.&lt;/h2&gt;

&lt;h4&gt;A.  Copy the linux 64bit packer installer URL from the following link:&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.packer.io/downloads.html&quot;&gt;https://www.packer.io/downloads.html&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;B.  Use wget on your CentOS vm to download the URL you copied&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# wget https://dl.bintray.com/mitchellh/packer/packer_0.7.5_linux_amd64.zip
--2015-01-02 21:17:41-- https://dl.bintray.com/mitchellh/packer/packer_0.7.5_linux_amd64.zip
Resolving dl.bintray.com... 108.168.194.91, 108.168.194.92
Connecting to dl.bintray.com|108.168.194.91|:443... connected.
HTTP request sent, awaiting response... 302 
Resolving d29vzk4ow07wi7.cloudfront.net... 54.230.5.16, 54.230.5.11, 54.230.5.30, ...
Connecting to d29vzk4ow07wi7.cloudfront.net|54.230.5.16|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 87262135 (83M) [application/unknown]
Saving to: &amp;quot;packer_0.7.5_linux_amd64.zip&amp;quot;

100%[==================================================================================================&amp;gt;] 87,262,135 1.92M/s in 50s 

2015-01-02 21:18:42 (1.65 MB/s) - &amp;quot;packer_0.7.5_linux_amd64.zip&amp;quot; saved [87262135/87262135]&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;C.  Install the unzip package on our CentOS vm since that&amp;#39;s the format packer comes compressed in:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# yum install -y unzip
Loaded plugins: fastestmirror
Setting up Install Process
Loading mirror speeds from cached hostfile
 * base: repos.dfw.quadranet.com
 * extras: mirror.anl.gov
 * updates: mirror.us.oneandone.net
base                                                                                                                 | 3.7 kB     00:00     
extras                                                                                                               | 3.4 kB     00:00     
updates                                                                                                              | 3.4 kB     00:00     
Resolving Dependencies
--&amp;gt; Running transaction check
---&amp;gt; Package unzip.x86_64 0:6.0-1.el6 will be installed
--&amp;gt; Finished Dependency Resolution

Dependencies Resolved

============================================================================================================================================
 Package                         Arch                             Version                              Repository                      Size
============================================================================================================================================
Installing:
 unzip                           x86_64                           6.0-1.el6                            base                           149 k

Transaction Summary
============================================================================================================================================
Install       1 Package(s)

Total download size: 149 k
Installed size: 313 k
Downloading Packages:
unzip-6.0-1.el6.x86_64.rpm                                                                                           | 149 kB     00:00     
Running rpm_check_debug
Running Transaction Test
Transaction Test Succeeded
Running Transaction
  Installing : unzip-6.0-1.el6.x86_64                                                                                                   1/1 
  Verifying  : unzip-6.0-1.el6.x86_64                                                                                                   1/1 

Installed:
  unzip.x86_64 0:6.0-1.el6                                                                                                                  

Complete!
[root@vagrant ~]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;D.  Create the /usr/local/packer_7.5 directory and unzip the packer download to this location.&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# mkdir /usr/local/packer_7.5
[root@vagrant ~]# cp packer_0.7.5_linux_amd64.zip /usr/local/packer_7.5/
[root@vagrant ~]# cd /usr/local/packer_7.5/
[root@vagrant packer_7.5]# unzip packer_0.7.5_linux_amd64.zip
Archive: packer_0.7.5_linux_amd64.zip
inflating: packer 
inflating: packer-builder-amazon-chroot 
inflating: packer-builder-amazon-ebs 
inflating: packer-builder-amazon-instance 
inflating: packer-builder-digitalocean 
inflating: packer-builder-docker 
inflating: packer-builder-googlecompute 
inflating: packer-builder-null 
inflating: packer-builder-openstack 
inflating: packer-builder-parallels-iso 
inflating: packer-builder-parallels-pvm 
inflating: packer-builder-qemu 
inflating: packer-builder-virtualbox-iso 
inflating: packer-builder-virtualbox-ovf 
inflating: packer-builder-vmware-iso 
inflating: packer-builder-vmware-vmx 
inflating: packer-post-processor-atlas 
inflating: packer-post-processor-compress 
inflating: packer-post-processor-docker-import 
inflating: packer-post-processor-docker-push 
inflating: packer-post-processor-docker-save 
inflating: packer-post-processor-docker-tag 
inflating: packer-post-processor-vagrant 
inflating: packer-post-processor-vagrant-cloud 
inflating: packer-post-processor-vsphere 
inflating: packer-provisioner-ansible-local 
inflating: packer-provisioner-chef-client 
inflating: packer-provisioner-chef-solo 
inflating: packer-provisioner-file 
inflating: packer-provisioner-puppet-masterless 
inflating: packer-provisioner-puppet-server 
inflating: packer-provisioner-salt-masterless 
inflating: packer-provisioner-shell&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;E.  Add /usr/local/packer_7.5 to your path by running the following command.&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant packer_7.5]# export PATH=&amp;quot;/usr/local/packer_7.5:$PATH&amp;quot;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;F. Add the export command we just ran into ~/.bash_profile to ensure this path change persists after reboots.&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant packer_7.5]# cat ~/.bash_profile
export PATH=&amp;quot;/usr/local/packer_7.5:$PATH&amp;quot;
[root@vagrant packer_7.5]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;G. Stop iptables since packer will be using a http server to publish the ESXi kickstart files:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant packer_7.5]# service iptables stop
iptables: Setting chains to policy ACCEPT: filter [ OK ]
iptables: Flushing firewall rules: [ OK ]
iptables: Unloading modules: [ OK ]
[root@vagrant packer_7.5]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;2. The next thing we need to do is download and install ovftool:&lt;/h2&gt;

&lt;h4&gt;A. Download and install the latest version of the VMware OVF tool. VMware-ovftool-3.5.1-1747221-lin.x86_64.bundle is what I used.&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# chmod +x VMware-ovftool-3.5.1-1747221-lin.x86_64.bundle
[root@vagrant ~]# ./VMware-ovftool-3.5.1-1747221-lin.x86_64.bundle
Extracting VMware Installer...done.
You must accept the VMware OVF Tool component for Linux End User
License Agreement to continue. Press Enter to proceed.
VMWARE END USER LICENSE AGREEMENT

1.4 &amp;quot;Intellectual Property Rights&amp;quot; means all worldwide intellectual
property rights, including without limitation, copyrights, trademarks, service
Do you agree? [yes/no]: yes

The product is ready to be installed. Press Enter to begin
installation or Ctrl-C to cancel.

Installing VMware OVF Tool component for Linux 3.5.1
Configuring...
[######################################################################] 100%
Installation was successful.&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;B. Verify ovftool is successfully added to your path by running &amp;quot;ovftool -v&amp;quot;. This command should output the version of ovftool we installed.&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# ovftool -v
VMware ovftool 3.5.1 (build-1747221)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;3. Now we can download and install the gosddc packer components we will need.&lt;/h2&gt;

&lt;h4&gt;A. Download the most recent linux amd64 version of the compiled packer-processor-vagrant-vmware-ovf binary from the following link:&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/gosddc/packer-post-processor-vagrant-vmware-ovf/releases&quot;&gt;https://github.com/gosddc/packer-post-processor-vagrant-vmware-ovf/releases&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;B. Unzip packer-post-processor-vagrant-vmware-ovf and copy it to &amp;quot;/usr/local/packer_7.5&amp;quot;. Ensure the permissions of this file match the other files in this directory.&lt;/h4&gt;

&lt;h4&gt;C. Create a directory to contain the packer templates:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# mkdir ~/packer
[root@vagrant ~]# cd ~/packer&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h5&gt;D. Clone the gosddc packer-templates repository:&lt;/h5&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant packer]# git clone https://github.com/gosddc/packer-templates.git
Initialized empty Git repository in /root/packer/packer-templates/.git/
remote: Counting objects: 195, done.
remote: Total 195 (delta 0), reused 0 (delta 0)
Receiving objects: 100% (195/195), 39.32 KiB, done.
Resolving deltas: 100% (105/105), done.&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;4. Next we need to download the ESXi 5.5 .iso and copy it into the proper directory location.&lt;/h2&gt;

&lt;h4&gt;A. create an &amp;quot;iso&amp;quot; directory for storing the ESXi iso files:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant packer]# cd packer-templates/
[root@vagrant packer-templates]# mkdir iso&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;B. Download and copy the &amp;quot;VMware-VMvisor-Installer-5.5.0-1331820.x86_64.iso&amp;quot; ESXi 5.5 installer to the iso directory.&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant packer-templates]# cp ~/VMware-VMvisor-Installer-5.5.0-1331820.x86_64.iso iso/&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;5. Finally we will need to modify, validate and build the packer esxi.json packer template we will be using.&lt;/h2&gt;

&lt;h4&gt;A. modify ~/packer/packer-templates/templates/esxi.json to look like the following:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant packer-templates]# vi templates/esxi.json
[root@vagrant packer-templates]# cat templates/esxi.json
{
  &amp;quot;variables&amp;quot;: {
  &amp;quot;version&amp;quot;: &amp;quot;1.0&amp;quot;
  },
  &amp;quot;builders&amp;quot;: [
    {
      &amp;quot;name&amp;quot;: &amp;quot;esxi55&amp;quot;,
      &amp;quot;vm_name&amp;quot;: &amp;quot;esxi55&amp;quot;,
      &amp;quot;vmdk_name&amp;quot;: &amp;quot;esxi55-disk0&amp;quot;,
      &amp;quot;type&amp;quot;: &amp;quot;vmware-iso&amp;quot;,
      &amp;quot;headless&amp;quot;: true,
      &amp;quot;disk_size&amp;quot;: 4096,
      &amp;quot;guest_os_type&amp;quot;: &amp;quot;centos-64&amp;quot;,
      &amp;quot;iso_url&amp;quot;: &amp;quot;./iso/VMware-VMvisor-Installer-5.5.0-1331820.x86_64.iso&amp;quot;,
      &amp;quot;iso_checksum&amp;quot;: &amp;quot;ef599dc7e647177027684c0eee346ccdbc8704f2&amp;quot;,
      &amp;quot;iso_checksum_type&amp;quot;: &amp;quot;sha1&amp;quot;,
      &amp;quot;remote_host&amp;quot;: &amp;quot;192.168.1.201&amp;quot;,
      &amp;quot;remote_datastore&amp;quot;: &amp;quot;esx01-local-sata&amp;quot;,
      &amp;quot;remote_username&amp;quot;: &amp;quot;root&amp;quot;,
      &amp;quot;remote_password&amp;quot;: &amp;quot;mySecretP@ssw0rd&amp;quot;,
      &amp;quot;remote_type&amp;quot;: &amp;quot;esx5&amp;quot;,
      &amp;quot;ssh_username&amp;quot;: &amp;quot;root&amp;quot;,
      &amp;quot;ssh_password&amp;quot;: &amp;quot;vagrant&amp;quot;,
      &amp;quot;ssh_wait_timeout&amp;quot;: &amp;quot;60m&amp;quot;,
      &amp;quot;shutdown_command&amp;quot;: &amp;quot;esxcli system maintenanceMode set -e true -t 0 ; esxcli system shutdown poweroff -d 10 -r &amp;#39;Packer Shutdown&amp;#39; ; esxcli system maintenanceMode set -e false -t 0&amp;quot;,
      &amp;quot;tools_upload_flavor&amp;quot;: &amp;quot;linux&amp;quot;,
      &amp;quot;http_directory&amp;quot;: &amp;quot;.&amp;quot;,
      &amp;quot;boot_wait&amp;quot;: &amp;quot;5s&amp;quot;,
      &amp;quot;vmx_data&amp;quot;: {
        &amp;quot;memsize&amp;quot;: &amp;quot;4096&amp;quot;,
        &amp;quot;numvcpus&amp;quot;: &amp;quot;2&amp;quot;,
        &amp;quot;vhv.enable&amp;quot;: &amp;quot;TRUE&amp;quot;,
        &amp;quot;ethernet0.virtualDev&amp;quot;: &amp;quot;e1000&amp;quot;,
        &amp;quot;ethernet0.networkName&amp;quot;: &amp;quot;vlan2&amp;quot;,
        &amp;quot;ethernet0.present&amp;quot;: &amp;quot;TRUE&amp;quot;
      },
      &amp;quot;vmx_data_post&amp;quot;: {
        &amp;quot;guestos&amp;quot;: &amp;quot;vmkernel5&amp;quot;,
        &amp;quot;ide1:0.present&amp;quot;: &amp;quot;FALSE&amp;quot;
      },
      &amp;quot;boot_command&amp;quot;: [
        &amp;quot;&amp;lt;enter&amp;gt;&amp;lt;wait&amp;gt;O&amp;lt;wait&amp;gt; ks=http://{{ .HTTPIP }}:{{ .HTTPPort }}/scripts/esxi-5-kickstart.cfg&amp;lt;enter&amp;gt;&amp;quot;
      ]
    }
  ],
  &amp;quot;provisioners&amp;quot;: [
    {
      &amp;quot;type&amp;quot;: &amp;quot;file&amp;quot;,
      &amp;quot;source&amp;quot;: &amp;quot;puppet/modules/vagrantbaseconfig/files/vagrant.pub&amp;quot;,
      &amp;quot;destination&amp;quot;: &amp;quot;/etc/ssh/keys-root/authorized_keys&amp;quot;
    },
    {
      &amp;quot;type&amp;quot;: &amp;quot;shell&amp;quot;,
      &amp;quot;script&amp;quot;: &amp;quot;scripts/esxi-vmware-tools_install.sh&amp;quot;
    },
    {
      &amp;quot;type&amp;quot;: &amp;quot;shell&amp;quot;,
      &amp;quot;script&amp;quot;: &amp;quot;scripts/esxi-cloning_configuration.sh&amp;quot;
    }
  ]
}&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;There are several things I would like to point out in the esxi.json file we just created.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;builder - this section specifies that we will be using the &amp;quot;vmware-iso&amp;quot; builder, with a remote ESXi host, to create our packer template. You will need to modify several part of this to match your ESXi server:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt; remote_host - this is the IP address of our ESXi server.&lt;/li&gt;
&lt;li&gt; remote_datastore - this is the datastore our virtual machine will be built on.&lt;/li&gt;
&lt;li&gt; remote_username - the ESXi username used to connect&lt;/li&gt;
&lt;li&gt; remote_password - the ESXi password used to connect&lt;/li&gt;
&lt;li&gt; remote_type - this specifies the remote server is ESXi 5.x&lt;/li&gt;
&lt;li&gt; ethernet0.networkName - this is the portgroup that the packer virtual machine connects to. This should be the same portgroup that the CentOS vm is one, since the ESXi install will need a kickstart file served by a http server included with packer.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Also in the builder section you might notice we are setting the &amp;quot;guest_os_type&amp;quot; to be &amp;quot;centos-64&amp;quot; during install. This is due to packer attempting to mount the vmtools .iso at the end of the install process. This has no impact on the install and we change it back to &amp;quot;vmkernel5&amp;quot; in the &amp;quot;vmx_data_post&amp;quot; section, which gets processed post after the virtual machine is powered down after the install process has completed.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;provisioners - this section specifies we will be using multiple provisioners to modify our template after it has been created:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;a file provisioner that will copy the vagrant public ssh key into our ESXi template&lt;/li&gt;
&lt;li&gt;a shell script to install the vmware tools VIB for nested ESXi&lt;/li&gt;
&lt;li&gt;a shell script to make necessary MAC address changes in our nested ESXi template.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;post-processors - I have removed the vagrant-vmware-ovf post processor due to the bug I mentioned earlier.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4&gt;B. modify the esxi-cloning_configuration.sh script in the scripts directory to read as follows:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant packer-templates]# vi scripts/esxi-cloning_configuration.sh
[root@vagrant packer-templates]# cat scripts/esxi-cloning_configuration.sh
# Settings to ensure that ESXi cloning goes smooth, thanks @lamw ! see:
# http://www.virtuallyghetto.com/2013/12/how-to-properly-clone-nested-esxi-vm.html
esxcli system settings advanced set -o /Net/FollowHardwareMac -i 1
sed -i &amp;#39;/\/system\/uuid/d&amp;#39; /etc/vmware/esx.conf
sed -i &amp;#39;/\/net\/vmkernelnic\/child\[0000\]\/mac/d&amp;#39; /etc/vmware/esx.conf

# DHCP doesn&amp;#39;t refresh correctly upon boot, this will force a renew, it will
# be executed on every boot, if you don&amp;#39;t like this behavior you can remove
# the line during the Vagrant provisioning part.
sed -i &amp;#39;/exit 0/d&amp;#39; /etc/rc.local.d/local.sh 
echo &amp;#39;esxcli network ip interface set -e false -i vmk0; esxcli network ip interface set -e true -i vmk0&amp;#39; &amp;gt;&amp;gt; /etc/rc.local.d/local.sh 
echo &amp;#39;exit 0&amp;#39; &amp;gt;&amp;gt; /etc/rc.local.d/local.sh 

# Ensure changes are persistent
/sbin/auto-backup.sh&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We are adding the following line to delete the line that contains the vmkernel port MAC address in /etc/vmware/esx.conf, in order to force it to be regenerated on each clones template:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;sed -i &amp;#39;/\/net\/vmkernelnic\/child\[0000\]\/mac/d&amp;#39; /etc/vmware/esx.conf&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;C. Validate the packer esxi.json file is ready for building by running the following command:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant packer-templates]# packer validate templates/esxi.json 
Template validated successfully.&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;D. Start the build by running:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant packer-templates]# packer build templates/esxi.json 
esxi55 output will be in this color.

==&amp;gt; esxi55: Downloading or copying ISO
esxi55: Downloading or copying: file:///root/packer/packer-templates/iso/VMware-VMvisor-Installer-5.5.0-1331820.x86_64.iso
==&amp;gt; esxi55: Uploading ISO to remote machine...
==&amp;gt; esxi55: Creating virtual machine disk
==&amp;gt; esxi55: Building and writing VMX file
==&amp;gt; esxi55: Starting HTTP server on port 8828
==&amp;gt; esxi55: Registering remote VM...
==&amp;gt; esxi55: Starting virtual machine...
esxi55: The VM will be run headless, without a GUI. If you want to
esxi55: view the screen of the VM, connect via VNC without a password to
esxi55: 192.168.1.201:5988
==&amp;gt; esxi55: Waiting 5s for boot...
==&amp;gt; esxi55: Connecting to VM via VNC
==&amp;gt; esxi55: Typing the boot command over VNC...
==&amp;gt; esxi55: Waiting for SSH to become available...
==&amp;gt; esxi55: Connected to SSH!
==&amp;gt; esxi55: Uploading puppet/modules/vagrantbaseconfig/files/vagrant.pub =&amp;gt; /etc/ssh/keys-root/authorized_keys
==&amp;gt; esxi55: Provisioning with shell script: scripts/esxi-vmware-tools_install.sh
esxi55: Installation Result
esxi55: Message: The update completed successfully, but the system needs to be rebooted for the changes to be effective.
esxi55: Reboot Required: true
esxi55: VIBs Installed: VMware_bootbank_esx-tools-for-esxi_9.7.0-0.0.00000
esxi55: VIBs Removed:
esxi55: VIBs Skipped:
==&amp;gt; esxi55: Provisioning with shell script: scripts/esxi-cloning_configuration.sh
esxi55: diff: can&amp;#39;t stat &amp;#39;/tmp/auto-backup.35224//etc/ssh/keys-root/authorized_keys&amp;#39;: No such file or directory
esxi55: Saving current state in /bootbank
esxi55: Clock updated.
esxi55: Time: 04:39:41 Date: 01/03/2015 UTC
==&amp;gt; esxi55: Gracefully halting virtual machine...
esxi55: Waiting for VMware to clean up after itself...
==&amp;gt; esxi55: Deleting unnecessary VMware files...
esxi55: Deleting: /vmfs/volumes/esx01-local-sata/output-esxi55/vmware.log
==&amp;gt; esxi55: Cleaning VMX prior to finishing up...
esxi55: Unmounting floppy from VMX...
esxi55: Detaching ISO from CD-ROM device...
==&amp;gt; esxi55: Compacting the disk image
==&amp;gt; esxi55: Unregistering virtual machine...
Build &amp;#39;esxi55&amp;#39; finished.

==&amp;gt; Builds finished. The artifacts of successful builds are:
--&amp;gt; esxi55: VM files in directory: /vmfs/volumes/esx01-local-sata/output-esxi55
[root@vagrant packer-templates]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If you want to keep an eye on the build process you can connect a VNC client to the address and port listed during the packer build process. For my build this was what was displayed:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;esxi55: The VM will be run headless, without a GUI. If you want to
esxi55: view the screen of the VM, connect via VNC without a password to
esxi55: 192168.1.201:5988&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The other option to track the progress of the ESXi install is to open the console of the virtual machine in the VI or web client.&lt;/p&gt;

&lt;h4&gt;D. The last line of the packer output shows the datastore location of the virtual machine that was created. We need to SSH to the ESXi host in order to re-register the virtual machine, in order to export it using ovftool:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# ssh root@192.168.1.201 &amp;quot;vim-cmd solo/registervm /vmfs/volumes/esx01-local-sata/output-esxi55/*.vmx&amp;quot;
Password: 
70&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Update this command to reflect the IP address and path to the virtual machine that packer created. Also take note of the vmid that the command returns, since we&amp;#39;ll need this in a future step.&lt;/p&gt;

&lt;h4&gt;E. Use the following command to export the virtual machine we just registered:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# ovftool vi://root:[esxi_password]@192.168.1.201/esxi55 ./
Opening VI source: vi://root@192.168.1.201:443/esxi55
Opening OVF target: ./
Writing OVF package: ./esxi55/esxi55.ovf
Transfer Completed 
Completed successfully&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Replace the IP address, password and virtual machine name if needed.&lt;/p&gt;

&lt;h4&gt;F. Unregister the virtual machine template using the following command with the vmid returned by the registration command:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# ssh root@192.168.1.201 &amp;quot;vim-cmd vmsvc/unregister 70&amp;quot;
Password:&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;6. Since we disable the vagrant-vcenter-ovf post processor, we will need to create the .box file according to the specifications&lt;a href=&quot;https://github.com/gosddc/packer-post-processor-vagrant-vmware-ovf/wiki/vmware_ovf-Box-Format&quot;&gt; listed here&lt;/a&gt;:&lt;/h2&gt;

&lt;h4&gt;A. Create the metadata.json file needed by the vagrant-vmware-ovf .box file:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant esxi55]# echo &amp;#39;{&amp;quot;provider&amp;quot;:&amp;quot;vmware_ovf&amp;quot;}&amp;#39; &amp;gt;&amp;gt; metadata.json
[root@vagrant esxi55]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;B. Create an empty Vagrantfile to include in the .box file:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant esxi55]# touch Vagrantfile
[root@vagrant esxi55]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;C. Use tar to compress the files in this directory into a .box file:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant esxi55]# tar cvzf esxi55-vmware_ovf-1.1.box ./*
./esxi55-disk1.vmdk
./esxi55.mf
./esxi55.ovf
./metadata.json
./Vagrantfile
[root@vagrant esxi55]# ls -la
total 968296
drwxr-xr-x. 2 root root      4096 Jan  3 15:36 .
drwxr-xr-x. 9 root root      4096 Jan  3 20:12 ..
-rw-r--r--. 1 root root 496975360 Jan  3 15:32 esxi55-disk1.vmdk
-rw-r--r--. 1 root root       125 Jan  3 15:32 esxi55.mf
-rw-r--r--. 1 root root      7522 Jan  3 15:32 esxi55.ovf
-rw-r--r--. 1 root root 494527542 Jan  3 15:36 esxi55-vmware_ovf-1.1.box
-rw-r--r--. 1 root root        26 Jan  3 15:34 metadata.json
-rw-r--r--. 1 root root         0 Jan  3 15:34 Vagrantfile
[root@vagrant esxi55]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3&gt;That all for this post covering the basics of getting packer and getting it installed/configured on CentOS. In the next blog post I&amp;#39;m planning  on covering how to install vagrant on CentOS and using it to deploy this .box templates to vCenter.&lt;/h3&gt;

&lt;h3&gt;Please provide any feedback or suggestions to my twitter account located on the about page.&lt;/h3&gt;
</content>
 </entry>
 
 <entry>
   <title>Using packer on OS X to create an ESXi .box template for vagrant deployment</title>
   <link href="http://0.0.0.0:4000/2014/12/28/getting-packer-installed-on-OS-X/"/>
   <updated>2014-12-28T00:00:00+00:00</updated>
   <id>http://0.0.0.0:4000/2014/12/28/getting-packer-installed-on-OS-X</id>
   <content type="html">&lt;p&gt;I&amp;#39;ve been recently working on using packer to create vagrant .box files rather than manually creating them as I documented in a previous post. For this post I will be using fusion and the vagrant vmware provider, each of which have an associated cost, but I will cover a free alternative using packer and CentOS in a future post.&lt;/p&gt;

&lt;p&gt;Several github projects by gosddc have helped me in getting packer up and running on my Macbook:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/gosddc/packer-post-processor-vagrant-vmware-ovf&quot;&gt;gosddc/packer-post-processor-vagrant-vmware-ovf&lt;/a&gt;.
This repo contains a packer post processor that leverages VMware OVF Tool to create a vmware_ovf Vagrant box that is compatible with vagrant-vcloud, vagrant-vcenter and vagrant-vcloudair vagrant providers. It is only compatible with the packer VMware builder.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/gosddc/packer-templates&quot;&gt;gosddc/packer-templates&lt;/a&gt;.
This repo contains Packer templates for boxes available at https://vagrantcloud.com/gosddc, they only work with VMware and require the packer-post-processor-vagrant-vmware-ovf post-processor to work. These templates are a good starting point for generating pakcer templates on VMware products.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;1. Let&amp;#39;s get started by installing packer.&lt;/h2&gt;

&lt;h4&gt;A.  Download packer from the following link:&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.packer.io/downloads.html&quot;&gt;https://www.packer.io/downloads.html&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;B.  Unzip the downloaded files to /usr/local/packer_7.5:&lt;/h4&gt;

&lt;h4&gt;C.  Add /usr/local/packer_7.5 to your path by running the following command.&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;sdorsett-mbp:~ sdorsett$ export PATH=&amp;quot;/usr/local/packer_7.5:$PATH&amp;quot;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;D. Add the export command we just ran into ~/.bash_profile to ensure this path change persists after reboots.&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;sdorsett-mbp:~ sdorsett$ cat ~/.bash_profile
export PATH=&amp;quot;/usr/local/packer_7.5:$PATH&amp;quot;
sdorsett-mbp:~ sdorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;2. Next we need to download and install VMware ovftool, since the vagrant-vmware-ovf requires it.&lt;/h2&gt;

&lt;h4&gt;A. Download and install the latest version of the VMware OVF tool. VMware-ovftool-3.5.0-1274719-mac.x64.dmg is what I used.&lt;/h4&gt;

&lt;h4&gt;B. Verify ovftool is succesfully added to your path by running &amp;quot;ovftool -v&amp;quot;. This command should output the version of ovftool we installed.&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;sdorsett-mbp:~ sdorsett$ ovftool -v
VMware ovftool 3.5.0 (build-1274719)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;3. Now we can download and install the gosddc packer components we will need.&lt;/h2&gt;

&lt;h4&gt;A. Download the most recent version of the compiled packer-processor-vagrant-vmware-ovf binary from the following link:&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/gosddc/packer-post-processor-vagrant-vmware-ovf/releases&quot;&gt;https://github.com/gosddc/packer-post-processor-vagrant-vmware-ovf/releases&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;B. Unzip packer-post-processor-vagrant-vmware-ovf and copy it to &amp;quot;usr/local/packer_7.5&amp;quot;. Ensure the permissions of this file match the other files in this directory.&lt;/h4&gt;

&lt;h4&gt;C. Create a directory to contain the packer templates:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;sdorsett-mbp:~ sdorsett$ mkdir ~/Documents/packer
sdorsett-mbp:~ sdorsett$ cd ~/Documents/packer/&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h5&gt;D. Clone the gosddc packer-templates repository:&lt;/h5&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;sdorsett-mbp:packer sdorsett$ git clone https://github.com/gosddc/packer-templates.git
sdorsett-mbp:packer sdorsett$ cd packer-templates&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt;4. Now we need to download the ESXi 5.5 .iso and copy it into the proper directory location.&lt;/h2&gt;

&lt;h4&gt;A. create an &amp;quot;iso&amp;quot; directory for storing the ESXi iso files:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;sdorsett-mbp:packer-templates sdorsett$ mkdir ~/Documents/packer/packer-templates/iso&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;B. Download and copy the &amp;quot;VMware-VMvisor-Installer-5.5.0-1331820.x86_64.iso&amp;quot; ESXi 5.5 installer to the iso directory.&lt;/h4&gt;

&lt;h2&gt;5. Finally we will need to modify, validate and build the packer esxi.json packer template we will be using.&lt;/h2&gt;

&lt;h4&gt;A. modify ~/Documents/packer/packer-templates/templates/esxi.json to look like the following:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;sdorsett-mbp:packer-templates sdorsett$ cat templates/esxi.json
{
  &amp;quot;variables&amp;quot;: {
    &amp;quot;version&amp;quot;: &amp;quot;1.0&amp;quot;
  },
  &amp;quot;builders&amp;quot;: [
    {
      &amp;quot;name&amp;quot;: &amp;quot;esxi55&amp;quot;,
      &amp;quot;vm_name&amp;quot;: &amp;quot;esxi55&amp;quot;,
      &amp;quot;vmdk_name&amp;quot;: &amp;quot;esxi55-disk0&amp;quot;,
      &amp;quot;type&amp;quot;: &amp;quot;vmware-iso&amp;quot;,
      &amp;quot;headless&amp;quot;: true,
      &amp;quot;disk_size&amp;quot;: 4096,
      &amp;quot;guest_os_type&amp;quot;: &amp;quot;vmkernel5&amp;quot;,
      &amp;quot;iso_url&amp;quot;: &amp;quot;./iso/VMware-VMvisor-Installer-5.5.0-1331820.x86_64.iso&amp;quot;,
      &amp;quot;iso_checksum&amp;quot;: &amp;quot;ef599dc7e647177027684c0eee346ccdbc8704f2&amp;quot;,
      &amp;quot;iso_checksum_type&amp;quot;: &amp;quot;sha1&amp;quot;,
      &amp;quot;ssh_username&amp;quot;: &amp;quot;root&amp;quot;,
      &amp;quot;ssh_password&amp;quot;: &amp;quot;vagrant&amp;quot;,
      &amp;quot;ssh_wait_timeout&amp;quot;: &amp;quot;60m&amp;quot;,
      &amp;quot;shutdown_command&amp;quot;: &amp;quot;esxcli system maintenanceMode set -e true -t 0 ; esxcli system shutdown poweroff -d 10 -r &amp;#39;Packer Shutdown&amp;#39; ; esxcli system maintenanceMode set -e false -t 0&amp;quot;,
      &amp;quot;http_directory&amp;quot;: &amp;quot;.&amp;quot;,
      &amp;quot;boot_wait&amp;quot;: &amp;quot;5s&amp;quot;,
      &amp;quot;vmx_data&amp;quot;: {
        &amp;quot;memsize&amp;quot;: &amp;quot;4096&amp;quot;,
        &amp;quot;numvcpus&amp;quot;: &amp;quot;2&amp;quot;,
        &amp;quot;vhv.enable&amp;quot;: &amp;quot;TRUE&amp;quot;
      },
      &amp;quot;boot_command&amp;quot;: [
        &amp;quot;&amp;lt;enter&amp;gt;&amp;lt;wait&amp;gt;O&amp;lt;wait&amp;gt; ks=http://{{ .HTTPIP }}:{{ .HTTPPort }}/scripts/esxi-5-kickstart.cfg&amp;lt;enter&amp;gt;&amp;quot;
      ]
    }
  ],
  &amp;quot;provisioners&amp;quot;: [
    {
      &amp;quot;type&amp;quot;: &amp;quot;file&amp;quot;,
      &amp;quot;source&amp;quot;: &amp;quot;puppet/modules/vagrantbaseconfig/files/vagrant.pub&amp;quot;,
      &amp;quot;destination&amp;quot;: &amp;quot;/etc/ssh/keys-root/authorized_keys&amp;quot;
    },
    {
      &amp;quot;type&amp;quot;: &amp;quot;shell&amp;quot;,
      &amp;quot;script&amp;quot;: &amp;quot;scripts/esxi-vmware-tools_install.sh&amp;quot;
    },
    {
      &amp;quot;type&amp;quot;: &amp;quot;shell&amp;quot;,
      &amp;quot;script&amp;quot;: &amp;quot;scripts/esxi-cloning_configuration.sh&amp;quot;
    }
  ],
  &amp;quot;post-processors&amp;quot;: [
   {
     &amp;quot;type&amp;quot;: &amp;quot;vagrant-vmware-ovf&amp;quot;,
     &amp;quot;compression_level&amp;quot;: 9,
     &amp;quot;output&amp;quot;: &amp;quot;{{.BuildName}}-{{.Provider}}-{{user `version`}}.box&amp;quot;

   }
  ]
}&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;There are several things I would like to point out in the esxi.json file we just created.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;builder - this section specifies that we will be using the &amp;quot;vmware-iso&amp;quot; builder, with VMware fusion, to create our packer template. We can modify attributes of our template virtual machine in this section:

&lt;ul&gt;
&lt;li&gt;disk size ( disk_size)&lt;/li&gt;
&lt;li&gt;memory ( memsize )&lt;/li&gt;
&lt;li&gt;vCPU count ( numvcpus )&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;provisioners - this section specifies we will be using multiple provisioners to modify our template after it has been created:

&lt;ul&gt;
&lt;li&gt;a file provisioner that will copy the vagrant public ssh key into our ESXi template&lt;/li&gt;
&lt;li&gt;a shell script to install the vmware tools VIB for nested ESXi&lt;/li&gt;
&lt;li&gt;a shell script to make necessary MAC address changes in our nested ESXi template.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;post-processors - this final section will convert the virtual machine artifact generated in VMware fusion into a vmware-ovf .box file we can use with vagrant&lt;/li&gt;
&lt;/ol&gt;

&lt;h4&gt;B. Validate the packer esxi.json file is ready for building by running the following command:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;sdorsett-mbp:packer-templates sdorsett$ packer validate templates/esxi.json
Template validated successfully.&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;C. Start the build by running:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;sdorsett-mbp:packer-templates sdorsett$ packer build templates/esxi.json
esxi55 output will be in this color.

==&amp;gt; esxi55: Downloading or copying ISO
    esxi55: Downloading or copying: file:///Users/sdorsett/Documents/packer/packer-templates/iso/VMware-VMvisor-Installer-5.5.0-1331820.x86_64.iso
==&amp;gt; esxi55: Creating virtual machine disk
==&amp;gt; esxi55: Building and writing VMX file
==&amp;gt; esxi55: Starting HTTP server on port 8351
==&amp;gt; esxi55: Starting virtual machine...
    esxi55: The VM will be run headless, without a GUI. If you want to
    esxi55: view the screen of the VM, connect via VNC without a password to
    esxi55: 127.0.0.1:5986
==&amp;gt; esxi55: Waiting 5s for boot...
==&amp;gt; esxi55: Connecting to VM via VNC
==&amp;gt; esxi55: Typing the boot command over VNC...
==&amp;gt; esxi55: Waiting for SSH to become available...
==&amp;gt; esxi55: Connected to SSH!
==&amp;gt; esxi55: Uploading puppet/modules/vagrantbaseconfig/files/vagrant.pub =&amp;gt; /etc/ssh/keys-root/authorized_keys
==&amp;gt; esxi55: Provisioning with shell script: scripts/esxi-vmware-tools_install.sh
    esxi55: Installation Result
    esxi55: Message: The update completed successfully, but the system needs to be rebooted for the changes to be effective.
    esxi55: Reboot Required: true
    esxi55: VIBs Installed: VMware_bootbank_esx-tools-for-esxi_9.7.0-0.0.00000
    esxi55: VIBs Removed:
    esxi55: VIBs Skipped:
==&amp;gt; esxi55: Provisioning with shell script: scripts/esxi-cloning_configuration.sh
    esxi55: diff: can&amp;#39;t stat &amp;#39;/tmp/auto-backup.35216//etc/ssh/keys-root/authorized_keys&amp;#39;: No such file or directory
    esxi55: Saving current state in /bootbank
    esxi55: Clock updated.
    esxi55: Time: 02:09:44   Date: 12/30/2014   UTC
==&amp;gt; esxi55: Gracefully halting virtual machine...
    esxi55: Waiting for VMware to clean up after itself...
==&amp;gt; esxi55: Deleting unnecessary VMware files...
    esxi55: Deleting: output-esxi55/564d2ab2-395b-a9ba-9c17-2fe36682237c.vmem
    esxi55: Deleting: output-esxi55/esxi55.plist
    esxi55: Deleting: output-esxi55/vmware.log
==&amp;gt; esxi55: Cleaning VMX prior to finishing up...
    esxi55: Unmounting floppy from VMX...
    esxi55: Detaching ISO from CD-ROM device...
==&amp;gt; esxi55: Compacting the disk image
==&amp;gt; esxi55: Running post-processor: vagrant-vmware-ovf
==&amp;gt; esxi55 (vagrant-vmware-ovf): Creating Vagrant box for &amp;#39;vmware_ovf&amp;#39; provider
    esxi55 (vagrant-vmware-ovf): Deleting key: ide1:0.filename
    esxi55 (vagrant-vmware-ovf): Deleting key: floppy0.present
    esxi55 (vagrant-vmware-ovf): Setting key: floppy0.present = FALSE
    esxi55 (vagrant-vmware-ovf): Setting key: ide1:0.present = FALSE
    esxi55 (vagrant-vmware-ovf): Creating directory: output-esxi55/ovf
    esxi55 (vagrant-vmware-ovf): Starting ovftool
    esxi55 (vagrant-vmware-ovf): Reading files in output-esxi55/ovf
    esxi55 (vagrant-vmware-ovf): Copying: esxi55-disk1.vmdk
    esxi55 (vagrant-vmware-ovf): Copying: esxi55.mf
    esxi55 (vagrant-vmware-ovf): Copying: esxi55.ovf
    esxi55 (vagrant-vmware-ovf): Compressing: Vagrantfile
    esxi55 (vagrant-vmware-ovf): Compressing: esxi55-disk1.vmdk
    esxi55 (vagrant-vmware-ovf): Compressing: esxi55.mf
    esxi55 (vagrant-vmware-ovf): Compressing: esxi55.ovf
    esxi55 (vagrant-vmware-ovf): Compressing: metadata.json
Build &amp;#39;esxi55&amp;#39; finished.

==&amp;gt; Builds finished. The artifacts of successful builds are:
--&amp;gt; esxi55: &amp;#39;vmware_ovf&amp;#39; provider box: esxi55-vmware_ovf-1.0.box
sdorsett-mbp:packer-templates sdorsett$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If you want to keep an eye on the build process you can connect a VNC client to the address and port listed during the packer build process. For my build this was what was displayed:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;esxi55: The VM will be run headless, without a GUI. If you want to
    esxi55: view the screen of the VM, connect via VNC without a password to
    esxi55: 127.0.0.1:5986&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;When I connected the &amp;quot;Chicken of the VNC&amp;quot; client installed on my macbook to &amp;quot;127.0.0.1:5986&amp;quot; I could see where the build was at during the install process:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/01-esxi-vnc-client.png&quot; alt=&quot;screenshot&quot;&gt;  &lt;/p&gt;

&lt;h4&gt;D. Once the packer build completes you should end up with a &amp;quot;esxi55-vmware_ovf-1.0.box&amp;quot; file in the same directory you ran the &amp;quot;packer build&amp;quot; command in. This .box file can be used with vagrant and the gosddc vagrant providers to deploy this template to ESXi, vCenter, vCloud Director and vCloud Air.&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;sdorsett-mbp:packer-templates sdorsett$ ls *.box
esxi55-vmware_ovf-1.0.box&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3&gt;Hopefully you found this post helpful in getting packer and the packer vagrant-vmware-ovf post processor installed/configured. The next blog post will be covering how to install packer on CentOS and build a packer virtual machine on a remote ESXi host.&lt;/h3&gt;

&lt;h3&gt;Please provide any feedback or suggestions to my twitter account located on the about page.&lt;/h3&gt;
</content>
 </entry>
 
 <entry>
   <title>Installing Vagrant and the vagrant-vcloud plugin on CentOS 6.x</title>
   <link href="http://0.0.0.0:4000/2014/05/26/vagrant-install/"/>
   <updated>2014-05-26T00:00:00+00:00</updated>
   <id>http://0.0.0.0:4000/2014/05/26/vagrant-install</id>
   <content type="html">&lt;p&gt;In this post I&amp;#39;m setting out to explain how to create a CentOS 6.4 vm, from template, in vCHS (or a vCloud Director instance) and then install vagrant-vcloud on that. &lt;/p&gt;

&lt;h3&gt;1. Create a CentOS 6.x minimal virtual machine from a template in a vCHS organization.&lt;/h3&gt;

&lt;p&gt;I will demonstrate creating a new CentOS vm in vCHS using a template, but you could just as easily create a new CentOS virtual machine from scratch in vCloud Director.&lt;/p&gt;

&lt;h4&gt;A. Log into vCHS using your credentials and click on your virtual datacenter.&lt;/h4&gt;

&lt;h4&gt;B. Next click on the &amp;quot;Virtual Machines&amp;quot; tab and then click the &amp;quot;Deploy a Virtual Machine&amp;quot; button:&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/01-create-centos-64-vm.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;h4&gt;C. Select the &amp;quot;CentOS 6.4 64 Bit&amp;quot; template and click &amp;quot;Continue&amp;quot;&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/02-select-centos-64-template.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;h4&gt;D. Name the virtual machine and set the guest OS name of your new virtual machine. Click &amp;quot;Deploy This Virtual Machine&amp;quot;:&lt;/h4&gt;

&lt;p&gt;I named my virtual machine and set the guest OS name both to &amp;quot;vagrant&amp;quot;, but you can change these to what ever you prefer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/03-virtual-machine-resource-settings.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;h4&gt;E. Power on and then click the name of the virtual machine you just created:&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/04-centos-vm-created.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;h4&gt;F. Click the &amp;quot;Launch Console&amp;quot; link to open the console of the virtual machine you just created.&lt;/h4&gt;

&lt;p&gt;Notice that the guest OS password created by guest customization is displayed on the left. Log into the virtual machine, using &amp;quot;root&amp;quot; and the displayed password, then change this password immediately to something else:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/05-open-console-and-log-in.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;h3&gt;2. Modify /etc/resolv.conf&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# cat /etc/resolv.conf
nameserver 8.8.8.8 
nameserver 8.8.4.4&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;At this point you should have network connectivity to your new vagrant vapp. While I won&amp;#39;t cover it in the post, you can now either setup a DNAT rule to forward SSH to the vagrant vapp or continue the configuration using the vapp console.&lt;/p&gt;

&lt;h3&gt;3. Install Ruby 1.9.3 using ruby-build:&lt;/h3&gt;

&lt;h4&gt;A. Use yum to install the packages we&amp;#39;ll need to get ruby, rubygems and vagrant installed:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# yum install -y gcc-c++ glibc-headers openssl-devel readline libyaml-devel readline-devel zlib zlib-devel iconv-devel libxml2 libxml2-devel libxslt libxslt-devel wget git&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;B. Pull down the latest version of ruby-build using git:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# git clone https://github.com/sstephenson/ruby-build.git&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;C. Change to the directory the previous command created:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# cd ruby-build/&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;D. Run the &amp;#39;install.sh&amp;#39; script to install ruby-build:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ruby-build]# ./install.sh&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;E. I would recommend installing Ruby version 1.9.3-p545 with the following command:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ruby-build]# ruby-build --definitionsruby-build 1.9.3-p545 /usr/local/
Downloading yaml-0.1.6.tar.gz...
-&amp;gt; http://dqw8nmjcqpjn7.cloudfront.net/5fe00cda18ca5daeb43762b80c38e06e
Installing yaml-0.1.6...
Installed yaml-0.1.6 to /usr/local/

Downloading ruby-1.9.3-p545.tar.gz...
-&amp;gt; http://dqw8nmjcqpjn7.cloudfront.net/8e8f6e4d7d0bb54e0edf8d9c4120f40c
Installing ruby-1.9.3-p545...

Installed ruby-1.9.3-p545 to /usr/local/&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3&gt;4. Install rubygems by downloading the latest version from &lt;a href=&quot;https://rubygems.org/pages/download&quot;&gt;rubygems.org&lt;/a&gt; and install it:&lt;/h3&gt;

&lt;h4&gt;A. Use wget to download the latest version listed on the rubygems download page:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ruby-build]# cd ~/
[root@vagrant ~]# wget http://production.cf.rubygems.org/rubygems/rubygems-2.2.2.tgz&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;B. Use tar to unzip the tar-gzipped file we downloaded:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# tar xvzf rubygems-2.2.2.tgz
[root@vagrant ~]# cd rubygems-2.2.2&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;C. Install rubygems by running the &amp;#39;setup.rb&amp;#39; script:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant rubygems-2.2.2]# ruby setup.rb&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3&gt;5. Install Vagrant&lt;/h3&gt;

&lt;h4&gt;A. Use wget to download the latest version listed on the &lt;a href=&quot;https://www.vagrantup.com/downloads.html&quot;&gt;Vagrant download&lt;/a&gt; page:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant rubygems-2.2.2]# cd ~/
[root@vagrant ~]# wget https://dl.bintray.com/mitchellh/vagrant/vagrant_1.6.2_x86_64.rpm&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;B. Use the &amp;#39;rpm&amp;#39; command to install the RPM package we downloaded in the previous step:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# rpm -i vagrant_1.6.2_x86_64.rpm&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;C. Export the directory that the vagrant executable was install to, in order to make it easier to run:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# export PATH=$PATH:/opt/vagrant/bin/&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3&gt;6. Installing the vagrant-vcloud plugin&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant blog]# vagrant plugin install vagrant-vcloud
Installing the &amp;#39;vagrant-vcloud&amp;#39; plugin. This can take a few minutes...
Installed the plugin &amp;#39;vagrant-vcloud (0.3.3)&amp;#39;!&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3&gt;7. Creating a Vagrantfile for testing vagrant-vcloud install&lt;/h3&gt;

&lt;h4&gt;A.  We need to create a directory structure and Vagrantfile to test that the vagrant-vcloud provider is properly working. For the .box file we will use a sample precise32 (Ubuntu 12.04 32bit) vagrant .box file created for the vcloud provider by &lt;a href=&quot;http://blog.tsugliani.fr/&quot;&gt;Timo Sugliani&lt;/a&gt;:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# mkdir -p ~/vagrant-vms/precise32-test
[root@vagrant ~]# cat ~/vagrant-vms/precise32-test/Vagrantfile 
precise32_vm_box_url = &amp;quot;http://vagrant.tsugliani.fr/precise32.box&amp;quot;

nodes = [
  { :hostname =&amp;gt; &amp;quot;test-vm&amp;quot;,  
    :box =&amp;gt; &amp;quot;precise32&amp;quot;, 
    :box_url =&amp;gt; precise32_vm_box_url }
]

Vagrant.configure(&amp;quot;2&amp;quot;) do |config|

  # vCloud Director provider settings
  config.vm.provider :vcloud do |vcloud|

    vcloud.hostname = &amp;#39;https://[Your_vCD_URL]:443&amp;#39;
    vcloud.username = &amp;#39;[Your_vCHS_username@somedomain.com]&amp;#39;
    vcloud.password = &amp;#39;[Your_vCHS_password]&amp;#39;

    vcloud.org_name = &amp;#39;[Your_vCHS_org_name]&amp;#39;
    vcloud.vdc_name = &amp;#39;[Your_vCHS_vdc_name]&amp;#39;
    vcloud.catalog_name = &amp;#39;[Your_vCHS_org_catalog]&amp;#39;
    vcloud.network_bridge = true
    vcloud.vdc_network_name = &amp;#39;[Your_vCHS_vdc_name]-default-routed&amp;#39;

  end

  nodes.each do |node|
    config.vm.define node[:hostname] do |node_config|
      node_config.vm.box = node[:box]
      node_config.vm.hostname = node[:hostname]
      node_config.vm.box_url = node[:box_url]
    end
  end
end&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In this configuration file:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&amp;quot;vcloud.hostname&amp;quot; is your vCloud Director FQDN or IP address. This should be the base URL and not by the specific URL for your org.&lt;/li&gt;
&lt;li&gt;&amp;quot;vcloud.username&amp;quot; is the username we will be using to connect to vCHS or vCloud Director&lt;/li&gt;
&lt;li&gt;&amp;quot;vcloud.password&amp;quot; is the password we will be using to connect to vCHS or vCloud Director&lt;/li&gt;
&lt;li&gt;&amp;quot;vcloud.org_name&amp;quot; will be the name of your vCHS or vCloud Director organization&lt;/li&gt;
&lt;li&gt;&amp;quot;vcloud.vdc_name&amp;quot; will be the name of your vCHS or vCloud Director virtual datacenter&lt;/li&gt;
&lt;li&gt;&amp;quot;vcloud.catalog_name&amp;quot; will be the name of your organization catalog a template derived from the .box file will be created in&lt;/li&gt;
&lt;li&gt;&amp;quot;vcloud.dc_network_name&amp;quot; is the organization network the vagrant vapp will be connected to&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;B.  Now we can change directory to that directory and test everything with a &amp;quot;vagrant up --provider=vcloud&amp;quot;:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# cd ~/vagrant-vms/precise32-test
oot@vagrant precise32-test]# vagrant up --provider=vcloud
Bringing machine &amp;#39;test-vm&amp;#39; up with &amp;#39;vcloud&amp;#39; provider...
==&amp;gt; test-vm: Box &amp;#39;precise32&amp;#39; could not be found. Attempting to find and install...
    test-vm: Box Provider: vcloud
    test-vm: Box Version: &amp;gt;= 0
==&amp;gt; test-vm: Adding box &amp;#39;precise32&amp;#39; (v0) for provider: vcloud
    test-vm: Downloading: http://vagrant.tsugliani.fr/precise32.box
==&amp;gt; test-vm: Successfully added box &amp;#39;precise32&amp;#39; (v0) for &amp;#39;vcloud&amp;#39;!
==&amp;gt; test-vm: Catalog item [precise32] in Catalog [Org Private Catalog] does not exist!
    test-vm: Would you like to upload the [precise32] box to [Org Private Catalog] Catalog?
    test-vm: Choice (yes/no): yes
==&amp;gt; test-vm: Uploading [precise32]...
Uploading Box...
Uploading Box...
Uploading Box... Progress: 22%  ETA: 00:02:16&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Once the .box files has started uploading, you can check the progress in the org catalog of your vCloud Director instance:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/06-precise32-upload.png&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;h4&gt;C. The uploading progress will get to 100% and you will see the vApp getting created, powered on and SSH access achieved:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;Uploading Box...
Uploading Box... Progress: 100% Time: 00:02:32                                                                                                   
==&amp;gt; test-vm: Adding [precise32] to Catalog [Org Private Catalog]
==&amp;gt; test-vm: Building vApp...
==&amp;gt; test-vm: vApp Vagrant-root-vagrant-8b9115d1 successfully created.
==&amp;gt; test-vm: Powering on VM...
==&amp;gt; test-vm: Waiting for SSH Access on 192.168.109.4:22 ... 
==&amp;gt; test-vm: Waiting for SSH Access on 192.168.109.4:22 ... 
==&amp;gt; test-vm: Waiting for SSH Access on 192.168.109.4:22 ... 
==&amp;gt; test-vm: Rsyncing folder: /root/vagrant-vms/precise32-test/ =&amp;gt; /vagrant
[root@vagrant precise32-test]# root@vagrant precise32-test]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;D. Test network connectivity by running &amp;quot;vagrant ssh&amp;quot;:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;root@vagrant precise32-test]# vagrant ssh
==&amp;gt; test-vm: External IP for test-vm: 192.168.109.4
Welcome to Ubuntu 12.04 LTS (GNU/Linux 3.2.0-23-generic-pae i686)

 * Documentation:  https://help.ubuntu.com/
Welcome to your Vagrant-built virtual machine.
Last login: Thu Jul 18 11:20:25 2013
vagrant@test-vm:~$ hostname -f
test-vm&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;E. Run &amp;quot;vagrant vcloud --status&amp;quot; to validate the vCloud Director status of your vagrant vapp:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;root@vagrant precise32-test]# vagrant vcloud --status
Initializing vCloud Director provider...
Fetching vCloud Director status...
+-------------------------------+-----------------------------------------+
|  Vagrant vCloud Director Status : https://[Your_vCD_URL]:443            |
+-------------------------------+-----------------------------------------+
| Organization Name             | [Your_vCHS_org_name]                    |
| Organization vDC Name         | vCHS-vagrant-demo                       |
| Organization vDC ID           | b9494d6a-ebda-4368-8f76-0e93b7edcb17    |
| Organization vDC Network Name | [Your_vCHS_vdc_name]-default-routed     |
+-------------------------------+-----------------------------------------+
| vApp Name                     | Vagrant-root-vagrant-8b9115d1           |
| vAppID                        | ec918e84-6026-42b2-8639-5390d8c88b0c    |
| -&amp;gt; test-vm                    | 4fc82b9b-c79d-483b-9a71-a87d22289102    |
+-------------------------------+-----------------------------------------+&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;F. Run &amp;quot;vagrant vcloud --network&amp;quot; to validate the vCloud Director network mapping of your vagrant vapp:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant precise32-test]# vagrant vcloud --network
Initializing vCloud Director provider...
Fetching vCloud Director network settings ...
+---------+---------------+------------+
|             Network Map              |
+---------+---------------+------------+
| VM Name | IP Address    | Connection |
+---------+---------------+------------+
| test-vm | 192.168.109.4 | Direct     |
+---------+---------------+------------+&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;g. Destroy the cloned vm by running &amp;quot;vagrant destroy&amp;quot; command:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant precise32-test]# vagrant destroy
    test-vm: Are you sure you want to destroy the &amp;#39;test-vm&amp;#39; VM? [y/N] y
==&amp;gt; test-vm: Powering off VM...
==&amp;gt; test-vm: Single VM left in the vApp, Powering off vApp...
==&amp;gt; test-vm: Destroying vApp...
[root@vagrant precise32-test]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You should see &amp;quot;Power Off vapp&amp;quot; and &amp;quot;Delete vapp&amp;quot; tasks being run and then completing on your organizations vapps.&lt;/p&gt;

&lt;h3&gt;Hopefully you found this post helpful in getting vagrant and the vagrant-cloud plugin installed and configured. The next blog post will be covering how to modify the Vagrantfile to create a vapp in a vCHS or vCloud Director instance that doesn&amp;#39;t contain the vagrant vm.&lt;/h3&gt;

&lt;h3&gt;Please provide any feedback or suggestions to my twitter account located on the about page.&lt;/h3&gt;
</content>
 </entry>
 
 <entry>
   <title>Introducing the vagrant-vcloud provider</title>
   <link href="http://0.0.0.0:4000/2014/05/25/vagrant-vcloud/"/>
   <updated>2014-05-25T00:00:00+00:00</updated>
   <id>http://0.0.0.0:4000/2014/05/25/vagrant-vcloud</id>
   <content type="html">&lt;p&gt;While continuing to explore what the vagrant-vsphere provider is capable of I came across the &lt;a href=&quot;https://github.com/frapposelli/vagrant-vcloud&quot;&gt;vagrant-vcloud&lt;/a&gt; provider, which had recently released a new version. I work for the vCHS operations group, so I figured it would be interesting to compare the feature differences of the vsphere &amp;amp; vcloud providers. &lt;/p&gt;

&lt;p&gt;Over the next few blog posts I intend to cover the following vagrant-vcloud provider related topics:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://sdorsett.github.io/2014/05/26/vagrant-install/&quot;&gt;Installing Vagant and the vagrant-vcloud plugin on a CentOS 6.x virtual machine in vCloud Directory or vCHS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Creating a simple vagrant-vcloud Vagrantfile configuration to deploy a vm &lt;/li&gt;
&lt;li&gt;Creating a more advanced vagrant-vcloud Vagrantfile configuration&lt;/li&gt;
&lt;li&gt;Creating a CentOS 6.x .box that is customized for Vagrant and vcloud director&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These posts will hopefully explain the steps necessary to get vagrant-vcloud installed and working with a vCloud Director or vCHS environment.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Creating a Puppet manifest and integrating it with Vagrant</title>
   <link href="http://0.0.0.0:4000/2014/05/06/vagrant-and-puppet/"/>
   <updated>2014-05-06T00:00:00+00:00</updated>
   <id>http://0.0.0.0:4000/2014/05/06/vagrant-and-puppet</id>
   <content type="html">&lt;p&gt;This post will cover configuring Vagrant to automatically run a Puppet manifest on the vm created by &amp;quot;vagrant up.&amp;quot; This capability allows you to test your Puppet manifests, make changes and test again, all quickly and easily. Let&amp;#39;s get started:&lt;/p&gt;

&lt;h3&gt;1. Create the Puppet manifest &amp;amp; modules we will be using for our Vagrant tests.&lt;/h3&gt;

&lt;p&gt;For testing purposes we will be creating a Puppet manifest that ensures NTP is installed and is configured to use the following NTP servers:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;0.pool.ntp.org&lt;/li&gt;
&lt;li&gt;1.pool.ntp.org&lt;/li&gt;
&lt;li&gt;2.pool.ntp.org&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;A. First we should install tree, since it will help us visualize the directory structure of the puppet manifest directories we will be creating:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vagrant-vms]# yum install -y tree
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
 * base: mirrors.centarra.com
 * epel: mirror.unl.edu
 * extras: mirrors.finalasp.com
 * updates: mirrors.centarra.com
Setting up Install Process
Resolving Dependencies
--&amp;gt; Running transaction check
---&amp;gt; Package tree.x86_64 0:1.5.3-2.el6 will be installed
--&amp;gt; Finished Dependency Resolution

Dependencies Resolved

====================================================================
 Package      Arch           Version             Repository    Size
====================================================================
Installing:
 tree         x86_64         1.5.3-2.el6         base          36 k

Transaction Summary
====================================================================
Install       1 Package(s)

Total download size: 36 k
Installed size: 65 k
Downloading Packages:
tree-1.5.3-2.el6.x86_64.rpm                    |  36 kB     00:00     
Running rpm_check_debug
Running Transaction Test
Transaction Test Succeeded
Running Transaction
  Installing : tree-1.5.3-2.el6.x86_64                          1/1 
  Verifying  : tree-1.5.3-2.el6.x86_64                          1/1 

Installed:
  tree.x86_64 0:1.5.3-2.el6                                           

Complete!
[root@vagrant vagrant-vms]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;B. Change to the directory that contains the Vagrantfile &amp;amp; example_box directory we&amp;#39;ve been working in over the last few blog posts:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# cd ~/vagrant-vms/
[root@vagrant vagrant-vms]# tree
.
├── example_box
│   └── dummy.box
└── Vagrantfile

1 directory, 2 files
[root@vagrant vagrant-vms]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;C. Create the following modules &amp;amp; manifests directory structure for our Puppet manifest files:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vagrant-vms]# mkdir -p {manifests,modules/ntp/manifests,modules/ntp/templates,modules/common/manifests}
[root@vagrant vagrant-vms]# tree
.
├── example_box
│   └── dummy.box
├── manifests
├── modules
│   ├── common
│   │   └── manifests
│   └── ntp
│       ├── manifests
│       └── templates
└── Vagrantfile

8 directories, 2 files
[root@vagrant vagrant-vms]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;D. Create manifests/site.pp with the following contents:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vagrant-vms]# vi manifests/site.pp
node default {
  include common
    class {&amp;#39;ntp&amp;#39;:}
}
[root@vagrant vagrant-vms]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This is the Puppet manifest that we will configure Vagrent to run automatically. The &amp;quot;node default&amp;quot; section will be applied by any hostname that runs this manifest. The &amp;quot;node default&amp;quot; section calls the common &amp;amp; ntp modules which we will create next.&lt;/p&gt;

&lt;h4&gt;E. Create modules/common/manifests/init.pp with the following contents:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vagrant-vms]# cat modules/common/manifests/init.pp
class common {
  include common::data
}

class common::data {
  $ntpServerList = [ &amp;#39;0.pool.ntp.org&amp;#39;,&amp;#39;1.pool.ntp.org&amp;#39;,&amp;#39;2.pool.ntp.org&amp;#39; ]
}
[root@vagrant vagrant-vms]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This manifest is what gets run when Puppets runs &amp;quot;include common&amp;quot; in the manifests/site.pp manifest. When this manifest is run it creates an array named &amp;quot;common::data::ntpServerList&amp;quot; containing the ntp servers we&amp;#39;re needing to have defined.&lt;/p&gt;

&lt;h4&gt;F. Create modules/ntp/manifests/init.pp with the following contents:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vagrant-vms]# cat modules/ntp/manifests/init.pp
class ntp( $ntpServerList = $common::data::ntpServerList) {
  package { &amp;#39;ntp&amp;#39;:
  ensure =&amp;gt; &amp;#39;present&amp;#39;,
  } #package

  file { &amp;#39;/etc/ntp.conf&amp;#39;:
    mode    =&amp;gt; &amp;quot;644&amp;quot;,
    content =&amp;gt; template(&amp;quot;ntp/client-ntp.conf.erb&amp;quot;),
    notify  =&amp;gt; Service[&amp;quot;ntpd&amp;quot;],
    require =&amp;gt; Package[&amp;quot;ntp&amp;quot;],
  } # file

  service { &amp;#39;ntpd&amp;#39;:
    ensure  =&amp;gt; running,
    enable  =&amp;gt; true,
    require =&amp;gt; Package[&amp;quot;ntp&amp;quot;],
  } # service
}
[root@vagrant vagrant-vms]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This manifest is what gets run when Puppet calls &amp;quot;class {&amp;#39;ntp&amp;#39;:}&amp;quot; in the manifests/site.pp manifest. When this manifest is run it will ensure the ntp package is installed, the file &amp;quot;/etc/ntp.conf&amp;quot; is created containing our list of ntp servers and the ntpd service is started.    &lt;/p&gt;

&lt;h4&gt;G. Create modules/ntp/templates/client-ntp.conf.erb with the following contents:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;# This file is being maintained by Puppet.
# DO NOT EDIT

# Permit time synchronization with our time source, but do not
# permit the source to query or modify the service on this system.
restrict default kod nomodify notrap nopeer noquery
restrict -6 default kod nomodify notrap nopeer noquery

# Permit all access over the loopback interface.  This could
# be tightened as well, but to do so would effect some of
# the administrative functions.
restrict 127.0.0.1
restrict -6 ::1

&amp;lt;% ntpServerList.each do |ntpServer| -%&amp;gt;
server &amp;lt;%= ntpServer %&amp;gt;
&amp;lt;% end -%&amp;gt;

# Drift file.  Put this in a directory which the daemon can write to.
# No symbolic links allowed, either, since the daemon updates the file
# by creating a temporary in the same directory and then rename()&amp;#39;ing
# it to the file.
driftfile /var/lib/ntp/drift
[root@vagrant vagrant-vms]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This .erb file is a ruby template describing what the file /etc/ntp.conf should contain. The magic of this is the section:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&amp;lt;% ntpServerList.each do |ntpServer| -%&amp;gt;
server &amp;lt;%= ntpServer %&amp;gt;
&amp;lt;% end -%&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This section is actually ruby code that runs a foreach loop on the array &amp;quot;ntpServerList&amp;quot; and adds a line for each ntp server contained in that array. &lt;/p&gt;

&lt;h4&gt;H. Run the tree command and you should see the following file/directory structure:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vagrant-vms]# tree
.
├── example_box
│   └── dummy.box
├── manifests
│   └── site.pp
├── modules
│   ├── common
│   │   └── manifests
│   │       └── init.pp
│   └── ntp
│       ├── manifests
│       │   └── init.pp
│       └── templates
│           └── client-ntp.conf.erb
└── Vagrantfile

8 directories, 6 files
[root@vagrant vagrant-vms]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3&gt;2. Modify the Vagrantfile we created in the last post to include the following:&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vagrant-vms]# cat Vagrantfile
Vagrant.configure(&amp;quot;2&amp;quot;) do |config|
  config.vm.box = &amp;#39;dummy&amp;#39;
  config.vm.box_url = &amp;#39;./example_box/dummy.box&amp;#39;

  config.vm.provider :vsphere do |vsphere|
    vsphere.host = &amp;#39;192.168.1.195&amp;#39;
    vsphere.name = &amp;#39;vagrant-test&amp;#39;
    vsphere.clone_from_vm = true
    vsphere.template_name = &amp;#39;vagrant-centos-6.5&amp;#39;
    vsphere.user = &amp;#39;root@localos&amp;#39;
    vsphere.password = &amp;#39;S0meR@nd0mP@ssw0rd&amp;#39;
    vsphere.insecure = true
    vsphere.data_store_name = &amp;#39;vsanDatastore&amp;#39;
    vsphere.linked_clone = true
    vsphere.customization_spec_name = &amp;#39;vagrant-centos&amp;#39;
  end
  config.vm.provision &amp;quot;puppet&amp;quot; do |puppet|
    puppet.manifests_path = &amp;quot;manifests&amp;quot;
    puppet.manifest_file = &amp;quot;site.pp&amp;quot;
    puppet.module_path = &amp;quot;modules&amp;quot;
  end
end
[root@vagrant vagrant-vms]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The following new lines have been added:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&amp;quot;puppet.manifests_path&amp;quot; specifies the sub-directory, from the directory containing the Vagrantfile, that contains the puppet manifests.&lt;/li&gt;
&lt;li&gt;&amp;quot;puppet.manifest_file&amp;quot; specifies the puppet manifest that will be initially run.&lt;/li&gt;
&lt;li&gt;&amp;quot;puppet.module_path&amp;quot; specifies the sub-directory, from the directory containing the Vagrantfile, that contains the puppet modules.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;3. Test the configuration changes we made by running a &amp;quot;vagrant up&amp;quot;:&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vagrant-vms]# vagrant up --provider=vsphere
Bringing machine &amp;#39;default&amp;#39; up with &amp;#39;vsphere&amp;#39; provider...
==&amp;gt; default: Calling vSphere CloneVM with the following settings:
==&amp;gt; default:  -- Source VM: vagrant-centos-6.5
==&amp;gt; default:  -- Name: vagrant-test
==&amp;gt; default: Waiting for SSH to become available...
==&amp;gt; default: New virtual machine successfully cloned and started
==&amp;gt; default: Rsyncing folder: /root/vagrant-vms/ =&amp;gt; /vagrant
==&amp;gt; default: Rsyncing folder: /root/vagrant-vms/manifests/ =&amp;gt; /tmp/vagrant-puppet-1/manifests
==&amp;gt; default: Rsyncing folder: /root/vagrant-vms/modules/ =&amp;gt; /tmp/vagrant-puppet-1/modules-0
==&amp;gt; default: Running provisioner: puppet...
Running Puppet with site.pp...
notice: /File[/etc/ntp.conf]/content: content changed &amp;#39;{md5}d7e1e16f9c0cd6382f6b68b486163db1&amp;#39; to &amp;#39;{md5}f7a83a4ca84e1ba2bba0166c5620e9e7&amp;#39;
notice: /Stage[main]/Ntp/Service[ntpd]: Triggered &amp;#39;refresh&amp;#39; from 1 events
notice: Finished catalog run in 0.72 seconds
[root@vagrant vagrant-vms]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You might notice a few new lines since the &amp;quot;vagrant up&amp;quot; we ran in the last post. &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The manifests &amp;amp; modules folders we specified in Vagrantfile have been copied over using rsync. &lt;/li&gt;
&lt;li&gt;The Puppet agent is running a &amp;quot;puppet apply&amp;quot; using the site.pp file we specified. &lt;/li&gt;
&lt;li&gt;/etc/ntp.conf was modified by Puppet&lt;/li&gt;
&lt;li&gt;The NTP service was notified that it needed to refresh it&amp;#39;s configuration since /etc/ntp.conf had been modified.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;4. Connect to the vagrant vm to validate that /etc/ntp.conf has been modified to include the three NTP servers we specified:&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vagrant-vms]# vagrant ssh
[vagrant@vagrant-test ~]$ cat /etc/ntp.conf
# This file is being maintained by Puppet.
# DO NOT EDIT

# Permit time synchronization with our time source, but do not
# permit the source to query or modify the service on this system.
restrict default kod nomodify notrap nopeer noquery
restrict -6 default kod nomodify notrap nopeer noquery

# Permit all access over the loopback interface.  This could
# be tightened as well, but to do so would effect some of
# the administrative functions.
restrict 127.0.0.1
restrict -6 ::1

server 0.pool.ntp.org iburst
server 1.pool.ntp.org iburst
server 2.pool.ntp.org iburst

# Drift file.  Put this in a directory which the daemon can write to.
# No symbolic links allowed, either, since the daemon updates the file
# by creating a temporary in the same directory and then rename()&amp;#39;ing
# it to the file.
driftfile /var/lib/ntp/drift
[vagrant@vagrant-test ~]$ exit
logout
Connection to 192.168.1.123 closed.
[root@vagrant vagrant-vms]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3&gt;5. Modify the site.pp file to add a node definition for our vagrant vm:&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vagrant-vms]# cat site.pp
node default {
  include common
    class {&amp;#39;ntp&amp;#39;:}
}
node &amp;#39;vagrant-test.mylab.net&amp;#39; {
  include common
    class {&amp;#39;ntp&amp;#39;:
      ntpServerList =&amp;gt; [&amp;#39;3.pool.ntp.org&amp;#39;,&amp;#39;4.pool.ntp.org&amp;#39;,&amp;#39;5.pool.ntp.org&amp;#39;]
    }
}&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The new &amp;quot;node &amp;#39;vagrant-test.mylab.net&amp;#39;&amp;quot; definition will only be run by hosts with a matching hostname. Any host that doesn&amp;#39;t have a matching host definition will continue to run the &amp;quot;node default&amp;quot; section. Node definitions allow you to change variables or even modules that will be run by specific hosts and provide flexibility in the changes made to a given host. Our new node section will configure &amp;quot;vagrant-test.mylab.net&amp;quot; to use a different set of ntp servers than any other host that runs this Puppet manifest  &lt;/p&gt;

&lt;h3&gt;6. Run &amp;#39;vagrant provision&amp;#39; to apply the updated Puppet manifest to our Vagrant vm:&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vagrant-vms]# vagrant provision
==&amp;gt; default: Rsyncing folder: /root/vagrant-vms/ =&amp;gt; /vagrant
==&amp;gt; default: Rsyncing folder: /root/vagrant-vms/manifests/ =&amp;gt; /tmp/vagrant-puppet-1/manifests
==&amp;gt; default: Rsyncing folder: /root/vagrant-vms/modules/ =&amp;gt; /tmp/vagrant-puppet-1/modules-0
==&amp;gt; default: Running provisioner: puppet...
Running Puppet with site.pp...
notice: /File[/etc/ntp.conf]/content: content changed &amp;#39;{md5}f7a83a4ca84e1ba2bba0166c5620e9e7&amp;#39; to &amp;#39;{md5}414e811fdbfa3f8cfa38e1e4e6d0586f&amp;#39;
notice: /Stage[main]/Ntp/Service[ntpd]: Triggered &amp;#39;refresh&amp;#39; from 1 events
notice: Finished catalog run in 0.34 seconds
[root@vagrant vagrant-vms]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;While we could have destroyed and recreated our Vagrant vm to test the change to the Puppet manifest, &amp;#39;vagrant provision&amp;#39; quickly resyncs the folders defined in our Vagrantfile and re-runs our Puppet manifest.&lt;/p&gt;

&lt;h3&gt;7. Connect to the vagrant vm to validate that /etc/ntp.conf has been modified to include the three NTP servers we defined fof this specific node:&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;root@vagrant vagrant-vms]# vagrant ssh
Last login: Tue May  6 15:09:50 2014 from 192.168.1.40
[vagrant@vagrant-test ~]$ cat /etc/ntp.conf
# This file is being maintained by Puppet.
# DO NOT EDIT

# Permit time synchronization with our time source, but do not
# permit the source to query or modify the service on this system.
restrict default kod nomodify notrap nopeer noquery
restrict -6 default kod nomodify notrap nopeer noquery

# Permit all access over the loopback interface.  This could
# be tightened as well, but to do so would effect some of
# the administrative functions.
restrict 127.0.0.1
restrict -6 ::1

server 3.pool.ntp.org iburst
server 4.pool.ntp.org iburst
server 5.pool.ntp.org iburst

# Drift file.  Put this in a directory which the daemon can write to.
# No symbolic links allowed, either, since the daemon updates the file
# by creating a temporary in the same directory and then rename()&amp;#39;ing
# it to the file.
driftfile /var/lib/ntp/drift
[vagrant@vagrant-test ~]$ exit
logout
Connection to 192.168.1.123 closed.
[root@vagrant vagrant-vms]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3&gt;8. Shutdown the vagrant machine:&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vagrant-vms]# vagrant destroy
==&amp;gt; default: Calling vSphere PowerOff
==&amp;gt; default: Calling vShpere Destroy
[root@vagrant vagrant-vms]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3&gt;Hopefully you found this post helpful in demonstrating how to use Puppet with Vagrant, as well as the benefits it provides for testing Puppet manfests and modules&lt;/h3&gt;

&lt;h3&gt;Please provide any feedback or suggestions to my twitter account located on the about page.&lt;/h3&gt;
</content>
 </entry>
 
 <entry>
   <title>Using advanced vagrant-vsphere provider settings and vCenter guest customization</title>
   <link href="http://0.0.0.0:4000/2014/04/24/vagrant-and-vcenter-guest-customization/"/>
   <updated>2014-04-24T00:00:00+00:00</updated>
   <id>http://0.0.0.0:4000/2014/04/24/vagrant-and-vcenter-guest-customization</id>
   <content type="html">&lt;p&gt;This post will pick up where we left off by demonstrating more vagrant-vsphere provider settings using the CentOS template we customized in the last blog post. Let&amp;#39;s get started:&lt;/p&gt;

&lt;h3&gt;1. Create a new customization specification in the vSphere web client. This customization specification will allow us to set the hostname of the vm created by &amp;quot;vagrant up&amp;quot; to the same name as the virtual machine.&lt;/h3&gt;

&lt;h4&gt;A. Go to Home | Customization Specification Manager:&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/01-web-client-home.jpg&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;h4&gt;B. Click the &amp;quot;Create a new specification&amp;quot; button:&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/02-create-guest-customization.jpg&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;h4&gt;C. Select &amp;quot;Linux&amp;quot; for the &amp;quot;Target VM Operating System&amp;quot; and name the customization specification. Click &amp;quot;Next&amp;quot; to continue&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/03-new-guest-customization.jpg&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;h4&gt;D. For &amp;quot;Computer Name&amp;quot; select &amp;quot;Use the virtual machine name&amp;quot; and click &amp;quot;Next&amp;quot; to continue.&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/04-set-computer-name.jpg&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;h4&gt;E. For &amp;quot;Time Zone&amp;quot; select the time zone you want to use and click &amp;quot;Next&amp;quot; to continue.&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/05-set-timezone.jpg&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;h4&gt;F. For &amp;quot;Configure Network&amp;quot; keep the default and click &amp;quot;Next to continue.&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/06-configure-network.jpg&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;h4&gt;G. For &amp;quot;Enter DNS and Domain settings&amp;quot; enter the DNS servers and the domain name you want to assign to the cloned vm. Click &amp;quot;Next to continue.&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/07-dns-settings.jpg&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;h4&gt;H. Click &amp;quot;Finish&amp;quot; to create the customization specification.&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/08-finish.jpg&quot; alt=&quot;screenshot&quot;&gt;&lt;/p&gt;

&lt;h3&gt;2. Modify the Vagrantfile we created in the initial &amp;quot;&lt;a href=&quot;http://sdorsett.github.io/2014/04/19/vagrant-install/&quot;&gt;Installing Vagrant and the vagrant-vsphere plugin on CentOS 6.x&lt;/a&gt;&amp;quot; post.&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vagrant-vms]# cat Vagrantfile
Vagrant.configure(&amp;quot;2&amp;quot;) do |config|
  config.vm.box = &amp;#39;dummy&amp;#39;
  config.vm.box_url = &amp;#39;./example_box/dummy.box&amp;#39;

  config.vm.provider :vsphere do |vsphere|
    vsphere.host = &amp;#39;192.168.1.195&amp;#39;
    vsphere.name = &amp;#39;vagrant-test&amp;#39;
    vsphere.clone_from_vm = true
    vsphere.template_name = &amp;#39;vagrant-centos-6.5&amp;#39;
    vsphere.user = &amp;#39;root@localos&amp;#39;
    vsphere.password = &amp;#39;S0meR@nd0mP@ssw0rd&amp;#39;
    vsphere.insecure = true
    vsphere.data_store_name = &amp;#39;vsanDatastore&amp;#39;
    vsphere.linked_clone = true
    vsphere.customization_spec_name = &amp;#39;vagrant-centos&amp;#39;
  end
end&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The following new lines have been added:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&amp;quot;vsphere.data_store_name&amp;quot; is the datastore that the cloned vm will be deployed to.&lt;/li&gt;
&lt;li&gt;&amp;quot;vsphere.linked_clone&amp;quot; being set to &amp;quot;true&amp;quot; will cause the cloned vm to be deployed as a linked clone. This will dramatically decrease the amount of time needed to clone the vm when &amp;quot;vagrant up&amp;quot; command is issued.&lt;/li&gt;
&lt;li&gt;&amp;quot;vsphere.customization_spec_name&amp;quot; will specify the customization specification that will be applied to the cloned vm. Set this to the customization specification we created in the previous step.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;3. Test the configuration changes we made by running a &amp;quot;vagrant up&amp;quot;:&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vagrant-vms]# vagrant up --provider=vsphere
Bringing machine &amp;#39;default&amp;#39; up with &amp;#39;vsphere&amp;#39; provider...
==&amp;gt; default: Calling vSphere CloneVM with the following settings:
==&amp;gt; default:  -- Source VM: vagrant-centos-6.5
==&amp;gt; default:  -- Name: vagrant-test
==&amp;gt; default: Waiting for SSH to become available...
==&amp;gt; default: New virtual machine successfully cloned and started
==&amp;gt; default: Rsyncing folder: /root/vagrant-vms/ =&amp;gt; /vagrant
[root@vagrant vagrant-vms]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You should find the time it took to deploy the linked clone vm was much faster than previous full clones of the vagrant template vm.&lt;/p&gt;

&lt;h3&gt;4. Connect to the vagrant vm to validate the hostname of the vm matched the vm name &amp;amp; search domain:&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vagrant-vms]# vagrant ssh
[vagrant@vagrant-test ~]$ hostname -f
vagrant-test.mylab.net
[vagrant@vagrant-test ~]$ exit
logout
Connection to 192.168.1.117 closed.
[root@vagrant vagrant-vms]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3&gt;5. Clean up after ourselves and call it a day:&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vagrant-vms]# vagrant destroy
==&amp;gt; default: Calling vSphere PowerOff
==&amp;gt; default: Calling vShpere Destroy
[root@vagrant vagrant-vms]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You might be wondering why setting the hostname &amp;amp; domain name might be so important. We will see in future posts how to use hostnames to determine which manifests will be applied to specific vms we clone and power up with Vagrant.&lt;/p&gt;

&lt;h3&gt;Hopefully you found this post helpful understanding more about the usefulness of customization specifications and advanced vagrant-vsphere configurations. The next blog post will start diving into using Vagrant to test Puppet manifests.&lt;/h3&gt;

&lt;h3&gt;Please provide any feedback or suggestions to my twitter account located on the about page.&lt;/h3&gt;
</content>
 </entry>
 
 <entry>
   <title>Creating a CentOS 6.x template that is customized for Vagrant</title>
   <link href="http://0.0.0.0:4000/2014/04/20/vagrant-boxes/"/>
   <updated>2014-04-20T00:00:00+00:00</updated>
   <id>http://0.0.0.0:4000/2014/04/20/vagrant-boxes</id>
   <content type="html">&lt;p&gt;This post will continue our examination of using the vagrant-vsphere plugin, by customizing a CentOS template for integration with Vagrant. Having a Vagrant customize template will allow us to deploy this template and SSH to the cloned vm using the &amp;quot;vagrant ssh&amp;quot; command, as well automatically run puppet manifests when we deploy a vm using Vagrant. Let&amp;#39;s get started:&lt;/p&gt;

&lt;h3&gt;1. Ensure you have a DHCP server on the network you will be connecting the Vagrant deployed templates to. While you can utilized guest customization to assign a static IP addresses to a Vagrant deployed vm, using DHCP will make this process much easier.&lt;/h3&gt;

&lt;h3&gt;2. Create a CentOS 6.x minimal virtual machine for configuring as our Vagrant CentOS template.&lt;/h3&gt;

&lt;p&gt;Create or clone a fresh CentOS 6.x minimal virtual machine. I already have an existing Centos 6.5 minimal template that I simply cloned for this purpose.&lt;/p&gt;

&lt;h3&gt;3. Power on the new cloned vm and connect using SSH to the DHCP assigned IP address for this vm.&lt;/h3&gt;

&lt;h3&gt;4. Ensure you have perl, rsync, ruby &amp;amp; puppet agent installed:&lt;/h3&gt;

&lt;h4&gt;A. Import the EPEL 6 key and RPM:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@centos-6-5 ~]# rpm --import http://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-6
[root@centos-6-5 ~]# rpm -Kih http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm
########################################### [100%]
########################################### [100%]&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;B. Use yum to install ruby, wget, puppet, facter &amp;amp; rsync:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@centos-6-5 ~]# yum install ruby ruby-libs ruby-shad wget yum-priorities puppet facter rsync -y&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3&gt;5. Ensure you have vmtools installed on the new CentOS template.&lt;/h3&gt;

&lt;h4&gt;A. Start the install process by clicking &amp;quot;VM | Guest | Install/Upgrade VMware Tools&amp;quot; in the vShpere client. When prompted select &amp;quot;Automatice Tools Upgrade&amp;quot; and click &amp;quot;OK&amp;quot;&lt;/h4&gt;

&lt;h4&gt;B. Create a directory to mount the virtual CDROM to with the following command:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@centos-6-5 ~]# mkdir /mnt/cdrom&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;C. Mount the virtual CDROM to /mnt/cdrom with the following command:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@centos-6-5 ~]# mount /dev/cdrom /mnt/cdrom
mount: block device /dev/sr0 is write-protected, mounting read-only&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;D. Copy the VMware Tools installer over to the /root directory and unzip it:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@centos-6-5 ~]# cp /mnt/cdrom/VMwareTools-* ~/
[root@centos-6-5 ~]# tar -zxf ~/VMwareTools*&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;E. Change directory into the directory we just untared and run the &amp;quot;vmware-install.pl&amp;quot; to start the installer:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@centos-6-5 ~]# cd vmware-tools-distrib/
[root@centos-6-5 vmware-tools-distrib]# ./vmware-install.pl&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You should be able to accept the defaults for all prompts and complete the VMware tools installer. Once the installer has completed check the vm summary of this template vm in the vSphere client. The VMware Tools should show as &amp;quot;Running&amp;quot; and &amp;quot;Current.&amp;quot;&lt;/p&gt;

&lt;h4&gt;F. Unmount the virtual CDROM using the following command:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@centos-6-5 ~]# umount /dev/cdrom&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3&gt;6. We will next follow the steps for &lt;a href=&quot;https://docs.vagrantup.com/v2/boxes/base.html&quot;&gt;creating a base box&lt;/a&gt; from the Vagrant website.&lt;/h3&gt;

&lt;p&gt;One thing to point out is we will be configuring the vagrant template for password less SSH using a vagrant user with the password of &amp;quot;vagrant.&amp;quot; The Vagrant website mentions &amp;#39;by default, Vagrant expects a &amp;quot;vagrant&amp;quot; user to SSH into the machine as. This user should be setup with the insecure keypair that Vagrant uses as a default to attempt to SSH. Also, even though Vagrant uses key-based authentication by default, it is a general convention to set the password for the &amp;quot;vagrant&amp;quot; user to &amp;quot;vagrant.&amp;quot;&amp;#39;&lt;/p&gt;

&lt;p&gt;This vagrant template should only be used for lab or development work and never used for production or publicly facing virtual machines due to the security risks of having known users, passwords &amp;amp; private SSH keys configured by default. &lt;/p&gt;

&lt;h4&gt;A. Add a new user named &amp;quot;vagrant&amp;quot; to the template.&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@centos-6-5 ~]# useradd vagrant&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;B. Change the password of the user &amp;quot;vagrant&amp;quot; to &amp;quot;vagrant&amp;quot;&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@centos-6-5 ~]# [root@centos-6-5 ~]# passwd vagrant
Changing password for user vagrant.
New password: &amp;lt;type the word &amp;quot;vagrant&amp;quot;&amp;gt;
BAD PASSWORD: it is based on a dictionary word
BAD PASSWORD: is too simple
Retype new password: &amp;lt;type the word &amp;quot;vagrant&amp;quot; again&amp;gt;
passwd: all authentication tokens updated successfully.&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;C. We will next need to copy the contents of the vagrant.pub file located at the &lt;a href=&quot;9https://github.com/mitchellh/vagrant/tree/master/keys&quot;&gt;vagrant github site&lt;/a&gt; into the /home/vagrant/.ssh/authorized_keys file.&lt;/h4&gt;

&lt;p&gt;First we will su (switch user) to the vagrant user by running the following command:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@centos-6-5 ~]# su vagrant
[vagrant@centos-6-5 root]$&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Next we will create the .ssh directory since is does not exist:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[vagrant@centos-6-5 root]$ mkdir ~/.ssh
[vagrant@centos-6-5 root]$ cd ~/.ssh
[vagrant@centos-6-5 .ssh]$ pwd
/home/vagrant/.ssh&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We will now create /home/vagrant/.ssh/authorized_keys with the contents of &lt;a href=&quot;https://raw.githubusercontent.com/mitchellh/vagrant/master/keys/vagrant.pub&quot;&gt;this file&lt;/a&gt; from the vagrant github page:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[vagrant@centos-6-5 .ssh]$ echo &amp;#39;ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA6NF8iallvQVp22WDkTkyrtvp9eWW6A8YVr+kz4TjGYe7gHzIw+niNltGEFHzD8+v1I2YJ6oXevct1YeS0o9HZyN1Q9qgCgzUFtdOKLv6IedplqoPkcmF0aYet2PkEDo3MlTBckFXPITAMzF8dJSIFo9D8HfdOV0IAdx4O7PtixWKn5y2hMNG0zQPyUecp4pzC6kivAIhyfHilFR61RGL+GPXQ2MWZWFYbAGjyiYJnAmCP3NOTd0jMZEnDkbUvxhMmBYSdETk1rRgm+R4LOzFUGaHqHDLKLX+FIPKcF96hrucXzcWyLbIbEgE98OHlnVYCzRdK8jlqm8tehUc9c9WhQ== vagrant insecure public key&amp;#39; &amp;gt; ~/.ssh/authorized_keys
[vagrant@centos-6-5 .ssh]$ cat ~/.ssh/authorized_keys
ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA6NF8iallvQVp22WDkTkyrtvp9eWW6A8YVr+kz4TjGYe7gHzIw+niNltGEFHzD8+v1I2YJ6oXevct1YeS0o9HZyN1Q9qgCgzUFtdOKLv6IedplqoPkcmF0aYet2PkEDo3MlTBckFXPITAMzF8dJSIFo9D8HfdOV0IAdx4O7PtixWKn5y2hMNG0zQPyUecp4pzC6kivAIhyfHilFR61RGL+GPXQ2MWZWFYbAGjyiYJnAmCP3NOTd0jMZEnDkbUvxhMmBYSdETk1rRgm+R4LOzFUGaHqHDLKLX+FIPKcF96hrucXzcWyLbIbEgE98OHlnVYCzRdK8jlqm8tehUc9c9WhQ== vagrant insecure public key&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The vagrant website also mentions &amp;#39;that OpenSSH is very picky about file permissions. Therefore, make sure that ~/.ssh has 0700 permissions and the authorized keys file has 0600 permissions.&amp;#39; We will correct those permissions now:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[vagrant@centos-6-5 .ssh]$ cd ~/
[vagrant@centos-6-5 ~]$ chmod 700 ~/.ssh
[vagrant@centos-6-5 ~]$ chmod 600 ~/.ssh/authorized_keys
[vagrant@centos-6-5 ~]$ ls -la
total 24
drwx------. 3 vagrant vagrant 4096 Apr 20 14:22 .
drwxr-xr-x. 3 root    root    4096 Apr 20 14:15 ..
-rw-r--r--. 1 vagrant vagrant   18 Jul 18  2013 .bash_logout
-rw-r--r--. 1 vagrant vagrant  176 Jul 18  2013 .bash_profile
-rw-r--r--. 1 vagrant vagrant  124 Jul 18  2013 .bashrc
drwx------. 2 vagrant vagrant 4096 Apr 20 14:26 .ssh
[vagrant@centos-6-5 ~]$ ls -la .ssh
total 12
drwx------. 2 vagrant vagrant 4096 Apr 20 14:26 .
drwx------. 3 vagrant vagrant 4096 Apr 20 14:22 ..
-rw-------. 1 vagrant vagrant  409 Apr 20 14:26 authorized_keys
[vagrant@centos-6-5 ~]$ exit
exit
[root@centos-6-5 ~]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;D. We will to edit the sudo file. Type the following command to open the sudo file in vi:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;root@centos-6-5 ~]# visudo&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Locate the line containing:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;Defaults    requiretty&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;...and change it to read:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;# Defaults    requiretty&amp;#39;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The # sign at the beginning of the line will cause this line to be ignored as a comment.
We also need to add the following line at the bottom of the file:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;vagrant ALL=(ALL) NOPASSWD: ALL&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;E. One more recommendation from the vagrant website is to edit /etc/ssh/sshd_config and change any line matching:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;UseDNS yes&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;to &lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;UseDNS no&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This avoids a reverse DNS lookup on the connecting SSH client which can take many seconds.&lt;/p&gt;

&lt;h3&gt;7. Lastly this is a template, so we will need to remove /etc/udev/rules.d/70-persistent-net.rules file because it contains the MAC address of the current virtual NIC. It will be recreated on boot up or when we deploy other virtual machines from this template.&lt;/h3&gt;

&lt;h3&gt;8. We will now shutdown the template by running halt:&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@centos-6-5 ~]# halt
Broadcast message from root@centos-6-5
    (/dev/pts/1) at 14:51 ...
The system is going down for halt NOW!&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3&gt;9. We now need to SSH to the vagrant vm that has the vagrant-vsphere plugin installed. This is the vm we created if you followed the &lt;a href=&quot;https://sdorsett.github.io/2014/04/19/vagrant-install/&quot;&gt;last blog post.&lt;/a&gt;&lt;/h3&gt;

&lt;h4&gt;A. Edit the Vagrantfile we created to modify the following line to reflect the name you gave the new vagrant template we cloned or created in the steps above:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;vsphere.template_name = &amp;#39;vagrant-centos-6.5&amp;#39;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;B. We can test this new template by issuing the following command:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vagrant-vms]# vagrant up --provider=vsphere
Bringing machine &amp;#39;default&amp;#39; up with &amp;#39;vsphere&amp;#39; provider...
==&amp;gt; default: Calling vSphere CloneVM with the following settings:
==&amp;gt; default:  -- Source VM: vagrant-centos-6.5
==&amp;gt; default:  -- Name: vagrant-test
==&amp;gt; default: Waiting for SSH to become available...
==&amp;gt; default: New virtual machine successfully cloned and started
==&amp;gt; default: Rsyncing folder: /root/vagrant-vms/ =&amp;gt; /vagrant&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Unlike our &amp;#39;vagrant up&amp;quot; command in the last post we have established a SSH session with the cloned template using the vagrant user we created and the public key in that users authorized_keys file.&lt;/p&gt;

&lt;p&gt;You also might notice that the entire directory we launched the &amp;#39;vagrant up&amp;quot; command from has been copied over to /vagrant on the cloned vm using rsync. We will go into leveraging this feature with puppet in our next blog post.&lt;/p&gt;

&lt;h4&gt;C. Using SSH to connect to the vagrant vm is as easy as &amp;#39;vagrant ssh&amp;#39;&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vagrant-vms]# hostname
vagrant.mylab.net
[root@vagrant vagrant-vms]# vagrant ssh
[vagrant@vagrant-centos-6-5 ~]$ hostname
vagrant-centos-6-5
[vagrant@vagrant-centos-6-5 ~]$ exit
logout
Connection to 192.168.1.133 closed.
[root@vagrant vagrant-vms]#&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;D. Finally we will power down and delete the cloned template by running the following command:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vagrant-vms]# vagrant destroy
==&amp;gt; default: Calling vSphere PowerOff
==&amp;gt; default: Calling vShpere Destroy&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3&gt;Hopefully you found this post helpful in demonstrating how to create a vagrant specific template to use with the vagrant-vsphere plugin. The next blog post will be covering how to take advantage of customization specifications and advanced Vagrantfile configurations on a vm deployed using vagrant.&lt;/h3&gt;

&lt;h3&gt;Please provide any feedback or suggestions to my twitter account located on the about page.&lt;/h3&gt;
</content>
 </entry>
 
 <entry>
   <title>Installing Vagrant and the vagrant-vsphere plugin on CentOS 6.x</title>
   <link href="http://0.0.0.0:4000/2014/04/19/vagrant-install/"/>
   <updated>2014-04-19T00:00:00+00:00</updated>
   <id>http://0.0.0.0:4000/2014/04/19/vagrant-install</id>
   <content type="html">&lt;p&gt;Some readers might find this post to be a little &amp;quot;in the weeds&amp;quot; and cover details they are already familiar with. I struggled in my first attempts to get Vagrant working on CentOS, because I couldn&amp;#39;t find any good tutorials that covered the entire process. If you get bored with basic network configuration or have another preferred method for installing Ruby, please understand I&amp;#39;m trying to provide as much detail as possible to those without much linux experience. I am also assuming you have a working vCenter, ESXi hosts and dhcp server configured to assign IP addresses for the Vagrant virtual machines we will be deploying.&lt;/p&gt;

&lt;h3&gt;1. Create a CentOS 6.x minimal virtual machine for installing Vagrant.&lt;/h3&gt;

&lt;p&gt;Create or clone a fresh CentOS 6.x minimal virtual machine. I already had an existing Centos 6.5 minimal template that I simply cloned for this purpose.&lt;/p&gt;

&lt;h3&gt;2. Power on the new cloned vm and connect to the virtual machine console&lt;/h3&gt;

&lt;h3&gt;3. Modify /etc/sysconfig/network, /etc/sysconfig/network-scripts/ifcfg-eth0 &amp;amp; /etc/resolv.conf to reflect the network settings for your vagrant vm:&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# cat /etc/sysconfig/network 
NETWORKING=yes 
HOSTNAME=vagrant.mylab.net 
GATEWAY=192.168.1.1 
[root@vagrant ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth0 
DEVICE=eth0 
TYPE=Ethernet 
ONBOOT=yes 
NM_CONTROLLED=no 
BOOTPROTO=static 
IPADDR=192.168.1.40 
NETMASK=255.255.255.0 
[root@vagrant ~]# cat /etc/resolv.conf
 search mylab.net 
nameserver 192.168.1.1 
nameserver 8.8.8.8 
nameserver 8.8.4.4&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3&gt;4. Restart the vm for the hostname change to take affect&lt;/h3&gt;

&lt;h3&gt;5. SSH to the vagrant vm using the IP address you assigned in /etc/sysconf/network-scripts/ifcfg-eth0&lt;/h3&gt;

&lt;h3&gt;6. Install Ruby 1.9.3. There are &lt;a href=&quot;http://www.agilesysadmin.net/acquiring-a-modern-ruby-part-one&quot;&gt;several ways&lt;/a&gt; to install ruby 1.9.3 on CentOS, but I&amp;#39;ll cover using ruby-build to accomplish this:&lt;/h3&gt;

&lt;h4&gt;A. Use yum to install the packages we&amp;#39;ll need to get ruby, rubygems and vagrant installed:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# yum install -y gcc-c++ glibc-headers openssl-devel readline libyaml-devel readline-devel zlib zlib-devel iconv-devel libxml2 libxml2-devel libxslt libxslt-devel wget git&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;B. Pull down the latest version of ruby-build using git:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# git clone https://github.com/sstephenson/ruby-build.git&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;C. Change to the directory the previous command created:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# cd ruby-build/&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;D. Run the &amp;#39;install.sh&amp;#39; script to install ruby-build:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ruby-build]# ./install.sh&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;E. I have not been successful in getting vagrant-vsphere to run on any version of ruby newer than 1.9.3-p545. As a result I would recommend installing that version with the following command:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ruby-build]# ruby-build --definitionsruby-build 1.9.3-p545 /usr/local/
Downloading yaml-0.1.6.tar.gz...
-&amp;gt; http://dqw8nmjcqpjn7.cloudfront.net/5fe00cda18ca5daeb43762b80c38e06e
Installing yaml-0.1.6...
Installed yaml-0.1.6 to /usr/local/

Downloading ruby-1.9.3-p545.tar.gz...
-&amp;gt; http://dqw8nmjcqpjn7.cloudfront.net/8e8f6e4d7d0bb54e0edf8d9c4120f40c
Installing ruby-1.9.3-p545...

Installed ruby-1.9.3-p545 to /usr/local/&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3&gt;7. We will next need to install rubygems. The process I used was to download the latest version from &lt;a href=&quot;https://rubygems.org/pages/download&quot;&gt;rubygems.org&lt;/a&gt; and install it:&lt;/h3&gt;

&lt;h4&gt;A. Use wget to download the latest version listed on the rubygems download page:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ruby-build]# cd ~/
[root@vagrant ~]# wget http://production.cf.rubygems.org/rubygems/rubygems-2.2.2.tgz&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;B. Use tar to unzip the tar-gzipped file we downloaded:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# tar xvzf rubygems-2.2.2.tgz
[root@vagrant ~]# cd rubygems-2.2.2&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;C. Install rubygems by running the &amp;#39;setup.rb&amp;#39; script:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant rubygems-2.2.2]# ruby setup.rb&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3&gt;8. Installing Vagrant&lt;/h3&gt;

&lt;h4&gt;A. Use wget to download the latest version listed on the &lt;a href=&quot;https://www.vagrantup.com/downloads.html&quot;&gt;Vagrant download&lt;/a&gt; page:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant rubygems-2.2.2]# cd ~/
[root@vagrant ~]# wget https://dl.bintray.com/mitchellh/vagrant/vagrant_1.5.2_x86_64.rpm&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;B. Use the &amp;#39;rpm&amp;#39; command to install the RPM package we downloaded in the previous step:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# rpm -i vagrant_1.5.2_x86_64.rpm&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;C. Export the directory that the vagrant executable was install to, in order to make it easier to run:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# export PATH=$PATH:/opt/vagrant/bin/&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3&gt;9. Installing the vagrant-vsphere plugin&lt;/h3&gt;

&lt;h4&gt;A. The nokogiri gem, which is a ruby HTML, XML &amp;amp; SAM parser, is required by vagrant-vsphere so we will install it with the following commands:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# gem install nokogiri
Building native extensions.  This could take a while...
Successfully installed nokogiri-1.6.1
unable to convert &amp;quot;\x89&amp;quot; from ASCII-8BIT to UTF-8 for ext/nokogiri/tmp/x86_64-redhat-linux/ports/libxslt/1.1.26/libxslt-1.1.26/tests/xmlspec/logo-REC, skipping
unable to convert &amp;quot;\xF6&amp;quot; from ASCII-8BIT to UTF-8 for ext/nokogiri/tmp/x86_64-redhat-linux/ports/libxslt/1.1.26/libxslt-1.1.26/libxslt/xslt.c, skipping
unable to convert &amp;quot;\xF6&amp;quot; from ASCII-8BIT to UTF-8 for ext/nokogiri/tmp/x86_64-redhat-linux/ports/libxslt/1.1.26/libxslt-1.1.26/ChangeLog, skipping
unable to convert &amp;quot;\xE1&amp;quot; from ASCII-8BIT to UTF-8 for ext/nokogiri/tmp/x86_64-redhat-linux/ports/libxslt/1.1.26/libxslt-1.1.26/NEWS, skipping
unable to convert &amp;quot;\xFD&amp;quot; from ASCII-8BIT to UTF-8 for ext/nokogiri/tmp/x86_64-redhat-linux/ports/libxslt/1.1.26/libxslt-1.1.26/win32/Readme.txt, skipping
unable to convert &amp;quot;\xC0&amp;quot; from ASCII-8BIT to UTF-8 for ext/nokogiri/tmp/x86_64-redhat-linux/ports/libxml2/2.8.0/libxml2-2.8.0/test/XInclude/ents/isolatin.txt, skipping
unable to convert &amp;quot;\xE4&amp;quot; from ASCII-8BIT to UTF-8 for ext/nokogiri/tmp/x86_64-redhat-linux/ports/libxml2/2.8.0/libxml2-2.8.0/doc/examples/testWriter.c, skipping
unable to convert &amp;quot;\xF8&amp;quot; from ASCII-8BIT to UTF-8 for ext/nokogiri/tmp/x86_64-redhat-linux/ports/libxml2/2.8.0/libxml2-2.8.0/testapi.c, skipping
unable to convert &amp;quot;\xE9&amp;quot; from ASCII-8BIT to UTF-8 for ext/nokogiri/tmp/x86_64-redhat-linux/ports/libxml2/2.8.0/libxml2-2.8.0/runtest.c, skipping
unable to convert &amp;quot;\xF8&amp;quot; from ASCII-8BIT to UTF-8 for ext/nokogiri/tmp/x86_64-redhat-linux/ports/libxml2/2.8.0/libxml2-2.8.0/entities.c, skipping
unable to convert &amp;quot;\xC0&amp;quot; from ASCII-8BIT to UTF-8 for ports/x86_64-redhat-linux/libxslt/1.1.26/bin/xsltproc, skipping
unable to convert &amp;quot;\xE4&amp;quot; from ASCII-8BIT to UTF-8 for ports/x86_64-redhat-linux/libxml2/2.8.0/share/doc/libxml2-2.8.0/html/testWriter.c, skipping
unable to convert &amp;quot;\xC0&amp;quot; from ASCII-8BIT to UTF-8 for ports/x86_64-redhat-linux/libxml2/2.8.0/bin/xmllint, skipping
unable to convert &amp;quot;\xC0&amp;quot; from ASCII-8BIT to UTF-8 for ports/x86_64-redhat-linux/libxml2/2.8.0/bin/xmlcatalog, skipping
1 gem installed&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;B. Next we will install the rbvmomi gem with the following command. rbvmomi is a ruby interface for using the vSphere API.&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# gem install rbvmomi
Successfully installed rbvmomi-1.8.1
1 gem installed&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;C. Now we can install the vagrant-vsphere plugin with the following command:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# vagrant plugin install vagrant-vsphere
Installing the &amp;#39;vagrant-vsphere&amp;#39; plugin. This can take a few minutes...
Installed the plugin &amp;#39;vagrant-vsphere (0.8.1)&amp;#39;!&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3&gt;8. Configuring vagrant-vsphere&lt;/h3&gt;

&lt;p&gt;The vagrant-vsphere &lt;a href=&quot;https://github.com/nsidc/vagrant-vsphere&quot;&gt;README.md on github&lt;/a&gt; mentions we need to create a vSphere box from the metadata.json located in the example_box directory. &lt;/p&gt;

&lt;h4&gt;A.  We can use the linux the &amp;quot;find&amp;quot; command to determine the path to the example_box directory:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# find / -name example_box
/root/.vagrant.d/gems/gems/vagrant-vsphere-0.8.1/example_box&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;B.  Now we can change directory to that directory and create the necessary dummy box file:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant ~]# cd /root/.vagrant.d/gems/gems/vagrant-vsphere-0.8.1/example_box
[root@vagrant example_box]# tar cvzf dummy.box ./metadata.json
./metadata.json&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;C.  Next we will create a new folder under /root for storing the box file we created and other vagrant configuration files&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant example_box]# mkdir -p ~/vagrant-vms/example_box
[root@vagrant example_box]# mv dummy.box ~/vagrant-vms/example_box&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;D. Vagrant reads it&amp;#39;s configuration from a file named Vagrantfile, located in the directory from which the &amp;quot;vagrant up&amp;quot; command is run in. We need to go back to our &amp;quot;vagrant-vms&amp;quot; folder and create this file:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant example_box]# cd ~/vagrant-vms/
[root@vagrant vagrant-vms]# touch Vagrantfile&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;E. Modify the Vagrantfile to reflect your vsphere configuration. Here&amp;#39;s an example of my working Vagrantfile:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vagrant-vms]# cat Vagrantfile

Vagrant.configure(&amp;quot;2&amp;quot;) do |config|
  config.vm.box = &amp;#39;dummy&amp;#39;
  config.vm.box_url = &amp;#39;./example_box/dummy.box&amp;#39;

  config.vm.provider :vsphere do |vsphere|
    vsphere.host = &amp;#39;vcenter.mylab.net&amp;#39;
    vsphere.name = &amp;#39;vagrant-test&amp;#39;
    vsphere.clone_from_vm = true
    vsphere.template_name = &amp;#39;Centos-6.5&amp;#39;
    vsphere.user = &amp;#39;root@localos&amp;#39;
    vsphere.password = &amp;#39;S0meR@nd0mP@ssw0rd&amp;#39;
    vsphere.insecure = true
  end
end&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In this configuration file:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&amp;quot;vsphere.host&amp;quot; is your vCenter server FQDN or IP address&lt;/li&gt;
&lt;li&gt;&amp;quot;vsphere.name&amp;quot; will be the name of the virtual machine that gets created&lt;/li&gt;
&lt;li&gt;&amp;quot;vsphere.clone_from_vm&amp;quot; specifies we will be cloning a vm rather than template&lt;/li&gt;
&lt;li&gt;&amp;quot;vsphere.template_name&amp;quot; is the name of the vm we will be cloning&lt;/li&gt;
&lt;li&gt;&amp;quot;vsphere.user&amp;quot; is the username we will be using to connect to vCenter&lt;/li&gt;
&lt;li&gt;&amp;quot;vsphere.password&amp;quot; is the password we will be using to connect ot vCenter&lt;/li&gt;
&lt;li&gt;&amp;quot;vsphere.insecure&amp;quot; tells vagrant to not worry about validating the certificate on the vCenter server&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;9. Vargrant up!!!!&lt;/h3&gt;

&lt;p&gt;If everything has gone correctly up to this point we can issue a &amp;quot;vagrant up --provider=vsphere&amp;quot; command from the directory that contains the Vagrantfile:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vagrant-vms]# vagrant up --provider=vsphere
Bringing machine &amp;#39;default&amp;#39; up with &amp;#39;vsphere&amp;#39; provider...
==&amp;gt; default: Calling vSphere CloneVM with the following settings:
==&amp;gt; default:  -- Source VM: CentOS-6.5
==&amp;gt; default:  -- Name: vagrant-test
==&amp;gt; default: Waiting for SSH to become available...&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You should see a &amp;quot;Clone virtual machine&amp;quot; task being run on your vCenter server and then completing.&lt;/p&gt;

&lt;h3&gt;10. Canceling the &amp;quot;Vagrant up&amp;quot; command and destroying our cloned vm.&lt;/h3&gt;

&lt;p&gt;Since we have not properly prepared the vm we are cloning (I will have another blog post covering this topic) Vagrant will never be able to successfully connect to the cloned vm using SSH. As a result we will need to cancel the &amp;quot;vagrant up&amp;quot; command:&lt;/p&gt;

&lt;h4&gt;A. Press [control-c] to attempt to gracefully exit &amp;quot;vagrant-up&amp;quot;&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;^C==&amp;gt; default: Waiting for cleanup before exiting...&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;I&amp;#39;ve never seen this task complete so I will press [control-c] again to immediately exit:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;^C==&amp;gt; default: Exiting immediately, without cleanup!&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4&gt;B. Destroy the cloned vm by running &amp;quot;vagrant destroy&amp;quot; command:&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;[root@vagrant vagrant-vms]# vagrant destroy
==&amp;gt; default: Calling vSphere PowerOff
==&amp;gt; default: Calling vShpere Destroy&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You should see a &amp;quot;Power Off virtual machine&amp;quot; and &amp;quot;Delet virtual machine&amp;quot; tasks being run and then completing on your vCenter server.&lt;/p&gt;

&lt;h3&gt;Hopefully you found this post helpful in getting vagrant and the vagrant-vsphere plugin installed and configured. The next blog post will be covering how to create a template vm that has been prepared for vagrant to connect seemlessly using SSH.&lt;/h3&gt;

&lt;h3&gt;Please provide any feedback or suggestions to my twitter account located on the about page.&lt;/h3&gt;
</content>
 </entry>
 
 <entry>
   <title>Vagrant and why it is useful in learning Puppet</title>
   <link href="http://0.0.0.0:4000/2014/04/16/vagrant-explained/"/>
   <updated>2014-04-16T00:00:00+00:00</updated>
   <id>http://0.0.0.0:4000/2014/04/16/vagrant-explained</id>
   <content type="html">&lt;p&gt;Vagrant, according to the &lt;a href=&quot;http://docs.vagrantup.com/v2/why-vagrant/&quot;&gt;documentation&lt;/a&gt;, provides a &amp;quot;disposable environment and consistent workflow for developing and testing infrastructure management scripts. You can quickly test things like shell scripts, Chef cookbooks, Puppet modules, and more using local virtualization such as VirtualBox or VMware. Then, with the same configuration, you can test these scripts on remote clouds such as AWS or RackSpace with the same workflow.&amp;quot;&lt;/p&gt;

&lt;p&gt;I&amp;#39;ve been wanting to dedicate more time to working with Puppet and needed a better way to quickly test my manifests on a fresh system. The vCHS automation team has demonstrated using Vagant with VMware fusion, but I wanted to be able to use it in my homelab, natively with vSphere/ESXi. The vagrant-vsphere plugin provides the ability to use simple commands to deploy a vm/template, apply a specified manifest and even SSH to the freshly create vm. &lt;/p&gt;

&lt;p&gt;This will be the first of several blog posts in which I will cover:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://sdorsett.github.io/2014/04/19/vagrant-install/&quot;&gt;Installing Vagant and the vagrant-vsphere plugin on a CentOS 6.x virtual machine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://sdorsett.github.io/2014/04/20/vagrant-boxes/&quot;&gt;Creating a CentOS 6.x template that is customized for Vagrant&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://sdorsett.github.io/2014/04/24/vagrant-and-vcenter-guest-customization/&quot;&gt;Using advanced vagrant-vsphere provider settings and vCenter guest customization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://sdorsett.github.io/2014/05/06/vagrant-and-puppet/&quot;&gt;Creating a Puppet manifest and integrating it with Vagrant&lt;/a&gt; &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These posts will hopefully help you understand the steps necessary to get vagrant installed and working with a vSphere environment.&lt;/p&gt;
</content>
 </entry>
 

</feed>
