<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      sdorsett.github.io &middot; Things I find interesting
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700|PT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p></p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item active" href="/">Home</a>

    

    
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/about/">About</a>
        
      
    
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/posts/">All Posts</a>
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
    <a class="sidebar-nav-item" href="https://github.com/sdorsett/archive/v1.0.0.zip">Download</a>
    <a class="sidebar-nav-item" href="https://github.com/sdorsett">GitHub project</a>
    <span class="sidebar-nav-item">Currently v1.0.0</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2018. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">sdorsett.github.io</a>
            <small>Things I find interesting</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2018/08/10/using-the-openstack-cli-to-create-a-server-on-ovh-public-cloud/">
        Using the Openstack cli to create a server on OVH public cloud
      </a>
    </h1>

    <span class="post-date">10 Aug 2018</span>

    <p>This is the second in a series of posts that will walk you through using the Openstack-based OVH public cloud. This post is assumes you have already signed up for an account with ovhcloud.com, added a payment method and created a cloud project. If you have not done these steps you can follow <a href="http://sdorsett.github.io/2018/08/08/creating-an-openstack-instance-on-ovh-public-cloud/">the first blog post</a> in this series that will walk you through completing those steps. </p>

<p>I will attempt in this post to present the options that are available to OVH public cloud customers along side the choices I made that were specific to my Openstack server.</p>

<p>Let&#39;s get started...</p>

<hr>

<h2>1. Sign into <a href="https://ovhcloud.com/auth/">ovhcloud.com</a>.</h2>

<p>The first thing you will need to do before you can move forward with this post is sign into <a href="https://ovhcloud.com/auth/">https://ovhcloud.com/auth/</a> using the account you created in the first blog post..</p>

<p><img src="/assets/08102018-01-login-username-password.png" alt="screenshot"></p>

<p>If you setup two-factor authentication, you will be prompted to enter the current two-factor token.</p>

<p><img src="/assets/08102018-02-login-two-factor.png" alt="screenshot"></p>

<h2>2. Create a new user account to use with the Openstack API.</h2>

<p>The Openstack API will not use the credentials you used to log into the ovhcloud.com site, but will instead use an Openstack user to create, modify or view objects in an OVH cloud project.</p>

<p>In order to use the Openstack API you will need to create an Openstack user account within the cloud project you want to work with. You can display all the Openstack user accounts then have been created in a cloud project by clicking the &#39;Openstack&#39; tab within any cloud project. </p>

<p><img src="/assets/08102018-03-openstack-users.png" alt="screenshot"></p>

<p>Create a new user click the &quot;Add user&quot; button.</p>

<p><img src="/assets/08102018-04-openstack-add-user.png" alt="screenshot"></p>

<p>Once you have specified the username and clicked &quot;Confirm&quot; you will be taken back to the Openstack tab and see the user you just created. You will also be able to see the password for a short period of time. Make sure you record this password safely since it will not be visible perminantly. </p>

<p><img src="/assets/08102018-05-openstack-users-password-shown.png" alt="screenshot"></p>

<p>After a period of time you will no longer be able to see the password for the user you created, but If you do happen to forget the password, you can always have a new password regenerated.</p>

<p><img src="/assets/08102018-14-openstack-users-password-hidden.png" alt="screenshot"></p>

<h2>3. Download an Openstack configuration file.</h2>

<p>Now that you have an Openstack user created you can download an openrc.sh file that will contain all the details needed to connect as this user. Click the &#39;...&#39; icon and the end of the line for the user you want to use and select &#39;Downloading an Openstack configuration file&#39;.</p>

<p><img src="/assets/08102018-06-download-openrc.png" alt="screenshot"></p>

<p>Select the region (datacenter) you want to connect to and click &#39;confirm&#39;. Your browser will next prompt you where you want to save the downloaded openrc.sh file.</p>

<p><img src="/assets/08102018-07-download-openrc-region.png" alt="screenshot"></p>

<p>If you look at the downloaded file you will see that it sets environment variables containing the details needed to connect to your Openstack cloud project.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">MacBook-Pro:Desktop standorsett$ cat ~/Downloads/openrc.sh
#!/bin/bash

# To use an Openstack cloud you need to authenticate against keystone, which
# returns a **Token** and **Service Catalog**. The catalog contains the
# endpoint for all services the user/tenant has access to - including nova,
# glance, keystone, swift.
#
export OS_AUTH_URL=https://auth.cloud.ovh.us/v3/
export OS_IDENTITY_API_VERSION=3

export OS_USER_DOMAIN_NAME=${OS_USER_DOMAIN_NAME:-&quot;Default&quot;}
export OS_PROJECT_DOMAIN_NAME=${OS_PROJECT_DOMAIN_NAME:-&quot;Default&quot;}


# With the addition of Keystone we have standardized on the term **tenant**
# as the entity that owns the resources.
export OS_TENANT_ID=2897923a654b40d0b8b36a502bda4a8f
export OS_TENANT_NAME=&quot;3543682553721111&quot;

# In addition to the owning entity (tenant), openstack stores the entity
# performing the action as the **user**.
export OS_USERNAME=&quot;5r7jJyuwwwsv&quot;

# With Keystone you pass the keystone password.
echo &quot;Please enter your OpenStack Password: &quot;
read -sr OS_PASSWORD_INPUT
export OS_PASSWORD=$OS_PASSWORD_INPUT

# If your configuration has multiple regions, we set that information here.
# OS_REGION_NAME is optional and only valid in certain environments.
export OS_REGION_NAME=&quot;US-EAST-VA-1&quot;
# Don&#39;t leave a blank variable, unset it if it was empty
if [ -z &quot;$OS_REGION_NAME&quot; ]; then unset OS_REGION_NAME; fi
MacBook-Pro:Desktop standorsett$</code></pre></figure>

<p>The only piece of information that this file does not contain is your password, which it will prompt for when you source this file.</p>

<h2>4. Log into the Openstack Horizon UI.</h2>

<p>Just like you needed to create an Openstack user for interacting with the Openstack API, you will need to create a ssh keypair for connecting to Openstack servers. This can be done within the Openstack Horizon UI. </p>

<p>Click the &#39;...&#39; icon and the end of the line for the user you want to use for interacting with the Openstack API and select &#39;Launch Openstack Horizon&#39;.</p>

<p><img src="/assets/08102018-08-launch-openstack-horizon.png" alt="screenshot"></p>

<p>This will open another browser tab for the Openstack Horizon UI with your username already filled in. Provide the password for this user and click &#39;connect&#39;.</p>

<p><img src="/assets/08102018-09-login-openstack-horizon.png" alt="screenshot"></p>

<h2>5. Create a ssh key-pair in the Openstack Horizon UI.</h2>

<p>We need to add a new ssh keypair that the Openstack API can add to servers that are created through the API. To keep things simple I added the same public ssh key that I generated in the first blog post.</p>

<p>To add a new ssh key pair go to &#39;Project | Compute | Key pairs | Create Key Pair&#39;</p>

<p><img src="/assets/08102018-10-openstack-horizon-key-pairs.png" alt="screenshot"></p>

<p>Give the key pair a friendly name you can remember and paste the contents of the public key file. Click &#39;Import Key Pair&#39; to add this key pair.  </p>

<p><img src="/assets/08102018-12-openstack-horizon-import-key-pair.png" alt="screenshot"></p>

<p>The key pair you just added will now be visible in the Horizon UI.</p>

<p><img src="/assets/08102018-13-openstack-horizon-key-pairs.png" alt="screenshot"></p>

<h2>6. Installing the Openstack cli.</h2>

<p>You will need to install the openstack cli on the operating system you are using. You can easily find instruction for installing it on whatever OS you prefer to use. I quickly installed it on my Mac laptop with homebrew using the following commands.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">brew install python2
pip2 install --upgrade pip setuptools
pip2 install --upgrade python-openstackclient</code></pre></figure>

<h2>7. Sourcing the openrc.sh file</h2>

<p>In order to use the Openstack cli you just installed you first need to source the openrc.sh file you downloaded in step 3.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">MacBook-Pro:~ standorsett$ source ~/Downloads/openrc.sh
Please enter your OpenStack Password:
MacBook-Pro:~ standorsett$</code></pre></figure>

<p>If you take a look at your exports environmental variables after sourcing the openrc.sh file, you will see the values from the opensh.rc file set along with your password.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">MacBook-Pro:~ standorsett$ export | grep OS_
declare -x OS_AUTH_URL=&quot;https://auth.cloud.ovh.us/v3/&quot;
declare -x OS_IDENTITY_API_VERSION=&quot;3&quot;
declare -x OS_PASSWORD=&quot;*******************************&quot;
declare -x OS_PROJECT_DOMAIN_NAME=&quot;Default&quot;
declare -x OS_REGION_NAME=&quot;US-EAST-VA-1&quot;
declare -x OS_TENANT_ID=&quot;2897923a654b40d0b8b36a502bda4a8f&quot;
declare -x OS_TENANT_NAME=&quot;3543682553721111&quot;
declare -x OS_USERNAME=&quot;5r7jJyuwwwsv&quot;
declare -x OS_USER_DOMAIN_NAME=&quot;Default&quot;
MacBook-Pro:~ standorsett$</code></pre></figure>

<h2>8. Using the Openstack cli to create an Openstack server</h2>

<p>Now that you have sourced the openrc.sh file you can start using the Openstack cli.</p>

<p>First list the ssh key pair you created in step 5.</p>

<p><img src="/assets/08102018-16-openstack-cli-keypair-list.png" alt="screenshot"></p>

<p>We will need the name of this keypair shortly.</p>

<p>Next list the available operating system images.</p>

<p><img src="/assets/08102018-17-openstack-cli-image-list.png" alt="screenshot"></p>

<p>We will need the ID of the image you want to use, so I noted <code>e0a89ff2-e98f-4a34-afae-68f9f6c9b5ad</code> for the ID of Centos 7 I wanted to test.</p>

<p>Next list the available server sizes or flavors.</p>

<p><img src="/assets/08102018-18-openstack-cli-flavor-list.png" alt="screenshot"></p>

<p>We will need the ID of the flavor of the server you want to create. I chose a <code>s1-2</code> flavor since it was the smallest sandbox server with the lowest hourly rate, so I noted it&#39;s ID was <code>a64381e7-c4e7-4b01-9fbe-da405c544d2e</code></p>

<p>Finally list the security groups.</p>

<p><img src="/assets/08102018-19-openstack-cli-security-group-list.png" alt="screenshot"></p>

<p>The default security group is named <code>default</code> so you will need to specify it when creating a server..</p>

<p>With a flavor id, image id keypair name and security group name you are ready to create a new openstack server using a command like the following</p>
<figure class="highlight"><pre><code class="language-text" data-lang="text">openstack server create --flavor a64381e7-c4e7-4b01-9fbe-da405c544d2e --image e0a89ff2-e98f-4a34-afae-68f9f6c9b5ad --key-name ovhus_public_cloud --security-group default openstack_cli_test
</code></pre></figure>
<p>Below is a screenshot of me running that command.</p>

<p><img src="/assets/08102018-20-openstack-cli-server-create.png" alt="screenshot"></p>

<p>After the Openstack server has been created you can run <code>openstack server list</code> to show the details of the server you just created. The networks field will show the IP address of the server.</p>

<p><img src="/assets/08102018-21-openstack-cli-server-list.png" alt="screenshot"></p>

<p>Now that you know the IP address of the server you just created, You can test connecting to this server using the ssh key you created in the previous blog post.</p>

<p><img src="/assets/08102018-22-openstack-cli-server-ssh.png" alt="screenshot"></p>

<h2>9. Validating the Openstack server in ovhcloud.com UI</h2>

<p>You can check the ovhcloud.com UI to confirm the server has been created and the IP address is the same as what <code>openstack server list</code> displayed.</p>

<p><img src="/assets/08102018-23-ovhcloud-ui-server-created.png" alt="screenshot"></p>

<h2>10. Deleting the Openstack server using the Openstack cli.</h2>

<p>Once you are finished with your openstack server, you should delete the server to prevent being charged for it&#39;s usage. Run the following command in the Openstack cli to delete the server.</p>

<p><img src="/assets/08102018-24-openstack-cli-server-delete.png" alt="screenshot"></p>

<p>Running <code>openstack server list</code> will show that the server has been sucessfully deleted. </p>

<p><img src="/assets/08102018-25-openstack-cli-server-list.png" alt="screenshot"></p>

<p>You can also check the ovhcloud.com UI to confirm the server has been deleted.</p>

<p><img src="/assets/08102018-26-ovhcloud-ui-server-deleted.png" alt="screenshot"></p>

<hr>

<h3>That all for this post covering how to use the Openstack cli utility to create servers on the OVH public cloud.</h3>

<h3>Please provide any feedback or suggestions to my twitter account located on the about page.</h3>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2018/08/08/creating-an-openstack-server-on-ovh-public-cloud/">
        Creating an openstack server on OVH public cloud
      </a>
    </h1>

    <span class="post-date">08 Aug 2018</span>

    <p>This is the first in a series of posts that will walk you through using the openstack-based OVH public cloud. Over the last few years, I have worked primarily with VMware products and have not taken the time to learn with openstack. Now that the OVH public cloud has launched in the United States, and I work for OVH US, I figured it was time to start learning how customers could put this offering to use.</p>

<p>I will attempt in this post to present the options that are available to OVH public cloud customers along side the choices I made that were specific to my server.</p>

<p>Let&#39;s get started...</p>

<hr>

<h2>1. Create an account at <a href="https://ovhcloud.com/auth/">ovhcloud.com</a>.</h2>

<p>The first thing you will need to do before you can create OVH public cloud servers is setup an account by going to <a href="https://ovhcloud.com/auth/">https://ovhcloud.com/auth/</a> and clicking the &#39;Create an account&#39; button. </p>

<p><img src="/assets/08082018-01-create-an-account.png" alt="screenshot"></p>

<p>After providing my email address, specifying a complex password and adding my billing address I received an email within minutes that my account had been activated.</p>

<h2>2. Setup two-factor authentication.</h2>

<p>The second thing you should do after creating an account is setup two-factor authentication. Enabling two-factor authentication will require that you provide a code (token) from your two-factor authentication application on your phone or physical device, along with your username and password when you log into ovhcloud.com. I would highly recommend setting up two-factor authentication to help prevent unauthorized access to your OVH account.</p>

<p>I found the option to enable two-factor authentication by going to My Account | Security | Two-factor authentication | activate double authentication</p>

<p><img src="/assets/08082018-02-activate-two-factor-auth.png" alt="screenshot"></p>

<p>OVH supports mobile applications and physical devices for two-factor authentication. I selected mobile application since I would be using Google Authenticator.</p>

<p><img src="/assets/08082018-04-activate-two-factor-auth.png" alt="screenshot"></p>

<p>Next I scanned the QR code on my phone, provided a code (token) generated by the Google Authenticator app and gave it a friendly name.</p>

<p><img src="/assets/08082018-05-activate-two-factor-auth.png" alt="screenshot"></p>

<p>After clicking next you will be given emergency codes that can be used if you lose your mobile app or physical two-factor device. Copy these emergency codes and store them in a safe place since they can never be retrieved again. After clicking next the two-factor authentication option will show as being enabled.</p>

<p><img src="/assets/08082018-06-activate-two-factor-auth.png" alt="screenshot"></p>

<h2>3. Adding a payment method.</h2>

<p>You will now need to add a payment method, since it will be used to purchase cloud credits in the next step. I added a payment method by clicking my username at the upper right, clicking &#39;My payment methods&#39; and finally clicking the &#39;Add a payment method&#39; button. Provide your card number, experation date and security code and click the &#39;Add&#39; button.</p>

<p>Once your payment method shows a &quot;Confirmed&quot; status you are ready to proceed to the next step.</p>

<p><img src="/assets/08082018-07-add-payment-method.png" alt="screenshot"></p>

<h2>4. Creating a public cloud project.</h2>

<p>The next step you need to do is create a cloud project. To do this I went to order dropdown and clicked &#39;Cloud project&#39;.</p>

<p><img src="/assets/08082018-08-create-cloud-project.png" alt="screenshot"></p>

<p>Name your project and click &#39;launch your cloud project&#39;.</p>

<p><img src="/assets/08082018-09-create-cloud-project.png" alt="screenshot"></p>

<p>On the next screen you will be purchasing credits that your running servers will consume.</p>

<p><img src="/assets/08082018-10-create-cloud-project.png" alt="screenshot"></p>

<p>Accept the terms and conditions by clicking the check box. Your cloud project will be ordered and payment method charged once you click the &#39;Validate and pay&#39; button. </p>

<p><img src="/assets/08082018-11-create-cloud-project.png" alt="screenshot"></p>

<p>I received an email from OVH within 5 minutes of ordering to tell me that my cloud project had been created.</p>

<h2>5. Creating a new ssh key for OVH public cloud to use.</h2>

<p>You will need to generate a ssh key to connect to the OVH openstack servers. On my mac laptop I ran the following command to generate a new ssh key for this purpose.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">ssh-keygen -b 4096 -t rsa -C &quot;stan.dorsett+ovhus_public_cloud@gmail.com&quot; -f ~/.ssh/id_rsa-ovhus_public_cloud</code></pre></figure>

<ul>
<li>The <code>-b</code> option will specify the number of bits in the ssh key.</li>
<li>The <code>-t</code> option with specify the type of ssh key generated.</li>
<li>The <code>-C</code> option will add a comment to the end of the public key file.</li>
<li>The <code>-f</code> option will specify the file that the private key will be saved to. </li>
</ul>

<p>When running the command you will also get asked you if you want to add a passphrase that will need to be provided when you use the ssh key. I didn&#39;t choose to set a passphrase, but you can choose to if you feel the need.</p>

<p>Here is the output from my running of this command.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">MacBook-Pro:~ standorsett$ ssh-keygen -b 4096 -t rsa -C &quot;stan.dorsett+ovhus_public_cloud@gmail.com&quot; -f ~/.ssh/id_rsa-ovhus_public_cloud
Generating public/private rsa key pair.
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /Users/standorsett/.ssh/id_rsa-ovhus_public_cloud.
Your public key has been saved in /Users/standorsett/.ssh/id_rsa-ovhus_public_cloud.pub.
The key fingerprint is:
SHA256:7kSErA6S8zCIq0H/2hmcxE4+JUsW6ZmoDPIE/ULM8I4 stan.dorsett+ovhus_public_cloud@gmail.com
The key&#39;s randomart image is:
+---[RSA 4096]----+
|.                |
| *   ...         |
|. *  oo .        |
|o* .+.+.         |
|E.=.o@ .S        |
|=Xo+O =o         |
|ooo..O  o        |
|..  o +o         |
|.  ..+  .        |
+----[SHA256]-----+
MacBook-Pro:~ standorsett$</code></pre></figure>

<p>Output the public key so you can use it in the next step.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">MacBook-Pro:~ standorsett$ cat /Users/standorsett/.ssh/id_rsa-ovhus_public_cloud.pub
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDJ8LcFN8MRO/uwgVcdDg0grv
...lots of more characters here...
l0nXM7jjnjD9L3UMFw== stan.dorsett+ovhus_public_cloud@gmail.com
MacBook-Pro:~ standorsett$</code></pre></figure>

<h2>6. Add the ssh pub key to the OVH public cloud.</h2>

<p>You next need to add the public key generate in the previous step to OVH public cloud so it can be used to connect to servers you create.</p>

<p>Go to the project you created in the previous step. You can find this by clicking Cloud | Servers | [your project name].</p>

<p><img src="/assets/08082018-12-view-cloud-project.png" alt="screenshot"></p>

<p>Click the &#39;SSh keys&#39; tab to view or create new ssh keys.</p>

<p><img src="/assets/08082018-13-add-ssh-key.png" alt="screenshot"></p>

<p>Click &#39;Add a new key&#39; to add the public key generated in the previous step.</p>

<p><img src="/assets/08082018-14-add-ssh-key.png" alt="screenshot"></p>

<p>Paste the output from the public key file, provide a friendly name for the ssh key and click &quot;Add this key.&#39;</p>

<p><img src="/assets/08082018-15-add-ssh-key.png" alt="screenshot"></p>

<h2>7. Create a new openstack server.</h2>

<p>Under your cloud project and click the Infrastructure tab. From the Actions dropdown click &#39;Add a server.&#39;</p>

<p><img src="/assets/08082018-16-add-a-server.png" alt="screenshot"></p>

<p>Choose the OS (image) that you want to have deployed.</p>

<p><img src="/assets/08082018-17-pick-an-os-image.png" alt="screenshot"></p>

<p>Select the server type (flavor) of the server you want to create. The server type will specify the number of vcpu, memory, disk size and price per hour of the openstack server that will be created:</p>

<p><img src="/assets/08082018-18-select-instance-type.png" alt="screenshot"></p>

<p>Select the ssh key you created in the previous step. Click the &#39;Launch now&#39; button to create the server.</p>

<p><img src="/assets/08082018-19-launch-instance.png" alt="screenshot"></p>

<p>Once the server is created a &#39;Login information&#39; window will be displayed with the ssh username and ip address you will need to connect to the server.</p>

<p><img src="/assets/08082018-20-login-info.png" alt="screenshot"></p>

<h2>8. SSH to the new openstack server,</h2>

<p>You can now use the login infomation provided in the last step along with the private ssh key you generated to connect to your new server. I used the following commands to ssh from my Mac laptop to connect.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">ssh -i /Users/standorsett/.ssh/id_rsa-ovhus_public_cloud centos@147.135.76.67</code></pre></figure>

<ul>
<li>The <code>-i</code> option is used to specify the ssh private key you generated earlier.</li>
</ul>

<p>Here is the output from my running of this command.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">MacBook-Pro:~ standorsett$ ssh -i /Users/standorsett/.ssh/id_rsa-ovhus_public_cloud centos@147.135.76.67
The authenticity of host &#39;147.135.76.67 (147.135.76.67)&#39; can&#39;t be established.
ECDSA key fingerprint is SHA256:aVSsv0mHKgIizglcvouJkvtBBOiZfNcQTpIc5oxhNO4.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added &#39;147.135.76.67&#39; (ECDSA) to the list of known hosts.
[centos@server-1 ~]$</code></pre></figure>

<p>You can now run what commands you would like on your new openstack server, like checking the IP address of the server you created.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[centos@server-1 ~]$ ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether fa:16:3e:42:97:c4 brd ff:ff:ff:ff:ff:ff
    inet 147.135.76.67/32 brd 147.135.76.67 scope global dynamic eth0
       valid_lft 86350sec preferred_lft 86350sec
    inet6 fe80::f816:3eff:fe42:97c4/64 scope link
       valid_lft forever preferred_lft forever
[centos@server-1 ~]$</code></pre></figure>

<h2>9. Destroy the openstack server.</h2>

<p>Once you are finished with your openstack server, you should delete the server in order to stop being charged for it&#39;s usage. Click the drop down on the server and select &#39;delete&#39; to delete the server.</p>

<p><img src="/assets/08082018-21-instance-options.png" alt="screenshot"></p>

<p>Click &#39;Confirm&#39; to confirm that you want to delete the server. This is your last chance to reconsider if you really want to delete it.</p>

<p><img src="/assets/08082018-22-confirm-delete.png" alt="screenshot"></p>

<p>Your cloud project will now be back to not showing any servers.</p>

<p><img src="/assets/08082018-23-empty-cloud-project.png" alt="screenshot"></p>

<hr>

<h3>That all for this initial post covering how to sign up and get started using the OVH public cloud.</h3>

<h3>Please provide any feedback or suggestions to my twitter account located on the about page.</h3>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2015/12/28/scripted-packer-build-and-export/">
        Scripted Packer build, ovftool export and Vagrant .box file creation
      </a>
    </h1>

    <span class="post-date">28 Dec 2015</span>

    <p>This is the seventh in a series of posts on <a href="https://sdorsett.github.io/2015/12/22/pipeline-for-creating-packer-box-files/">using a Packer pipeline to generate Vagrant .box files</a>.</p>

<p>In the last post we covered <a href="https://sdorsett.github.io/2015/12/27/using-ovftool-to-export-packer-generated-virtual-machines/">using ovftool to convert Packer generated virtual machines into Vagrant .box files</a>. I promised to show you a better way of exporting and creating the Vagrant .box files, so in this post we will be combining the following items in one script:</p>

<ul>
<li>Kicking off the Packer build of a specific template</li>
<li>Exporting the Packer generated virtual machine</li>
<li>Creating the necessary metadata.json &amp; Vagrantfile files</li>
<li>Compressing the files into a TAR file with the .box extension</li>
<li>Copying the Vagrant .box files to /vat/www/html/box-files/ so they can accessed by HTTP</li>
</ul>

<p>Let&#39;s get started...</p>

<h2>1. Start off by connecting by SSH to CentOS virtual machine we have been using in previous posts.</h2>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">sdorsett-mbp:~ sdorsett$ ssh root@192.168.1.52
root@192.168.1.52&#39;s password:
Last login: Sat Dec 26 22:04:20 2015 from 192.168.1.163
[root@packer-centos ~]#</code></pre></figure>

<h2>2. Create a new directory, in the packer-templates directory, for storing build scripts.</h2>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@packer-centos ~]# mkdir ~/packer-templates/build-scripts
[root@packer-centos ~]# cd packer-templates/build-scripts/
[root@packer-centos build-scripts]#</code></pre></figure>

<h2>3. Next we will create a generic script for performing a Packer build of a template.</h2>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@packer-centos build-scripts]# cat generic-packer-build-script.sh
#!/bin/bash

source /root/.bashrc

echo &quot;starting packer build of $PACKER_VM_NAME&quot;
packer build -var-file=packer-remote-info.json /root/packer-templates/templates/$PACKER_VM_NAME.json

echo &quot;registering ${PACKER_VM_NAME} virtual machine on ${PACKER_REMOTE_HOST}&quot;
/usr/bin/sshpass -p ${PACKER_REMOTE_PASSWORD} ssh root@${PACKER_REMOTE_HOST} &quot;vim-cmd solo/registervm /vmfs/volumes/${PACKER_REMOTE_DATASTORE}/output-${PACKER_VM_NAME}/*.vmx&quot;

mkdir -p /root/box_files/ovf/empty_dir/
mkdir -p /root/box_files/vmx/empty_dir/

rm -rf /root/box_files/ovf/${PACKER_VM_NAME}
rm -rf /root/box_files/vmx/${PACKER_VM_NAME}

echo &quot;output of /vmfs/volumes/${PACKER_REMOTE_DATASTORE}/output-${PACKER_VM_NAME}/*.vmxf:&quot;
/usr/bin/sshpass -p ${PACKER_REMOTE_PASSWORD} ssh root@${PACKER_REMOTE_HOST} &quot;cat /vmfs/volumes/${PACKER_REMOTE_DATASTORE}/output-${PACKER_VM_NAME}/*.vmxf&quot;

ovftool vi://root:${PACKER_REMOTE_PASSWORD}@${PACKER_REMOTE_HOST}/${PACKER_VM_NAME} /root/box_files/ovf/
ovftool -tt=vmx vi://root:${PACKER_REMOTE_PASSWORD}@${PACKER_REMOTE_HOST}/${PACKER_VM_NAME} /root/box_files/vmx/

echo &quot;creating metadata.json and Vagrantfile files in ovf virtual machine directory&quot;
echo &#39;{&quot;provider&quot;:&quot;vmware_ovf&quot;}&#39; &gt;&gt; /root/box_files/ovf/${PACKER_VM_NAME}/metadata.json
touch /root/box_files/ovf/${PACKER_VM_NAME}/Vagrantfile
cd /root/box_files/ovf/empty_dir/
cd /root/box_files/ovf/${PACKER_VM_NAME}/

echo &quot;compressing ovf virtual machine files to /var/www/html/box-files/${PACKER_VM_NAME}-vmware_ovf-1.0.box&quot;
tar cvzf /var/www/html/box-files/$PACKER_VM_NAME-vmware_ovf-1.0.box ./*

echo &quot;creating metadata.json and Vagrantfile files in vmx virtual machine directory&quot;
echo &#39;{&quot;provider&quot;:&quot;vmware_desktop&quot;}&#39; &gt;&gt; /root/box_files/vmx/${PACKER_VM_NAME}/metadata.json
touch /root/box_files/vmx/${PACKER_VM_NAME}/Vagrantfile
cd /root/box_files/vmx/empty_dir/
cd /root/box_files/vmx/${PACKER_VM_NAME}/

echo &quot;compressing vmx virtual machine files to /var/www/html/box-files/${PACKER_VM_NAME}-vmware_desktop-1.0.box&quot;
tar cvzf /var/www/html/box-files/$PACKER_VM_NAME-vmware_desktop-1.0.box ./*

echo &quot;cleaning up /root/box_files directories&quot;
rm -rf /root/box_files/ovf/$PACKER_VM_NAME
rm -rf /root/box_files/vmx/$PACKER_VM_NAME

echo &quot;deleting $PACKER_VM_NAME from $PACKER_REMOTE_HOST&quot;
/usr/bin/sshpass -p ${PACKER_REMOTE_PASSWORD} ssh root@${PACKER_REMOTE_HOST}  &quot;vim-cmd vmsvc/getallvms | grep ${PACKER_VM_NAME} | cut -d &#39; &#39; -f 1 | xargs vim-cmd vmsvc/destroy&quot;

echo &quot;packer build of $PACKER_VM_NAME has been  completed&quot;

[root@packer-centos build-scripts]#</code></pre></figure>

<p>If you look over this script you will notice it covers all of the tasks we performed manually in the last two posts. You might all notice the following environmental variables listed:</p>

<ul>
<li>PACKER_REMOTE_HOST</li>
<li>PACKER_REMOTE_USERNAME</li>
<li>PACKER_REMOTE_PASSWORD</li>
<li>PACKER_REMOTE_DATASTORE</li>
<li>PACKER_VM_NAME</li>
</ul>

<p>We are again using the idea of &quot;separating data from code&quot; to keep environment specific information out of the scripts. This will help in keeping sensitive information from being store with our scripts in a code repository.</p>

<h2>4. We now need to added the above listed PACKER_REMOTE environmental variables into the .bashrc file, so the script will know how to connect to vCenter</h2>

<p>Add the following lines to the bottom of /root/.bashrc. Make sure to update any of them that do you match your environment:</p>

<pre>
PACKER_REMOTE_HOST=192.168.1.51
PACKER_REMOTE_USERNAME=root
PACKER_REMOTE_PASSWORD=password
PACKER_REMOTE_DATASTORE=datastore1
</pre>

<p>After updating the ~/.bashrc file, reread the file by running the following command:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@packer-centos build-scripts]# source ~/.bashrc
[root@packer-centos build-scripts]#</code></pre></figure>

<p>You can also validate the environmental variables exist by using the echo commmand:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@packer-centos build-scripts]# echo $PACKER_REMOTE_HOST
192.168.1.51
[root@packer-centos build-scripts]#</code></pre></figure>

<h2>5. We need to delete the existing Packer generated virtual machines from the ESXi virtual machine embedded host client, since the Packer build will halt if the virtual machine it is trying to build already exists:</h2>

<p><img src="/assets/01-delete-virtual-machine.png" alt="screenshot">  </p>

<h2>6. With the generic Packer build script and environmental variables in place we can test our script out. First off we need to ensure the execution bit is set on our generic Packer build script:</h2>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@packer-centos build-scripts]# cd ~/packer-templates/
[root@packer-centos packer-templates]# chmod +x build-scripts/generic-packer-build-script.sh
[root@packer-centos packer-templates]# ls -la build-scripts/generic-packer-build-script.sh
-rwxr-xr-x 1 root root 2091 Dec 26 22:27 build-scripts/generic-packer-build-script.sh
[root@packer-centos packer-templates]#</code></pre></figure>

<p>Now that we have set the script as executable, we can use it to build to centos67 template by passing in the PACKER<em>VM</em>NAME environmental variable like the following:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@packer-centos packer-templates]# PACKER_VM_NAME=centos67 build-scripts/generic-packer-build-script.sh
starting packer build of centos67
centos67 output will be in this color.

==&gt; centos67: Downloading or copying ISO
    centos67: Downloading or copying: file:///root/packer-templates/iso/CentOS-6.7-x86_64-minimal.iso
==&gt; centos67: Uploading ISO to remote machine...
==&gt; centos67: Creating virtual machine disk
==&gt; centos67: Building and writing VMX file
==&gt; centos67: Starting HTTP server on port 8001
==&gt; centos67: Registering remote VM...
==&gt; centos67: Starting virtual machine...
==&gt; centos67: Waiting 5s for boot...
==&gt; centos67: Connecting to VM via VNC
==&gt; centos67: Typing the boot command over VNC...
==&gt; centos67: Waiting for SSH to become available...
...</code></pre></figure>

<p>Checking the embedded host client we can see that the CentOS 6.7 operating system is being installed.</p>

<p><img src="/assets/02-centos67-os-install.png" alt="screenshot"></p>

<p>Once the Packer build completes you will see the following output:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">centos67: ==&gt; Zero out the free space to save space in the final image
centos67: dd: writing `/EMPTY&#39;: No space left on device
centos67: 12950+0 records in
centos67: 12949+0 records out
centos67: 13578776576 bytes (14 GB) copied, 462.514 s, 29.4 MB/s
==&gt; centos67: Gracefully halting virtual machine...
centos67: Waiting for VMware to clean up after itself...
==&gt; centos67: Deleting unnecessary VMware files...
centos67: Deleting: /vmfs/volumes/datastore1/output-centos67/vmware.log
==&gt; centos67: Cleaning VMX prior to finishing up...
centos67: Unmounting floppy from VMX...
centos67: Detaching ISO from CD-ROM device...
centos67: Disabling VNC server...
==&gt; centos67: Compacting the disk image
==&gt; centos67: Unregistering virtual machine...
Build &#39;centos67&#39; finished.

==&gt; Builds finished. The artifacts of successful builds are:
--&gt; centos67: VM files in directory: /vmfs/volumes/datastore1/output-centos67</code></pre></figure>

<p>At this point the script will re-register the vm Packer built and output the .vmxf file of the virtual machine. This is use for ensuring vmtools was properly installed, since this file contains the vmtool components and their versions that were successfully installed:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">registering centos67 virtual machine on 192.168.1.51
18
output of /vmfs/volumes/datastore1/output-centos67/*.vmxf:
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;Foundry&gt;&lt;VM/&gt;&lt;tools-install-info&gt;&lt;installError&gt;0&lt;/installError&gt;&lt;updateCounter&gt;1&lt;/updateCounter&gt;&lt;/tools-install-info&gt;&lt;tools-manifest&gt;&lt;monolithic version=&quot;9.9.4&quot;/&gt;&lt;svga33 version=&quot;10.3.0.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;svga4 version=&quot;10.4.0.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;vmmouse42 version=&quot;1.0.0.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;svga42 version=&quot;10.10.2.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;vmmouse43 version=&quot;12.6.4.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;svga43 version=&quot;10.16.7.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;vmmouse43_64 version=&quot;12.6.4.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;svga43_64 version=&quot;10.16.7.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;vmmouse67 version=&quot;12.6.4.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;svga67 version=&quot;10.16.7.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;vmmouse67_64 version=&quot;12.6.4.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;svga67_64 version=&quot;10.16.7.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;vmmouse68 version=&quot;12.6.4.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;svga68 version=&quot;10.16.7.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;vmmouse68_64 version=&quot;12.6.4.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;svga68_64 version=&quot;10.16.7.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;vmmouse70 version=&quot;12.7.0.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;svga70 version=&quot;11.0.99.4&quot; installed=&quot;FALSE&quot;/&gt;&lt;vmmouse70_64 version=&quot;12.7.0.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;svga70_64 version=&quot;11.0.99.4&quot; installed=&quot;FALSE&quot;/&gt;&lt;vmmouse71 version=&quot;12.7.0.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;svga71 version=&quot;11.0.99.4&quot; installed=&quot;FALSE&quot;/&gt;&lt;vmmouse71_64 version=&quot;12.7.0.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;svga71_64 version=&quot;11.0.99.4&quot; installed=&quot;FALSE&quot;/&gt;&lt;vmmouse73 version=&quot;12.7.0.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;svga73 version=&quot;11.0.99.4&quot; installed=&quot;FALSE&quot;/&gt;&lt;vmmouse73_64 version=&quot;12.7.0.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;svga73_64 version=&quot;11.0.99.4&quot; installed=&quot;FALSE&quot;/&gt;&lt;vmmouse73_99 version=&quot;12.7.0.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;svga73_99 version=&quot;11.0.99.4&quot; installed=&quot;FALSE&quot;/&gt;&lt;vmmouse73_99_64 version=&quot;12.7.0.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;svga73_99_64 version=&quot;11.0.99.4&quot; installed=&quot;FALSE&quot;/&gt;&lt;vmmouse74 version=&quot;12.7.0.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;svga74 version=&quot;11.0.99.4&quot; installed=&quot;FALSE&quot;/&gt;&lt;vmmouse74_64 version=&quot;12.7.0.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;svga74_64 version=&quot;11.0.99.4&quot; installed=&quot;FALSE&quot;/&gt;&lt;vmmouse75 version=&quot;12.7.0.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;svga75 version=&quot;11.0.99.4&quot; installed=&quot;FALSE&quot;/&gt;&lt;vmmouse75_64 version=&quot;12.7.0.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;svga75_64 version=&quot;11.0.99.4&quot; installed=&quot;FALSE&quot;/&gt;&lt;vmmouse76 version=&quot;12.7.0.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;svga76 version=&quot;11.0.99.4&quot; installed=&quot;FALSE&quot;/&gt;&lt;vmmouse76_64 version=&quot;12.7.0.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;svga76_64 version=&quot;11.0.99.4&quot; installed=&quot;FALSE&quot;/&gt;&lt;checkvm version=&quot;9.9.4.51858&quot; installed=&quot;TRUE&quot;/&gt;&lt;vmtoolsd version=&quot;9.9.4.51858&quot; installed=&quot;TRUE&quot;/&gt;&lt;upgrader version=&quot;9.9.4.51858&quot; installed=&quot;FALSE&quot;/&gt;&lt;hgfsclient version=&quot;9.9.4.51858&quot; installed=&quot;TRUE&quot;/&gt;&lt;hgfsmounter version=&quot;9.9.4.51858&quot; installed=&quot;TRUE&quot;/&gt;&lt;vmguestlib version=&quot;9.9.4.51858&quot; installed=&quot;TRUE&quot;/&gt;&lt;vmguestlibjava version=&quot;9.9.4.51858&quot; installed=&quot;TRUE&quot;/&gt;&lt;toolbox-cmd version=&quot;9.9.4.51858&quot; installed=&quot;TRUE&quot;/&gt;&lt;vmci version=&quot;9.6.2.0&quot; installed=&quot;TRUE&quot;/&gt;&lt;vmhgfs version=&quot;1.4.20.1&quot; installed=&quot;TRUE&quot;/&gt;&lt;vmmemctl version=&quot;1.2.1.2&quot; installed=&quot;FALSE&quot;/&gt;&lt;vmsync version=&quot;1.1.0.1&quot; installed=&quot;FALSE&quot;/&gt;&lt;vmxnet version=&quot;2.1.0.0&quot; installed=&quot;TRUE&quot;/&gt;&lt;vmxnet3 version=&quot;1.3.0.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;vmblock version=&quot;1.1.2.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;vsock version=&quot;9.6.1.0&quot; installed=&quot;TRUE&quot;/&gt;&lt;pvscsi version=&quot;1.2.3.0&quot; installed=&quot;FALSE&quot;/&gt;&lt;/tools-manifest&gt;&lt;/Foundry&gt;</code></pre></figure>

<p>The next stage of the build script will run the ovftool commands to export .vmx and .ovf format virtual machines from ESXi.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">Opening VI source: vi://root@192.168.1.51:443/centos67
Opening OVF target: /root/box_files/ovf/
Writing OVF package: /root/box_files/ovf/centos67/centos67.ovf
Transfer Completed
Completed successfully
Opening VI source: vi://root@192.168.1.51:443/centos67
Opening VMX target: /root/box_files/vmx/
Writing VMX file: /root/box_files/vmx/centos67/centos67.vmx
Transfer Completed
Completed successfully</code></pre></figure>

<p>The next stage of the build script will create the metadata.json and Vagrantfiles needed for the Vagrant .box files. It will then TAR up all the virtual machine files contained in the ovf &amp; vmx folders. This stage will also save the TAR files to /var/www/html/box-files/ so that they will be available over http.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">creating metadata.json and Vagrantfile files in ovf virtual machine directory
compressing ovf virtual machine files to /var/www/html/box-files/centos67-vmware_ovf-1.0.box
./centos67-disk1.vmdk
./centos67.mf
./centos67.ovf
./metadata.json
./Vagrantfile
creating metadata.json and Vagrantfile files in vmx virtual machine directory
compressing vmx virtual machine files to /var/www/html/box-files/centos67-vmware_desktop-1.0.box
./centos67-disk1.vmdk
./centos67.vmx
./metadata.json
./Vagrantfile</code></pre></figure>

<p>The final section of the build script will remove the directories that ovftool generated in /root/box_files (to keep disk space usage to a minimum) and delete the Packer virtual machine from ESXi.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">cleaning up /root/box_files directories
deleting centos67 from 192.168.1.51
packer build of centos67 has been  completed
[root@packer-centos packer-templates]#</code></pre></figure>

<h2>6. We can now list the files in /var/www/html/box-files and see the two .box files generated by the script.</h2>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@packer-centos packer-templates]# ls -lah /var/www/html/box-files/
total 654M
drwxr-xr-x 2 root root 4.0K Dec 28 15:44 .
drwxr-xr-x 3 root root 4.0K Dec 23 18:09 ..
-rw-r--r-- 1 root root 319M Dec 28 15:46 centos67-vmware_desktop-1.0.box
-rw-r--r-- 1 root root 336M Dec 28 15:44 centos67-vmware_ovf-1.0.box
[root@packer-centos packer-templates]#</code></pre></figure>

<p>We can also open a browser and see the .box files the script generated.</p>

<p><img src="/assets/01-browse-vagrant-box-files.png" alt="screenshot">  </p>

<h3>If you would like to not have to manually create the build script covered in this post, you can clone down <a href="https://github.com/sdorsett/packer-templates/tree/adding-build-script">this github repository</a> by running the following command:</h3>

<pre>
git clone -b "adding-build-script" https://github.com/sdorsett/packer-templates.git
</pre>  

<p>If you cloned the packer-templates repo in the last post, you can pull down the updates by running the following command:</p>

<pre>
git fetch --all
git pull origin "adding-build-script"
</pre>  

<h3>This brings us to the end of this post. I hope I made good on my promise of showing an automated ways of converting the Packer templates to .box files. In the next post I think we should cover how to test our Vagrant .box files are functional by deploying them using Vagrant.</h3>

<h3>Please provide any feedback or suggestions to my twitter account located on the about page.</h3>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2015/12/27/using-ovftool-to-export-packer-generated-virtual-machines/">
        Using ovftool to convert Packer generated virtual machines into Vagrant .box files
      </a>
    </h1>

    <span class="post-date">27 Dec 2015</span>

    <p>This is the sixth in a series of posts on <a href="https://sdorsett.github.io/2015/12/22/pipeline-for-creating-packer-box-files/">using a Packer pipeline to generate Vagrant .box files</a>.</p>

<p>In the last post we covered <a href="https://sdorsett.github.io/2015/12/26/copy-our-existing-template-and-add-the-puppet-agent/">copying our existing CentOS 6.7 template and adding the Puppet agent</a> in order to generate a new Packer template. In this post we will be covering how to use ovftool to convert Packer generated virtual machines into Vagrant .box files. This post will be going over the manual steps on purpose, since I feel it will make more sense when we start to cover automating the steps that you can already performed by hand.</p>

<p>Let&#39;s get started...</p>

<h2>1. Start off by connecting by SSH to CentOS virtual machine we have been using in previous posts.</h2>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">sdorsett-mbp:~ sdorsett$ ssh root@192.168.1.52
root@192.168.1.52&#39;s password:
Last login: Sat Dec 26 20:52:17 2015 from 192.168.1.163
[root@packer-centos ~]#</code></pre></figure>

<h2>2. Create two new directories for downloading virtual machines, one for .ovf and one for .vmx virtual machine formats.</h2>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">root@packer-centos ~]# mkdir -p ~/box_files/{ovf,vmx}
[root@packer-centos ~]# tree ~/box_files/
/root/box_files/
├── ovf
└── vmx

2 directories, 0 files
[root@packer-centos ~]#</code></pre></figure>

<h2>3. Now that we have locations for our ovftool exported virtual machine, we need to re-register the Packer created virtual machines. SSH to your ESXi virtual machine and cd to the datastore that you created the Packer virtual machines on.</h2>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">sdorsett-mbp:Desktop sdorsett$ ssh root@192.168.1.51
Password:
The time and date of this login have been sent to the system logs.

VMware offers supported, powerful system administration tools.  Please
see www.vmware.com/go/sysadmintools for details.

The ESXi Shell can be disabled by an administrative user. See the
vSphere Security documentation for more information.
[root@packer-esxi:~] cd /vmfs/volumes/datastore1/
[root@packer-esxi:/vmfs/volumes/5679efd9-3e1658e4-f977-0050569b912b] ls output-*
output-centos67:
centos67-flat.vmdk  centos67.vmdk       centos67.vmx
centos67.nvram      centos67.vmsd       centos67.vmxf

output-centos67-pe-puppet-382:
centos67-pe-puppet-382-flat.vmdk  centos67-pe-puppet-382.vmsd
centos67-pe-puppet-382.nvram      centos67-pe-puppet-382.vmx
centos67-pe-puppet-382.vmdk       centos67-pe-puppet-382.vmxf
[root@packer-esxi:/vmfs/volumes/5679efd9-3e1658e4-f977-0050569b912b]</code></pre></figure>

<h2>4. We next need to register both of the .vmx files of the Packer virtual machines displayed above on the ESXi virtual machine.</h2>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@packer-esxi:/vmfs/volumes/] vim-cmd solo/registervm /vmfs
/volumes/datastore1/output-centos67/*.vmx
15
[root@packer-esxi:/vmfs/volumes/] vim-cmd solo/registervm /vmfs
/volumes/datastore1/output-centos67-pe-puppet-382/*.vmx
16
[root@packer-esxi:/vmfs/volumes/]</code></pre></figure>

<p>You will notice that each of the vim-cmd commands returned a number. That number is the vmid of the virtual machine that was registered. Also if you check the embedded host client, you will see those virtual machines we just registered with the vim-cmd command listed under &quot;Virtual Machines.&quot;</p>

<p><img src="/assets/01-registered-virtual-machines.png" alt="screenshot">  </p>

<h2>5. Once the Packer virtual machines have been registered, we can use ovftool on the CentOS virtual machine to export them as .ovf files.</h2>

<p>After the files are exported, we can then compress the .ovf files into a vmware_ovf compatible .box file. Using the vmware_ovf format will provide a generic .box file that can be deployed to vCenter, vCloud Director or vCloud Air Vagrant providers.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@packer-centos ~]# ovftool vi://root@192.168.1.51/centos67 /root/box_files/ovf/
Enter login information for source vi://192.168.1.51/
Username: root
Password: ********
Opening VI source: vi://root@192.168.1.51:443/centos67
Opening OVF target: /root/box_files/ovf/
Writing OVF package: /root/box_files/ovf/centos67/centos67.ovf
Transfer Completed
Completed successfully

[root@packer-centos ~]# ovftool vi://root@192.168.1.51/centos67-pe-puppet-382 /root/box_files/ovf/
Enter login information for source vi://192.168.1.51/
Username: root
Password: ********
Opening VI source: vi://root@192.168.1.51:443/centos67-pe-puppet-382
Opening OVF target: /root/box_files/ovf/
Writing OVF package: /root/box_files/ovf/centos67-pe-puppet-382/centos67-pe-puppet-382.ovf
Transfer Completed
Completed successfully

[root@packer-centos ~]#</code></pre></figure>

<h2>6. We can also use ovftool to export the Packer virtual machines in a .vmx format for use with VMware Fusion or Workstation:</h2>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@packer-centos ~]# ovftool -tt=vmx vi://root@192.168.1.51/centos67 /root/box_files/vmx/Enter login information for source vi://192.168.1.51/
Username: root
Password: ********
Opening VI source: vi://root@192.168.1.51:443/centos67
Opening VMX target: /root/box_files/vmx/
Writing VMX file: /root/box_files/vmx/centos67/centos67.vmx
Transfer Completed
Completed successfully

[root@packer-centos ~]# ovftool -tt=vmx vi://root@192.168.1.51/centos67-pe-puppet-382 /root/box_files/vmx/
Enter login information for source vi://192.168.1.51/
Username: root
Password: ********
Opening VI source: vi://root@192.168.1.51:443/centos67-pe-puppet-382
Opening VMX target: /root/box_files/vmx/
Writing VMX file: /root/box_files/vmx/centos67-pe-puppet-382/centos67-pe-puppet-382.vmx
Transfer Completed
Completed successfully

[root@packer-centos ~]#</code></pre></figure>

<h2>7. We will start with converting the .ovf exported templates.</h2>

<p>Each of the exported template directories will need a metadata.js and Vagrantfile created. After creating the metadata.js and Vagrantfile files, we will tar all of the files in each directory into a .box file.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@packer-centos ~]# cd /root/box_files/ovf/centos67
[root@packer-centos centos67]# echo &#39;{&quot;provider&quot;:&quot;vmware_ovf&quot;}&#39; &gt;&gt; metadata.json
[root@packer-centos centos67]# touch Vagrantfile
[root@packer-centos centos67]# tar cvzf /root/box_files/centos67-vmware_ovf-1.0.box ./*
./centos67-disk1.vmdk
./centos67.mf
./centos67.ovf
./metadata.json
./Vagrantfile

[root@packer-centos centos67]# cd /root/box_files/ovf/centos67-pe-puppet-382
[root@packer-centos centos67-pe-puppet-382]# echo &#39;{&quot;provider&quot;:&quot;vmware_ovf&quot;}&#39; &gt;&gt; metadata.json
[root@packer-centos centos67-pe-puppet-382]# touch Vagrantfile
[root@packer-centos centos67-pe-puppet-382]# tar cvzf /root/box_files/centos67-pe-puppet-382-vmware_ovf-1.0.box ./*
./centos67-pe-puppet-382-disk1.vmdk
./centos67-pe-puppet-382.mf
./centos67-pe-puppet-382.ovf
./metadata.json
./Vagrantfile

[root@packer-centos centos67-pe-puppet-382]#</code></pre></figure>

<h2>8. Next we will convert the .vmx exported templates.</h2>

<p>Each of the exported template directories will also need a slightly different metadata.js file created, Vagrantfile created and finally tar all of the files in each directory into a .box file.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@packer-centos centos67-pe-puppet-382]# cd /root/box_files/vmx/centos67
[root@packer-centos centos67]# echo &#39;{&quot;provider&quot;:&quot;vmware_desktop&quot;}&#39; &gt;&gt; metadata.json
[root@packer-centos centos67]# touch Vagrantfile
[root@packer-centos centos67]# tar cvzf /root/box_files/centos67-vmware_desktop-1.0.box ./*
./centos67-disk1.vmdk
./centos67.vmx
./metadata.json
./Vagrantfile

[root@packer-centos centos67]# cd /root/box_files/vmx/centos67-pe-puppet-382/
[root@packer-centos centos67-pe-puppet-382]# echo &#39;{&quot;provider&quot;:&quot;vmware_desktop&quot;}&#39; &gt;&gt; metadata.json
[root@packer-centos centos67-pe-puppet-382]# touch Vagrantfile
[root@packer-centos centos67-pe-puppet-382]# tar cvzf /root/box_files/centos67-pe-puppet-382-vmware_desktop-1.0.box ./*
./centos67-pe-puppet-382-disk1.vmdk
./centos67-pe-puppet-382.vmx
./metadata.json
./Vagrantfile

[root@packer-centos centos67-pe-puppet-382]#</code></pre></figure>

<h2>9. Finally we can use the tree command to see the overall directory structure of the .ovf and .vmx templates, as well as the list of the .box files:</h2>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@packer-centos centos67-pe-puppet-382]# tree /root/box_files/
/root/box_files/
├── centos67-pe-puppet-382-vmware_desktop-1.0.box
├── centos67-pe-puppet-382-vmware_ovf-1.0.box
├── centos67-vmware_desktop-1.0.box
├── centos67-vmware_ovf-1.0.box
├── ovf
│   ├── centos67
│   │   ├── centos67-disk1.vmdk
│   │   ├── centos67.mf
│   │   ├── centos67.ovf
│   │   ├── metadata.json
│   │   └── Vagrantfile
│   └── centos67-pe-puppet-382
│       ├── centos67-pe-puppet-382-disk1.vmdk
│       ├── centos67-pe-puppet-382.mf
│       ├── centos67-pe-puppet-382.ovf
│       ├── metadata.json
│       └── Vagrantfile
└── vmx
    ├── centos67
    │   ├── centos67-disk1.vmdk
    │   ├── centos67.vmx
    │   ├── metadata.json
    │   └── Vagrantfile
    └── centos67-pe-puppet-382
        ├── centos67-pe-puppet-382-disk1.vmdk
        ├── centos67-pe-puppet-382.vmx
        ├── metadata.json
        └── Vagrantfile

6 directories, 22 files
[root@packer-centos centos67-pe-puppet-382]#</code></pre></figure>

<h3>This brings us to the end of this post. Again I&#39;m sorry for this post not covering any automated ways of converting the Packer templates to .box files, but in the next post you&#39;ll learn how we can ease this manual pain with some  scripts.</h3>

<h3>Please provide any feedback or suggestions to my twitter account located on the about page.</h3>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2015/12/26/copy-our-existing-template-and-add-the-puppet-agent/">
        Copying our existing CentOS 6.7 template and adding the Puppet agent
      </a>
    </h1>

    <span class="post-date">26 Dec 2015</span>

    <p>This is the fifth in a series of posts on <a href="https://sdorsett.github.io/2015/12/22/pipeline-for-creating-packer-box-files/">using a Packer pipeline to generate Vagrant .box files</a>.</p>

<p>In the last post we covered <a href="https://sdorsett.github.io/2015/12/25/creating-a-packer-template-for-installing-centos-67/">creating a Packer template for installing CentOS 6.7 with vmtools</a>. In this post we will be basing a new Packer template on the one we created last post and add installing the Puppet Enterprise 3.8.2 agent.</p>

<p>This is an older version of the Puppet Enterprise agent, but it will let us create a Vagrant .box file that can be used for testing Puppet 3.x code.</p>

<p>Let&#39;s get started...</p>

<h2>1. Start off by connecting by SSH to CentOS virtual machine we have been using in previous posts.</h2>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">sdorsett-mbp:~ sdorsett$ ssh root@192.168.1.52
root@192.168.1.52&#39;s password:
Last login: Fri Dec 25 20:22:44 2015 from 192.168.1.253
[root@packer-centos ~]#</code></pre></figure>

<h2>2. Create a new bash script named centos-install-pe-puppet-382.sh at packer-templates/scripts/ to install the Puppet Enterprise 3.8.2 agent:</h2>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@packer-centos ~]# cd ~/packer-templates/scripts/
[root@packer-centos scripts]# cat centos-install-pe-puppet-382.sh
#!/bin/bash

VERSION=&#39;3.8.2&#39;
PKGNAME=&quot;puppet-enterprise-${VERSION}-el-6-x86_64&quot;
TARFILE=&quot;${PKGNAME}.tar.gz&quot;
URL=&quot;https://pm.puppetlabs.com/puppet-enterprise/${VERSION}/${TARFILE}&quot;

TMPDIR=&#39;/tmp&#39;
HOSTNAME=`hostname`
cd $TMPDIR
echo &quot;Fetching ${TARFILE}&quot;
[ -e $TARFILE ] || curl -fLO $URL
echo &quot;Extracting ${TARFILE}&quot;
[ -d $PKGNAME ] || tar -xf $TARFILE
cd $PKGNAME

cat &gt; agent.ans &lt;&lt;EOF
q_all_in_one_install=n
q_database_install=n
q_pe_database=n
q_puppetca_install=n
q_puppetdb_install=n
q_puppetmaster_install=n
q_puppet_enterpriseconsole_install=n
q_run_updtvpkg=n
q_continue_or_reenter_master_hostname=c
q_fail_on_unsuccessful_master_lookup=n
q_puppetagent_certname=$HOSTNAME
q_puppetagent_install=y
q_puppetagent_server=puppet
q_puppet_cloud_install=y
q_puppet_symlinks_install=y
q_vendor_packages_install=y
q_install=y
EOF

./puppet-enterprise-installer -a agent.ans

# Remove certname so the system will use host FQDN
sed -i &#39;/certname =/d&#39; /etc/puppetlabs/puppet/puppet.conf

# Symlink so puppet is in vagrant provisioner PATH
ln -s /opt/puppet/bin/facter /usr/bin/facter
ln -s /opt/puppet/bin/puppet /usr/bin/puppet

[root@packer-deploy scripts]#</code></pre></figure>

<h2>3. Copy the Packer template we created previously, as the starting point for a new CentOS 6.7 template that will have Puppet Enterprise 3.8.2 installed</h2>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@packer-centos ~]# cd ~/packer-templates/templates/
[root@packer-centos templates]# ls
centos67.json
[root@packer-centos templates]# cp centos67.json centos67-pe-puppet-382.json
[root@packer-centos templates]#
[root@packer-deploy scripts]#</code></pre></figure>

<h2>4. Update the provisioners blocker to include the &quot;scripts/centos-install-pe-puppet-382.sh&quot; bash script.</h2>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@packer-centos templates]# cat centos67-pe-puppet-382.json
{
  &quot;variables&quot;: {
    &quot;version&quot;: &quot;1.0&quot;
  },
  &quot;builders&quot;: [
    {
      &quot;name&quot;: &quot;centos67-pe-puppet-382&quot;,
      &quot;vm_name&quot;: &quot;centos67-pe-puppet-382&quot;,
      &quot;vmdk_name&quot;: &quot;centos67-pe-puppet-382&quot;,
      &quot;type&quot;: &quot;vmware-iso&quot;,
      &quot;communicator&quot;: &quot;ssh&quot;,
      &quot;ssh_pty&quot;: &quot;true&quot;,
      &quot;headless&quot;: false,
      &quot;disk_size&quot;: 16384,
      &quot;guest_os_type&quot;: &quot;rhel6-64&quot;,
      &quot;iso_url&quot;: &quot;./iso/CentOS-6.7-x86_64-minimal.iso&quot;,
      &quot;iso_checksum&quot;: &quot;2ed5ea551dffc3e4b82847b3cee1f6cd748e8071&quot;,
      &quot;iso_checksum_type&quot;: &quot;sha1&quot;,
      &quot;shutdown_command&quot;: &quot;echo &#39;vagrant&#39; | sudo -S /sbin/shutdown -P now&quot;,

      &quot;remote_host&quot;: &quot;&quot;,
      &quot;remote_datastore&quot;: &quot;&quot;,
      &quot;remote_username&quot;: &quot;&quot;,
      &quot;remote_password&quot;: &quot;&quot;,
      &quot;remote_type&quot;: &quot;esx5&quot;,
      &quot;ssh_username&quot;: &quot;root&quot;,
      &quot;ssh_password&quot;: &quot;vagrant&quot;,
      &quot;ssh_wait_timeout&quot;: &quot;60m&quot;,
      &quot;tools_upload_flavor&quot;: &quot;linux&quot;,
      &quot;http_directory&quot;: &quot;.&quot;,
      &quot;boot_wait&quot;: &quot;5s&quot;,
      &quot;vmx_data&quot;: {
        &quot;memsize&quot;: &quot;4096&quot;,
        &quot;numvcpus&quot;: &quot;2&quot;,
        &quot;ethernet0.networkName&quot;: &quot;&quot;,
        &quot;ethernet0.present&quot;: &quot;TRUE&quot;,
        &quot;ethernet0.startConnected&quot;: &quot;TRUE&quot;,
        &quot;ethernet0.virtualDev&quot;: &quot;e1000&quot;,
        &quot;ethernet0.addressType&quot;: &quot;generated&quot;,
        &quot;ethernet0.generatedAddressOffset&quot;: &quot;0&quot;,
        &quot;ethernet0.wakeOnPcktRcv&quot;: &quot;FALSE&quot;

      },
      &quot;vmx_data_post&quot;: {
        &quot;ide1:0.startConnected&quot;: &quot;FALSE&quot;,
        &quot;ide1:0.clientDevice&quot;: &quot;TRUE&quot;,
        &quot;ide1:0.fileName&quot;: &quot;emptyBackingString&quot;,
        &quot;ethernet0.virtualDev&quot;: &quot;vmxnet3&quot;
      },
      &quot;boot_command&quot;: [
        &quot;&lt;tab&gt; text ks=http://:/scripts/centos-6-kickstart.cfg&lt;enter&gt;&lt;wait&gt;&quot;
      ]
    }
  ],
  &quot;provisioners&quot;: [
    {
      &quot;type&quot;: &quot;file&quot;,
      &quot;source&quot;: &quot;iso/vmware-tools-linux.iso&quot;,
      &quot;destination&quot;: &quot;/tmp/vmware-tools-linux.iso&quot;
    },
    {
      &quot;type&quot;: &quot;shell&quot;,
      &quot;execute_command&quot;: &quot;echo &#39;vagrant&#39; |  sudo -E -S sh &#39;&#39;&quot;,
      &quot;script&quot;: &quot;scripts/centos-vagrant-settings.sh&quot;
    },
    {
      &quot;type&quot;: &quot;shell&quot;,
      &quot;execute_command&quot;: &quot;echo &#39;vagrant&#39; |  sudo -E -S sh &#39;&#39;&quot;,
      &quot;script&quot;: &quot;scripts/centos-vmware-tools_install.sh&quot;
    },
    {
      &quot;type&quot;: &quot;shell&quot;,
      &quot;execute_command&quot;: &quot;echo &#39;vagrant&#39; |  sudo -E -S sh &#39;&#39;&quot;,
      &quot;script&quot;: &quot;scripts/centos-install-pe-puppet-382.sh&quot;
    },
    {
      &quot;type&quot;: &quot;shell&quot;,
      &quot;execute_command&quot;: &quot;echo &#39;vagrant&#39; |  sudo -E -S sh &#39;&#39;&quot;,
      &quot;script&quot;: &quot;scripts/centos-vmware-cleanup.sh&quot;
    }
  ]
}</code></pre></figure>

<h2>5. The packer-templates directory should now contain the following directories and files:</h2>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@packer-centos packer-templates]# tree /root/packer-templates/
/root/packer-templates/
├── iso
│   ├── CentOS-6.7-x86_64-minimal.iso
│   ├── linux.iso
│   └── vmware-tools-linux.iso -&gt; ./linux.iso
├── packer_cache
├── packer-remote-info.json
├── README.md
├── scripts
│   ├── centos-6-kickstart.cfg
│   ├── centos-install-pe-puppet-382.sh
│   ├── centos-vagrant-settings.sh
│   ├── centos-vmware-cleanup.sh
│   └── centos-vmware-tools_install.sh
└── templates
    ├── centos67.json
    └── centos67-pe-puppet-382.json

4 directories, 12 files
[root@packer-centos packer-templates]#</code></pre></figure>

<h2>6. Push the changes we have made to a github repository</h2>

<p>During this step pushed the two new files to the <a href="https://github.com/sdorsett/packer-templates/tree/adding-second-packer-template">adding-second-packer-template branch</a> of the github repository I have created for this series of blog posts:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">[root@packer-centos packer-templates]# git status
# On branch first-packer-template
# Untracked files:
#   (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed)
#
#   scripts/centos-install-pe-puppet-382.sh
#   templates/centos67-pe-puppet-382.json
nothing added to commit but untracked files present (use &quot;git add&quot; to track)
[root@packer-centos packer-templates]# git add .
[root@packer-centos packer-templates]# git commit -m &quot;adding templates/centos67-pe-puppet-382.json and scripts/centos-install-pe-puppet-382.sh&quot;
[first-packer-template 75bb98e] adding templates/centos67-pe-puppet-382.json and scripts/centos-install-pe-puppet-382.sh
 2 files changed, 125 insertions(+), 0 deletions(-)
 create mode 100644 scripts/centos-install-pe-puppet-382.sh
 create mode 100644 templates/centos67-pe-puppet-382.json
[root@packer-centos packer-templates]# git checkout -b adding-second-packer-template
Switched to a new branch &#39;adding-second-packer-template&#39;
[root@packer-centos packer-templates]# git push origin adding-second-packer-template
Password:
Counting objects: 9, done.
Delta compression using up to 2 threads.
Compressing objects: 100% (6/6), done.
Writing objects: 100% (6/6), 1.95 KiB, done.
Total 6 (delta 1), reused 0 (delta 0)
To https://sdorsett@github.com/sdorsett/packer-templates.git
 * [new branch]      adding-second-packer-template -&gt; adding-second-packer-template
[root@packer-centos packer-templates]#</code></pre></figure>

<h2>13. Have Packer build the new template we created</h2>

<p>Now that we have updated the new template file and created the script file that will install the Puppet Enterprise 3.8.2 agent, we can test our build. In order to use the values in the packer-remote-info.json file, we will use the -var-file parameter to specify this file.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">root@packer-centos packer-templates]# packer build -var-file=./packer-remote-info.json templates/centos67-pe-puppet-382.json
centos67-pe-puppet-382 output will be in this color.

==&gt; centos67-pe-puppet-382: Downloading or copying ISO
    centos67-pe-puppet-382: Downloading or copying: file:///root/packer-templates/iso/CentOS-6.7-x86_64-minimal.iso
==&gt; centos67-pe-puppet-382: Uploading ISO to remote machine...
==&gt; centos67-pe-puppet-382: Creating virtual machine disk
==&gt; centos67-pe-puppet-382: Building and writing VMX file
==&gt; centos67-pe-puppet-382: Starting HTTP server on port 8019
==&gt; centos67-pe-puppet-382: Registering remote VM...
==&gt; centos67-pe-puppet-382: Starting virtual machine...
==&gt; centos67-pe-puppet-382: Waiting 7s for boot...
==&gt; centos67-pe-puppet-382: Connecting to VM via VNC
==&gt; centos67-pe-puppet-382: Typing the boot command over VNC...
==&gt; centos67-pe-puppet-382: Waiting for SSH to become available...</code></pre></figure>

<p>At this point you should see a centos67 virtual machine in the embedded host client of the ESXi virtual machine.</p>

<p><img src="/assets/01-centos-67-pe-puppet-382-virtual-machine.png" alt="screenshot">  </p>

<p>Once the CentOS 6.7 install completes and the virtual machine reboots, you will see the packer build output continue with the provisioners block of the Packer template.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">==&gt; centos67-pe-puppet-382: Connected to SSH!</code></pre></figure>

<p>The first provisioner is the file provisioner that will copy iso/vmware-tools-linux.iso to /tmp/vmware-tools-linux.iso within the CentOS 6.7 virtual machine.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">==&gt; centos67-pe-puppet-382: Uploading iso/vmware-tools-linux.iso =&gt; /tmp/vmware-tools-linux.iso</code></pre></figure>

<p>The second provisioner is a shell provisioner that will run the scripts/centos-vagrant-settings.sh bash script. This script will added the necessary changes for this Packer image to be used by Vagrant.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">==&gt; centos67-pe-puppet-382: Provisioning with shell script: scripts/centos-vagrant-settings.sh</code></pre></figure>

<p>The third provisioner is a shell provisioner that will run the scripts/centos-vmware-tools_install.sh bash script. This script will install all the needed dependencies for vmtools and then install vmtools using an answer file.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">==&gt; centos67-pe-puppet-382: Provisioning with shell script: scripts/centos-vmware-tools_install.sh
    centos67-pe-puppet-382: Loaded plugins: fastestmirror
    centos67-pe-puppet-382: Setting up Upgrade Process
    centos67-pe-puppet-382: Determining fastest mirrors
    centos67-pe-puppet-382: * base: centos.unixheads.org
    centos67-pe-puppet-382: * extras: mirror.rackspace.com
    centos67-pe-puppet-382: * updates: centos-mirror.jchost.net
    centos67-pe-puppet-382: No Packages marked for Update
    centos67-pe-puppet-382: Loaded plugins: fastestmirror
    centos67-pe-puppet-382: Setting up Install Process
    centos67-pe-puppet-382: Loading mirror speeds from cached hostfile
    centos67-pe-puppet-382: * base: centos.unixheads.org
    centos67-pe-puppet-382: * extras: mirror.rackspace.com
    centos67-pe-puppet-382: * updates: centos-mirror.jchost.net
    centos67-pe-puppet-382: Package 4:perl-5.10.1-141.el6_7.1.x86_64 already installed and latest version
    centos67-pe-puppet-382: Resolving Dependencies
    centos67-pe-puppet-382: --&gt; Running transaction check
    centos67-pe-puppet-382: ---&gt; Package fuse-libs.x86_64 0:2.8.3-4.el6 will be installed
    centos67-pe-puppet-382: --&gt; Finished Dependency Resolution
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Dependencies Resolved
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ========================================
    centos67-pe-puppet-382: Package  Arch   Version     Repository
    centos67-pe-puppet-382: Size
    centos67-pe-puppet-382: ========================================
    centos67-pe-puppet-382: Installing:
    centos67-pe-puppet-382: fuse-libs
    centos67-pe-puppet-382: x86_64 2.8.3-4.el6 base  74 k
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Transaction Summary
    centos67-pe-puppet-382: ========================================
    centos67-pe-puppet-382: Install       1 Package(s)
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Total download size: 74 k
    centos67-pe-puppet-382: Installed size: 251 k
    centos67-pe-puppet-382: Downloading Packages:
    centos67-pe-puppet-382: fuse-libs-2.8.3- |  74 kB     00:00
    centos67-pe-puppet-382: Running rpm_check_debug
    centos67-pe-puppet-382: Running Transaction Test
    centos67-pe-puppet-382: Transaction Test Succeeded
    centos67-pe-puppet-382: Running Transaction
    centos67-pe-puppet-382:   Installing : fuse-libs-2.8.3-4.   1/1
    centos67-pe-puppet-382: Verifying  : fuse-libs-2.8.3-4.   1/1
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Installed:
    centos67-pe-puppet-382: fuse-libs.x86_64 0:2.8.3-4.el6
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Complete!
    centos67-pe-puppet-382: total 488
    centos67-pe-puppet-382: drwxr-xr-x   7 root root   4096 Oct 30 07:10 .
    centos67-pe-puppet-382: drwxrwxrwt.  4 root root   4096 Dec 26 20:56 ..
    centos67-pe-puppet-382: drwxr-xr-x   2 root root   4096 Oct 30 07:10 bin
    centos67-pe-puppet-382: drwxr-xr-x   2 root root   4096 Oct 30 07:10 doc
    centos67-pe-puppet-382: drwxr-xr-x   5 root root   4096 Oct 30 07:10 etc
    centos67-pe-puppet-382: -rw-r--r--   1 root root 269196 Oct 30 07:10 FILES
    centos67-pe-puppet-382: -rw-r--r--   1 root root   2538 Oct 30 07:10 INSTALL
    centos67-pe-puppet-382: drwxr-xr-x   2 root root   4096 Oct 30 07:10 installer
    centos67-pe-puppet-382: drwxr-xr-x  15 root root   4096 Oct 30 07:10 lib
    centos67-pe-puppet-382: -rwxr-xr-x   1 root root 196237 Oct 30 07:10 vmware-install.pl
    centos67-pe-puppet-382: Creating a new VMware Tools installer database using the tar4 format.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Installing VMware Tools.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: In which directory do you want to install the binary files?
    centos67-pe-puppet-382: [/usr/bin]
    centos67-pe-puppet-382: The path &quot;yes&quot; is a relative path. Please enter an absolute path.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: In which directory do you want to install the binary files?
    centos67-pe-puppet-382: [/usr/bin]
    centos67-pe-puppet-382: What is the directory that contains the init directories (rc0.d/ to rc6.d/)?
    centos67-pe-puppet-382: [/etc/rc.d]
    centos67-pe-puppet-382: What is the directory that contains the init scripts?
    centos67-pe-puppet-382: [/etc/init.d]
    centos67-pe-puppet-382: In which directory do you want to install the daemon files?
    centos67-pe-puppet-382: [/usr/sbin]
    centos67-pe-puppet-382: In which directory do you want to install the library files?
    centos67-pe-puppet-382: [/usr/lib/vmware-tools]
    centos67-pe-puppet-382: The path &quot;/usr/lib/vmware-tools&quot; does not exist currently. This program is
    centos67-pe-puppet-382: going to create it, including needed parent directories. Is this what you want?
    centos67-pe-puppet-382: [yes]
    centos67-pe-puppet-382: In which directory do you want to install the documentation files?
    centos67-pe-puppet-382: [/usr/share/doc/vmware-tools]
    centos67-pe-puppet-382: The path &quot;/usr/share/doc/vmware-tools&quot; does not exist currently. This program
    centos67-pe-puppet-382: is going to create it, including needed parent directories. Is this what you
    centos67-pe-puppet-382: want? [yes]
    centos67-pe-puppet-382: The installation of VMware Tools 9.9.4 build-3193940 for Linux completed
    centos67-pe-puppet-382: successfully. You can decide to remove this software from your system at any
    centos67-pe-puppet-382: time by invoking the following command: &quot;/usr/bin/vmware-uninstall-tools.pl&quot;.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Before running VMware Tools for the first time, you need to configure it by
    centos67-pe-puppet-382: invoking the following command: &quot;/usr/bin/vmware-config-tools.pl&quot;. Do you want
    centos67-pe-puppet-382: this program to invoke the command for you now? [yes]
    centos67-pe-puppet-382: Initializing...
    centos67-pe-puppet-382:
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Making sure services for VMware Tools are stopped.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382:
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Found a compatible pre-built module for vmci.  Installing it...
    centos67-pe-puppet-382:
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Found a compatible pre-built module for vsock.  Installing it...
    centos67-pe-puppet-382:
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: The module vmxnet3 has already been installed on this system by another
    centos67-pe-puppet-382: installer or package and will not be modified by this installer.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: The module pvscsi has already been installed on this system by another
    centos67-pe-puppet-382: installer or package and will not be modified by this installer.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: The module vmmemctl has already been installed on this system by another
    centos67-pe-puppet-382: installer or package and will not be modified by this installer.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: The VMware Host-Guest Filesystem allows for shared folders between the host OS
    centos67-pe-puppet-382: and the guest OS in a Fusion or Workstation virtual environment.  Do you wish
    centos67-pe-puppet-382: to enable this feature? [no]
    centos67-pe-puppet-382: Found a compatible pre-built module for vmhgfs.  Installing it...
    centos67-pe-puppet-382:
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Found a compatible pre-built module for vmxnet.  Installing it...
    centos67-pe-puppet-382:
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: The vmblock enables dragging or copying files between host and guest in a
    centos67-pe-puppet-382: Fusion or Workstation virtual environment.  Do you wish to enable this feature?
    centos67-pe-puppet-382: [no]
    centos67-pe-puppet-382: VMware automatic kernel modules enables automatic building and installation of
    centos67-pe-puppet-382: VMware kernel modules at boot that are not already present. This feature can be
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: enabled/disabled by re-running vmware-config-tools.pl.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Would you like to enable VMware automatic kernel modules?
    centos67-pe-puppet-382: [no]
    centos67-pe-puppet-382: Do you want to enable Guest Authentication (vgauth)? [yes]
    centos67-pe-puppet-382: No X install found.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Creating a new initrd boot image for the kernel.
    centos67-pe-puppet-382: vmware-tools-thinprint start/running
    centos67-pe-puppet-382: vmware-tools start/running
    centos67-pe-puppet-382: The configuration of VMware Tools 9.9.4 build-3193940 for Linux for this
    centos67-pe-puppet-382: running kernel completed successfully.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: You must restart your X session before any mouse or graphics changes take
    centos67-pe-puppet-382: effect.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: You can now run VMware Tools by invoking &quot;/usr/bin/vmware-toolbox-cmd&quot; from the
    centos67-pe-puppet-382: command line.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: To enable advanced X features (e.g., guest resolution fit, drag and drop, and
    centos67-pe-puppet-382: file and text copy/paste), you will need to do one (or more) of the following:
    centos67-pe-puppet-382: 1. Manually start /usr/bin/vmware-user
    centos67-pe-puppet-382: 2. Log out and log back into your desktop session; and,
    centos67-pe-puppet-382: 3. Restart your X session.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Enjoy,
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: --the VMware team
    centos67-pe-puppet-382:</code></pre></figure>

<p>The fourth provisioner is the shell provisioner that will run the scripts/centos-install-pe-puppet-382.sh bash script. This is the script we added to the Packer template to install the Puppet Enterprise 3.8.2 agent.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">==&gt; centos67-pe-puppet-382: Provisioning with shell script: scripts/centos-install-pe-puppet-382.sh
    centos67-pe-puppet-382: Fetching puppet-enterprise-3.8.2-el-6-x86_64.tar.gz
    centos67-pe-puppet-382: % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
    centos67-pe-puppet-382: Dload  Upload   Total   Spent    Left  Speed
    centos67-pe-puppet-382: 100  437M  100  437M    0     0  6688k      0  0:01:06  0:01:06 --:--:-- 6879k
    centos67-pe-puppet-382: Extracting puppet-enterprise-3.8.2-el-6-x86_64.tar.gz
    centos67-pe-puppet-382: ========================================================================
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Puppet Enterprise v3.8.2 installer
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Puppet Enterprise documentation can be found at http://docs.puppetlabs.com/pe/3.8/
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ------------------------------------------------------------------------
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: STEP 1: READ ANSWERS FROM FILE
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ## Reading answers from file: ./agent.ans
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ------------------------------------------------------------------------
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: STEP 2: SELECT AND CONFIGURE ROLES
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: This installer lets you select and install the various roles
    centos67-pe-puppet-382: required in a Puppet Enterprise deployment: puppet master,
    centos67-pe-puppet-382: console, database, cloud provisioner, and puppet agent.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: NOTE: when specifying hostnames during installation, use the fully-qualified domain name (foo.example.com) rather than a shortened name (foo).
    centos67-pe-puppet-382:
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: -&gt; puppet master
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: The puppet master serves configurations to a group of puppet
    centos67-pe-puppet-382: agent nodes. This role also provides MCollective&#39;s message queue
    centos67-pe-puppet-382: and client interface. It should be installed on a robust,
    centos67-pe-puppet-382: dedicated server.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ?? Install puppet master? [y/N] n
    centos67-pe-puppet-382: ?? Puppet master hostname to connect to? [Default: puppet] puppet
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: -&gt; database support
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: This role provides database support for PuppetDB and PE&#39;s
    centos67-pe-puppet-382: console. PuppetDB is a centralized data service that caches data
    centos67-pe-puppet-382: generated by Puppet and provides access to it via a robust API.
    centos67-pe-puppet-382: The console uses data provided by a PostgreSQL server and
    centos67-pe-puppet-382: database both of which will be installed along with PuppetDB on
    centos67-pe-puppet-382: the node you specify.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: IMPORTANT: If you choose not to install PuppetDB at this time,
    centos67-pe-puppet-382: you will be prompted for the host name of the node you intend to
    centos67-pe-puppet-382: use to provide database services. Note that you must install
    centos67-pe-puppet-382: database support on that node for the console to function. When
    centos67-pe-puppet-382: using a separate node, you should install database support on it
    centos67-pe-puppet-382: BEFORE installing the console role.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ?? Install PuppetDB? [y/N] n
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: -&gt; console
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: The console is a web interface where you can view reports,
    centos67-pe-puppet-382: classify nodes, control Puppet runs, and invoke MCollective
    centos67-pe-puppet-382: agents. It can be installed on the puppet master&#39;s node, but for
    centos67-pe-puppet-382: performance considerations, especially in larger deployments, it
    centos67-pe-puppet-382: can also be installed on a separate node.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ?? Install the console? [y/N] n
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: -&gt; cloud provisioner
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: The cloud provisioner can create and bootstrap new machine
    centos67-pe-puppet-382: instances and add them to your Puppet infrastructure. It should
    centos67-pe-puppet-382: be installed on a trusted node where site administrators have
    centos67-pe-puppet-382: shell access.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ?? Install the cloud provisioner? [y/N] y
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: -&gt; puppet agent
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: The puppet agent role is automatically installed with the
    centos67-pe-puppet-382: console, puppet master, puppetdb, and cloud provisioner roles.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ?? Puppet agent needs a unique name (&quot;certname&quot;) for its
    centos67-pe-puppet-382: certificate; this can be an arbitrary string. Certname for this
    centos67-pe-puppet-382: node? [Default: localhost] localhost.localdomain
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: -&gt; Vendor Packages
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: The installer has detected that Puppet Enterprise requires
    centos67-pe-puppet-382: additional packages from your operating system vendor&#39;s
    centos67-pe-puppet-382: repositories, and can automatically install them. If you choose
    centos67-pe-puppet-382: not to install these packages automatically, the installer will
    centos67-pe-puppet-382: exit so you can install them manually.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Additional vendor packages required for installation:
    centos67-pe-puppet-382: * dmidecode
    centos67-pe-puppet-382: * libxslt
    centos67-pe-puppet-382: * pciutils
    centos67-pe-puppet-382:
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ?? Install these packages automatically? [Y/n] y
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ------------------------------------------------------------------------
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: STEP 3: CONFIRM PLAN
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: You have selected to install the following components (and their dependencies)
    centos67-pe-puppet-382: * Cloud Provisioner
    centos67-pe-puppet-382: * Puppet Agent
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ?? Perform installation? [Y/n] y
    centos67-pe-puppet-382: ## Answers saved in the following files: /tmp/puppet-enterprise-3.8.2-el-6-x86_64/answers.lastrun.localhost and /etc/puppetlabs/installer/answers.install
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ========================================================================
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ------------------------------------------------------------------------
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: STEP 4: INSTALL PACKAGES
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ## Installing packages from repositories...
    centos67-pe-puppet-382: Loaded plugins: fastestmirror
    centos67-pe-puppet-382: Setting up Install Process
    centos67-pe-puppet-382: Loading mirror speeds from cached hostfile
    centos67-pe-puppet-382: * base: centos.unixheads.org
    centos67-pe-puppet-382: * extras: mirror.rackspace.com
    centos67-pe-puppet-382: * updates: centos-mirror.jchost.net
    centos67-pe-puppet-382: Package zlib-1.2.3-29.el6.x86_64 already installed and latest version
    centos67-pe-puppet-382: Package which-2.19-6.el6.x86_64 already installed and latest version
    centos67-pe-puppet-382: Package net-tools-1.60-110.el6_2.x86_64 already installed and latest version
    centos67-pe-puppet-382: Resolving Dependencies
    centos67-pe-puppet-382: --&gt; Running transaction check
    centos67-pe-puppet-382: ---&gt; Package cronie.x86_64 0:1.4.4-15.el6 will be updated
    centos67-pe-puppet-382: --&gt; Processing Dependency: cronie = 1.4.4-15.el6 for package: cronie-anacron-1.4.4-15.el6.x86_64
    centos67-pe-puppet-382: ---&gt; Package cronie.x86_64 0:1.4.4-15.el6_7.1 will be an update
    centos67-pe-puppet-382: ---&gt; Package dmidecode.x86_64 1:2.12-6.el6 will be installed
    centos67-pe-puppet-382: ---&gt; Package libxml2.x86_64 0:2.7.6-20.el6 will be updated
    centos67-pe-puppet-382: ---&gt; Package libxml2.x86_64 0:2.7.6-20.el6_7.1 will be an update
    centos67-pe-puppet-382: ---&gt; Package libxslt.x86_64 0:1.1.26-2.el6_3.1 will be installed
    centos67-pe-puppet-382: ---&gt; Package pciutils.x86_64 0:3.1.10-4.el6 will be installed
    centos67-pe-puppet-382: ---&gt; Package pe-agent.noarch 0:3.8.2-1.pe.el6 will be installed
    centos67-pe-puppet-382: --&gt; Processing Dependency: pe-virt-what &gt;= 1.13-1.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&gt; Processing Dependency: pe-rubygem-deep-merge &gt;= 1.0.0-3.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&gt; Processing Dependency: pe-ruby-stomp &gt;= 1.3.3-1.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&gt; Processing Dependency: pe-ruby-shadow &gt;= 2.2.0-3.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&gt; Processing Dependency: pe-ruby-selinux &gt;= 2.0.94-4.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&gt; Processing Dependency: pe-ruby-rgen &gt;= 0.6.5-1.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&gt; Processing Dependency: pe-ruby-augeas &gt;= 0.5.0-7.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&gt; Processing Dependency: pe-ruby &gt;= 1.9.3.551-2.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&gt; Processing Dependency: pe-puppet-enterprise-release &gt;= 3.8.2.0-1.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&gt; Processing Dependency: pe-puppet &gt;= 3.8.2.0-1.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&gt; Processing Dependency: pe-openssl &gt;= 1.0.0s-1.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&gt; Processing Dependency: pe-mcollective-common &gt;= 2.7.0.1-1.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&gt; Processing Dependency: pe-mcollective &gt;= 2.7.0.1-1.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&gt; Processing Dependency: pe-libyaml &gt;= 0.1.6-5.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&gt; Processing Dependency: pe-libldap &gt;= 2.4.39-5.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&gt; Processing Dependency: pe-hiera &gt;= 1.3.4.5-1.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&gt; Processing Dependency: pe-facter &gt;= 2.4.4.0-1.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: --&gt; Processing Dependency: pe-augeas &gt;= 1.3.0-2.pe.el6 for package: pe-agent-3.8.2-1.pe.el6.noarch
    centos67-pe-puppet-382: ---&gt; Package pe-cloud-provisioner.noarch 0:1.2.0-1.el6 will be installed
    centos67-pe-puppet-382: ---&gt; Package pe-cloud-provisioner-libs.x86_64 0:0.3.2-2.pe.el6 will be installed
    centos67-pe-puppet-382: ---&gt; Package pe-ruby-ldap.x86_64 0:0.9.12-7.pe.el6 will be installed
    centos67-pe-puppet-382: ---&gt; Package pe-rubygem-net-ssh.noarch 0:2.1.4-2.pe.el6 will be installed
    centos67-pe-puppet-382: --&gt; Running transaction check
    centos67-pe-puppet-382: ---&gt; Package cronie-anacron.x86_64 0:1.4.4-15.el6 will be updated
    centos67-pe-puppet-382: ---&gt; Package cronie-anacron.x86_64 0:1.4.4-15.el6_7.1 will be an update
    centos67-pe-puppet-382: ---&gt; Package pe-augeas.x86_64 0:1.3.0-2.pe.el6 will be installed
    centos67-pe-puppet-382: ---&gt; Package pe-facter.x86_64 0:2.4.4.0-1.pe.el6 will be installed
    centos67-pe-puppet-382: ---&gt; Package pe-hiera.noarch 0:1.3.4.5-1.pe.el6 will be installed
    centos67-pe-puppet-382: ---&gt; Package pe-libldap.x86_64 0:2.4.39-5.pe.el6 will be installed
    centos67-pe-puppet-382: ---&gt; Package pe-libyaml.x86_64 0:0.1.6-5.el6 will be installed
    centos67-pe-puppet-382: ---&gt; Package pe-mcollective.noarch 0:2.7.0.1-1.pe.el6 will be installed
    centos67-pe-puppet-382: ---&gt; Package pe-mcollective-common.noarch 0:2.7.0.1-1.pe.el6 will be installed
    centos67-pe-puppet-382: ---&gt; Package pe-openssl.x86_64 0:1.0.0s-1.pe.el6 will be installed
    centos67-pe-puppet-382: ---&gt; Package pe-puppet.noarch 0:3.8.2.0-1.pe.el6 will be installed
    centos67-pe-puppet-382: ---&gt; Package pe-puppet-enterprise-release.noarch 0:3.8.2.0-1.pe.el6 will be installed
    centos67-pe-puppet-382: ---&gt; Package pe-ruby.x86_64 0:1.9.3.551-2.pe.el6 will be installed
    centos67-pe-puppet-382: ---&gt; Package pe-ruby-augeas.x86_64 0:0.5.0-7.pe.el6 will be installed
    centos67-pe-puppet-382: ---&gt; Package pe-ruby-rgen.noarch 0:0.6.5-1.pe.el6 will be installed
    centos67-pe-puppet-382: ---&gt; Package pe-ruby-selinux.x86_64 0:2.0.94-4.pe.el6 will be installed
    centos67-pe-puppet-382: ---&gt; Package pe-ruby-shadow.x86_64 0:2.2.0-3.pe.el6 will be installed
    centos67-pe-puppet-382: ---&gt; Package pe-ruby-stomp.noarch 0:1.3.3-1.pe.el6 will be installed
    centos67-pe-puppet-382: ---&gt; Package pe-rubygem-deep-merge.noarch 0:1.0.0-3.pe.el6 will be installed
    centos67-pe-puppet-382: ---&gt; Package pe-virt-what.x86_64 0:1.13-1.el6 will be installed
    centos67-pe-puppet-382: --&gt; Finished Dependency Resolution
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Dependencies Resolved
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ================================================================================
    centos67-pe-puppet-382: Package            Arch   Version            Repository                   Size
    centos67-pe-puppet-382: ================================================================================
    centos67-pe-puppet-382: Installing:
    centos67-pe-puppet-382: dmidecode          x86_64 1:2.12-6.el6       base                         74 k
    centos67-pe-puppet-382: libxslt            x86_64 1.1.26-2.el6_3.1   base                        452 k
    centos67-pe-puppet-382: pciutils           x86_64 3.1.10-4.el6       base                         85 k
    centos67-pe-puppet-382: pe-agent           noarch 3.8.2-1.pe.el6     puppet-enterprise-installer 3.7 k
    centos67-pe-puppet-382: pe-cloud-provisioner
    centos67-pe-puppet-382: noarch 1.2.0-1.el6        puppet-enterprise-installer  51 k
    centos67-pe-puppet-382: pe-cloud-provisioner-libs
    centos67-pe-puppet-382: x86_64 0.3.2-2.pe.el6     puppet-enterprise-installer 4.6 M
    centos67-pe-puppet-382: pe-ruby-ldap       x86_64 0.9.12-7.pe.el6    puppet-enterprise-installer  46 k
    centos67-pe-puppet-382: pe-rubygem-net-ssh noarch 2.1.4-2.pe.el6     puppet-enterprise-installer 227 k
    centos67-pe-puppet-382: Updating:
    centos67-pe-puppet-382: cronie             x86_64 1.4.4-15.el6_7.1   updates                      74 k
    centos67-pe-puppet-382: libxml2            x86_64 2.7.6-20.el6_7.1   updates                     803 k
    centos67-pe-puppet-382: Installing for dependencies:
    centos67-pe-puppet-382: pe-augeas          x86_64 1.3.0-2.pe.el6     puppet-enterprise-installer 523 k
    centos67-pe-puppet-382: pe-facter          x86_64 2.4.4.0-1.pe.el6   puppet-enterprise-installer  99 k
    centos67-pe-puppet-382: pe-hiera           noarch 1.3.4.5-1.pe.el6   puppet-enterprise-installer  18 k
    centos67-pe-puppet-382: pe-libldap         x86_64 2.4.39-5.pe.el6    puppet-enterprise-installer 776 k
    centos67-pe-puppet-382: pe-libyaml         x86_64 0.1.6-5.el6        puppet-enterprise-installer 193 k
    centos67-pe-puppet-382: pe-mcollective     noarch 2.7.0.1-1.pe.el6   puppet-enterprise-installer 8.0 k
    centos67-pe-puppet-382: pe-mcollective-common
    centos67-pe-puppet-382: noarch 2.7.0.1-1.pe.el6   puppet-enterprise-installer 129 k
    centos67-pe-puppet-382: pe-openssl         x86_64 1.0.0s-1.pe.el6    puppet-enterprise-installer 6.7 M
    centos67-pe-puppet-382: pe-puppet          noarch 3.8.2.0-1.pe.el6   puppet-enterprise-installer 1.6 M
    centos67-pe-puppet-382: pe-puppet-enterprise-release
    centos67-pe-puppet-382: noarch 3.8.2.0-1.pe.el6   puppet-enterprise-installer  12 k
    centos67-pe-puppet-382: pe-ruby            x86_64 1.9.3.551-2.pe.el6 puppet-enterprise-installer 8.1 M
    centos67-pe-puppet-382: pe-ruby-augeas     x86_64 0.5.0-7.pe.el6     puppet-enterprise-installer  22 k
    centos67-pe-puppet-382: pe-ruby-rgen       noarch 0.6.5-1.pe.el6     puppet-enterprise-installer 238 k
    centos67-pe-puppet-382: pe-ruby-selinux    x86_64 2.0.94-4.pe.el6    puppet-enterprise-installer  57 k
    centos67-pe-puppet-382: pe-ruby-shadow     x86_64 2.2.0-3.pe.el6     puppet-enterprise-installer  11 k
    centos67-pe-puppet-382: pe-ruby-stomp      noarch 1.3.3-1.pe.el6     puppet-enterprise-installer  54 k
    centos67-pe-puppet-382: pe-rubygem-deep-merge
    centos67-pe-puppet-382: noarch 1.0.0-3.pe.el6     puppet-enterprise-installer  74 k
    centos67-pe-puppet-382: pe-virt-what       x86_64 1.13-1.el6         puppet-enterprise-installer  22 k
    centos67-pe-puppet-382: Updating for dependencies:
    centos67-pe-puppet-382: cronie-anacron     x86_64 1.4.4-15.el6_7.1   updates                      31 k
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Transaction Summary
    centos67-pe-puppet-382: ================================================================================
    centos67-pe-puppet-382: Install      26 Package(s)
    centos67-pe-puppet-382: Upgrade       3 Package(s)
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Total download size: 25 M
    centos67-pe-puppet-382: Downloading Packages:
    centos67-pe-puppet-382: --------------------------------------------------------------------------------
    centos67-pe-puppet-382: Total                                            11 MB/s |  25 MB     00:02
    centos67-pe-puppet-382: Running rpm_check_debug
    centos67-pe-puppet-382: Running Transaction Test
    centos67-pe-puppet-382: Transaction Test Succeeded
    centos67-pe-puppet-382: Running Transaction
    centos67-pe-puppet-382: Installing : pe-puppet-enterprise-release-3.8.2.0-1.pe.el6.noarch        1/32
    centos67-pe-puppet-382: Installing : pe-openssl-1.0.0s-1.pe.el6.x86_64                           2/32
    centos67-pe-puppet-382: Installing : pe-libldap-2.4.39-5.pe.el6.x86_64                           3/32
    centos67-pe-puppet-382: Installing : pe-libyaml-0.1.6-5.el6.x86_64                               4/32
    centos67-pe-puppet-382: Installing : pe-ruby-1.9.3.551-2.pe.el6.x86_64                           5/32
    centos67-pe-puppet-382: Installing : pe-rubygem-net-ssh-2.1.4-2.pe.el6.noarch                    6/32
    centos67-pe-puppet-382: Installing : pe-ruby-stomp-1.3.3-1.pe.el6.noarch                         7/32
    centos67-pe-puppet-382: Installing : pe-mcollective-common-2.7.0.1-1.pe.el6.noarch               8/32
    centos67-pe-puppet-382: Installing : pe-ruby-selinux-2.0.94-4.pe.el6.x86_64                      9/32
    centos67-pe-puppet-382: Installing : pe-ruby-rgen-0.6.5-1.pe.el6.noarch                         10/32
    centos67-pe-puppet-382: Installing : pe-rubygem-deep-merge-1.0.0-3.pe.el6.noarch                11/32
    centos67-pe-puppet-382: Installing : pe-hiera-1.3.4.5-1.pe.el6.noarch                           12/32
    centos67-pe-puppet-382: Installing : pe-augeas-1.3.0-2.pe.el6.x86_64                            13/32
    centos67-pe-puppet-382: Installing : pe-ruby-augeas-0.5.0-7.pe.el6.x86_64                       14/32
    centos67-pe-puppet-382: Updating   : libxml2-2.7.6-20.el6_7.1.x86_64                            15/32
    centos67-pe-puppet-382: Installing : 1:dmidecode-2.12-6.el6.x86_64                              16/32
    centos67-pe-puppet-382: Installing : pe-virt-what-1.13-1.el6.x86_64                             17/32
    centos67-pe-puppet-382: Installing : libxslt-1.1.26-2.el6_3.1.x86_64                            18/32
    centos67-pe-puppet-382: Installing : pe-cloud-provisioner-libs-0.3.2-2.pe.el6.x86_64            19/32
    centos67-pe-puppet-382: Installing : pe-mcollective-2.7.0.1-1.pe.el6.noarch                     20/32
    centos67-pe-puppet-382: Installing : pe-ruby-ldap-0.9.12-7.pe.el6.x86_64                        21/32
    centos67-pe-puppet-382: Installing : pe-ruby-shadow-2.2.0-3.pe.el6.x86_64                       22/32
    centos67-pe-puppet-382: Updating   : cronie-anacron-1.4.4-15.el6_7.1.x86_64                     23/32
    centos67-pe-puppet-382: Updating   : cronie-1.4.4-15.el6_7.1.x86_64                             24/32
    centos67-pe-puppet-382: Installing : pciutils-3.1.10-4.el6.x86_64                               25/32
    centos67-pe-puppet-382: Installing : pe-facter-2.4.4.0-1.pe.el6.x86_64                          26/32
    centos67-pe-puppet-382: Installing : pe-puppet-3.8.2.0-1.pe.el6.noarch                          27/32
    centos67-pe-puppet-382: Installing : pe-agent-3.8.2-1.pe.el6.noarch                             28/32
    centos67-pe-puppet-382: Installing : pe-cloud-provisioner-1.2.0-1.el6.noarch                    29/32
    centos67-pe-puppet-382: Cleanup    : cronie-anacron-1.4.4-15.el6.x86_64                         30/32
    centos67-pe-puppet-382: Cleanup    : cronie-1.4.4-15.el6.x86_64                                 31/32
    centos67-pe-puppet-382: Cleanup    : libxml2-2.7.6-20.el6.x86_64                                32/32
    centos67-pe-puppet-382: Verifying  : pe-puppet-enterprise-release-3.8.2.0-1.pe.el6.noarch        1/32
    centos67-pe-puppet-382: Verifying  : pe-hiera-1.3.4.5-1.pe.el6.noarch                            2/32
    centos67-pe-puppet-382: Verifying  : pe-mcollective-common-2.7.0.1-1.pe.el6.noarch               3/32
    centos67-pe-puppet-382: Verifying  : pciutils-3.1.10-4.el6.x86_64                                4/32
    centos67-pe-puppet-382: Verifying  : pe-libldap-2.4.39-5.pe.el6.x86_64                           5/32
    centos67-pe-puppet-382: Verifying  : pe-ruby-augeas-0.5.0-7.pe.el6.x86_64                        6/32
    centos67-pe-puppet-382: Verifying  : pe-agent-3.8.2-1.pe.el6.noarch                              7/32
    centos67-pe-puppet-382: Verifying  : pe-ruby-ldap-0.9.12-7.pe.el6.x86_64                         8/32
    centos67-pe-puppet-382: Verifying  : 1:dmidecode-2.12-6.el6.x86_64                               9/32
    centos67-pe-puppet-382: Verifying  : pe-mcollective-2.7.0.1-1.pe.el6.noarch                     10/32
    centos67-pe-puppet-382: Verifying  : pe-cloud-provisioner-libs-0.3.2-2.pe.el6.x86_64            11/32
    centos67-pe-puppet-382: Verifying  : cronie-1.4.4-15.el6_7.1.x86_64                             12/32
    centos67-pe-puppet-382: Verifying  : pe-rubygem-net-ssh-2.1.4-2.pe.el6.noarch                   13/32
    centos67-pe-puppet-382: Verifying  : pe-openssl-1.0.0s-1.pe.el6.x86_64                          14/32
    centos67-pe-puppet-382: Verifying  : pe-facter-2.4.4.0-1.pe.el6.x86_64                          15/32
    centos67-pe-puppet-382: Verifying  : pe-ruby-selinux-2.0.94-4.pe.el6.x86_64                     16/32
    centos67-pe-puppet-382: Verifying  : pe-ruby-stomp-1.3.3-1.pe.el6.noarch                        17/32
    centos67-pe-puppet-382: Verifying  : pe-ruby-shadow-2.2.0-3.pe.el6.x86_64                       18/32
    centos67-pe-puppet-382: Verifying  : libxslt-1.1.26-2.el6_3.1.x86_64                            19/32
    centos67-pe-puppet-382: Verifying  : pe-ruby-rgen-0.6.5-1.pe.el6.noarch                         20/32
    centos67-pe-puppet-382: Verifying  : pe-rubygem-deep-merge-1.0.0-3.pe.el6.noarch                21/32
    centos67-pe-puppet-382: Verifying  : pe-puppet-3.8.2.0-1.pe.el6.noarch                          22/32
    centos67-pe-puppet-382: Verifying  : cronie-anacron-1.4.4-15.el6_7.1.x86_64                     23/32
    centos67-pe-puppet-382: Verifying  : pe-libyaml-0.1.6-5.el6.x86_64                              24/32
    centos67-pe-puppet-382: Verifying  : pe-virt-what-1.13-1.el6.x86_64                             25/32
    centos67-pe-puppet-382: Verifying  : pe-augeas-1.3.0-2.pe.el6.x86_64                            26/32
    centos67-pe-puppet-382: Verifying  : pe-cloud-provisioner-1.2.0-1.el6.noarch                    27/32
    centos67-pe-puppet-382: Verifying  : pe-ruby-1.9.3.551-2.pe.el6.x86_64                          28/32
    centos67-pe-puppet-382: Verifying  : libxml2-2.7.6-20.el6_7.1.x86_64                            29/32
    centos67-pe-puppet-382: Verifying  : libxml2-2.7.6-20.el6.x86_64                                30/32
    centos67-pe-puppet-382: Verifying  : cronie-1.4.4-15.el6.x86_64                                 31/32
    centos67-pe-puppet-382: Verifying  : cronie-anacron-1.4.4-15.el6.x86_64                         32/32
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Installed:
    centos67-pe-puppet-382: dmidecode.x86_64 1:2.12-6.el6
    centos67-pe-puppet-382: libxslt.x86_64 0:1.1.26-2.el6_3.1
    centos67-pe-puppet-382: pciutils.x86_64 0:3.1.10-4.el6
    centos67-pe-puppet-382: pe-agent.noarch 0:3.8.2-1.pe.el6
    centos67-pe-puppet-382: pe-cloud-provisioner.noarch 0:1.2.0-1.el6
    centos67-pe-puppet-382: pe-cloud-provisioner-libs.x86_64 0:0.3.2-2.pe.el6
    centos67-pe-puppet-382: pe-ruby-ldap.x86_64 0:0.9.12-7.pe.el6
    centos67-pe-puppet-382: pe-rubygem-net-ssh.noarch 0:2.1.4-2.pe.el6
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Dependency Installed:
    centos67-pe-puppet-382: pe-augeas.x86_64 0:1.3.0-2.pe.el6
    centos67-pe-puppet-382: pe-facter.x86_64 0:2.4.4.0-1.pe.el6
    centos67-pe-puppet-382: pe-hiera.noarch 0:1.3.4.5-1.pe.el6
    centos67-pe-puppet-382: pe-libldap.x86_64 0:2.4.39-5.pe.el6
    centos67-pe-puppet-382: pe-libyaml.x86_64 0:0.1.6-5.el6
    centos67-pe-puppet-382: pe-mcollective.noarch 0:2.7.0.1-1.pe.el6
    centos67-pe-puppet-382: pe-mcollective-common.noarch 0:2.7.0.1-1.pe.el6
    centos67-pe-puppet-382: pe-openssl.x86_64 0:1.0.0s-1.pe.el6
    centos67-pe-puppet-382: pe-puppet.noarch 0:3.8.2.0-1.pe.el6
    centos67-pe-puppet-382: pe-puppet-enterprise-release.noarch 0:3.8.2.0-1.pe.el6
    centos67-pe-puppet-382: pe-ruby.x86_64 0:1.9.3.551-2.pe.el6
    centos67-pe-puppet-382: pe-ruby-augeas.x86_64 0:0.5.0-7.pe.el6
    centos67-pe-puppet-382: pe-ruby-rgen.noarch 0:0.6.5-1.pe.el6
    centos67-pe-puppet-382: pe-ruby-selinux.x86_64 0:2.0.94-4.pe.el6
    centos67-pe-puppet-382: pe-ruby-shadow.x86_64 0:2.2.0-3.pe.el6
    centos67-pe-puppet-382: pe-ruby-stomp.noarch 0:1.3.3-1.pe.el6
    centos67-pe-puppet-382: pe-rubygem-deep-merge.noarch 0:1.0.0-3.pe.el6
    centos67-pe-puppet-382: pe-virt-what.x86_64 0:1.13-1.el6
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Updated:
    centos67-pe-puppet-382: cronie.x86_64 0:1.4.4-15.el6_7.1       libxml2.x86_64 0:2.7.6-20.el6_7.1
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Dependency Updated:
    centos67-pe-puppet-382: cronie-anacron.x86_64 0:1.4.4-15.el6_7.1
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Complete!
    centos67-pe-puppet-382: Loaded plugins: fastestmirror
    centos67-pe-puppet-382: Cleaning repos: puppet-enterprise-installer
    centos67-pe-puppet-382: Cleaning up Everything
    centos67-pe-puppet-382: Cleaning up list of fastest mirrors
    centos67-pe-puppet-382: ## Checking the agent certificate name detection...
    centos67-pe-puppet-382: ## Setting up puppet agent...
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ------------------------------------------------------------------------
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: STEP 5: DONE
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Thanks for installing Puppet Enterprise!
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: To learn more and get started using Puppet Enterprise, refer to
    centos67-pe-puppet-382: the Puppet Enterprise Quick Start Guide
    centos67-pe-puppet-382: (http://docs.puppetlabs.com/pe/latest/quick_start.html) and the
    centos67-pe-puppet-382: Puppet Enterprise Deployment Guide
    centos67-pe-puppet-382: (http://docs.puppetlabs.com/guides/deployment_guide/index.html).
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ========================================================================
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ## NOTES
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Puppet Enterprise has been installed to &quot;/opt/puppet,&quot; and its
    centos67-pe-puppet-382: configuration files are located in &quot;/etc/puppetlabs&quot;.
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: Answers from this session saved to
    centos67-pe-puppet-382: &#39;/tmp/puppet-enterprise-3.8.2-el-6-x86_64/answers.lastrun.localhost&#39;
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: If you have a firewall running, please ensure outbound
    centos67-pe-puppet-382: connections are allowed to the following TCP ports: 8140, 61613
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: ------------------------------------------------------------------------
    centos67-pe-puppet-382:</code></pre></figure>
<figure class="highlight"><pre><code class="language-text" data-lang="text">The final provisioner is a shell provisioner that will run the scripts/centos-vmware-cleanup.sh bash script. This script will clean up the centos67 virtual machine and zero out all unused disk space to reduce the size of the image.
</code></pre></figure>
<figure class="highlight"><pre><code class="language-bash" data-lang="bash">==&gt; centos67-pe-puppet-382: Provisioning with shell script: scripts/centos-vmware-cleanup.sh
    centos67-pe-puppet-382: ==&gt; Pausing for 0 seconds...
    centos67-pe-puppet-382: ==&gt; erasing unused packages to free up space
    centos67-pe-puppet-382: Loaded plugins: fastestmirror
    centos67-pe-puppet-382: Setting up Remove Process
    centos67-pe-puppet-382: No Match for argument: gtk2
    centos67-pe-puppet-382: Determining fastest mirrors
    centos67-pe-puppet-382: * base: centos.unixheads.org
    centos67-pe-puppet-382: * extras: centos.unixheads.org
    centos67-pe-puppet-382: * updates: centos-mirror.jchost.net
    centos67-pe-puppet-382: Package(s) gtk2 available, but not installed.
    centos67-pe-puppet-382: No Match for argument: libX11
    centos67-pe-puppet-382: Package(s) libX11 available, but not installed.
    centos67-pe-puppet-382: No Match for argument: hicolor-icon-theme
    centos67-pe-puppet-382: Package(s) hicolor-icon-theme available, but not installed.
    centos67-pe-puppet-382: No Match for argument: avahi
    centos67-pe-puppet-382: Package(s) avahi available, but not installed.
    centos67-pe-puppet-382: No Match for argument: freetype
    centos67-pe-puppet-382: Package(s) freetype available, but not installed.
    centos67-pe-puppet-382: No Match for argument: bitstream-vera-fonts
    centos67-pe-puppet-382: No Packages marked for removal
    centos67-pe-puppet-382: ==&gt; Cleaning up yum cache
    centos67-pe-puppet-382: Loaded plugins: fastestmirror
    centos67-pe-puppet-382: Cleaning repos: base extras updates
    centos67-pe-puppet-382: Cleaning up Everything
    centos67-pe-puppet-382: Cleaning up list of fastest mirrors
    centos67-pe-puppet-382: ==&gt; Force logs to rotate
    centos67-pe-puppet-382: ==&gt; Clear audit log and wtmp
    centos67-pe-puppet-382: ==&gt; Cleaning up udev rules
    centos67-pe-puppet-382: ==&gt; Remove the traces of the template MAC address and UUIDs
    centos67-pe-puppet-382: ==&gt; Cleaning up tmp
    centos67-pe-puppet-382: ==&gt; Remove the SSH host keys
    centos67-pe-puppet-382: ==&gt; Remove the root user’s shell history
    centos67-pe-puppet-382: ==&gt; yum -y clean all
    centos67-pe-puppet-382: Loaded plugins: fastestmirror
    centos67-pe-puppet-382: Cleaning repos: base extras updates
    centos67-pe-puppet-382: Cleaning up Everything
    centos67-pe-puppet-382: /
    centos67-pe-puppet-382: 24828344
    centos67-pe-puppet-382: /tmp/script_7943.sh: line 59: bc: command not found
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: dd: invalid number `&#39;
    centos67-pe-puppet-382: /boot
    centos67-pe-puppet-382: 60502
    centos67-pe-puppet-382: /tmp/script_7943.sh: line 59: bc: command not found
    centos67-pe-puppet-382:
    centos67-pe-puppet-382: dd: invalid number `&#39;
    centos67-pe-puppet-382: /tmp/script_7943.sh: line 67: /usr/sbin/vgdisplay: No such file or directory
    centos67-pe-puppet-382: ==&gt; Zero out the free space to save space in the final image
    centos67-pe-puppet-382: dd: writing `/EMPTY&#39;: No space left on device
    centos67-pe-puppet-382: 12783+0 records in
    centos67-pe-puppet-382: 12782+0 records out
    centos67-pe-puppet-382: 13403570176 bytes (13 GB) copied, 428.651 s, 31.3 MB/s</code></pre></figure>

<p>With the provisioners block having been completed, Packer will now shutdown the centos67-pe-puppet-382 virtual machine and unregister it from the ESXi virtual machine.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">==&gt; centos67-pe-puppet-382: Gracefully halting virtual machine...
    centos67-pe-puppet-382: Waiting for VMware to clean up after itself...
==&gt; centos67-pe-puppet-382: Deleting unnecessary VMware files...
    centos67-pe-puppet-382: Deleting: /vmfs/volumes/datastore1/output-centos67-pe-puppet-382/vmware.log
==&gt; centos67-pe-puppet-382: Cleaning VMX prior to finishing up...
    centos67-pe-puppet-382: Unmounting floppy from VMX...
    centos67-pe-puppet-382: Detaching ISO from CD-ROM device...
    centos67-pe-puppet-382: Disabling VNC server...
==&gt; centos67-pe-puppet-382: Compacting the disk image
==&gt; centos67-pe-puppet-382: Unregistering virtual machine...
Build &#39;centos67-pe-puppet-382&#39; finished.

==&gt; Builds finished. The artifacts of successful builds are:
--&gt; centos67-pe-puppet-382: VM files in directory: /vmfs/volumes/datastore1/output-centos67-pe-puppet-382
[root@packer-centos packer-templates]#</code></pre></figure>

<p>The output of the packer build command shows that the template was successfully created at /vmfs/volumes/datastore1/output-centos67-pe-puppet-382 on the ESXi virtual machine.</p>

<h3>This brings us to the end of this post. I think it is pretty powerful that we only had to copy/update the template and add another provisioner script, in order to modify the Packer template we created last time to install Puppet Enterprise 3.8.2.</h3>

<h3>If you would like to not have to manually create the two files covered in this post, you can clone down <a href="https://github.com/sdorsett/packer-templates/tree/adding-second-packer-template">this github repository</a> by running the following command:</h3>

<pre>
git clone -b "adding-second-packer-template" https://github.com/sdorsett/packer-templates.git
</pre>  

<p>If you cloned the packer-templates repo in the last post, you can pull down the updates by running the following command:</p>

<pre>
git fetch --all
git pull origin "adding-second-packer-template"
</pre>  

<h3>In the next post we will extend what was covered in this post by installing the Puppet agent in Packer image.</h3>

<h3>Please provide any feedback or suggestions to my twitter account located on the about page.</h3>

  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page2">Older</a>
  
  
    <span class="pagination-item newer">Newer</span>
  
</div>


      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

  </body>
</html>
